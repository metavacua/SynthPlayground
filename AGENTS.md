# ---
# DO NOT EDIT THIS FILE DIRECTLY.
# This file is programmatically generated by the `protocol_compiler.py` script.
# All changes to agent protocols must be made in the source files
# located in the `protocols/` directory.
#
# This file contains the compiled protocols in a human-readable Markdown format,
# with machine-readable JSON definitions embedded.
# ---


# System Documentation

---

## `tooling/` Directory

### `tooling/__init__.py`

_No module-level docstring found._

### `tooling/code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.


**Public Functions:**


- #### `def generate_suggestion_plan(filepath, diff_content)`

  > Generates a temporary, single-step plan file to apply a code change.
  >
  > Args:
  >     filepath: The path to the file that needs to be modified.
  >     diff_content: The git-style merge diff block to be applied.
  >
  > Returns:
  >     The path to the generated temporary plan file.


- #### `def main()`

  > Main entry point for the code suggester tool.
  > Parses arguments, generates a plan, and prints the plan's path to stdout.


### `tooling/context_awareness_scanner.py`

_No module-level docstring found._


**Public Functions:**


- #### `def find_references(symbol_name, search_path)`

  > Finds all files in a directory that reference a given symbol.


- #### `def get_defined_symbols(filepath)`

  > Parses a Python file to find all defined functions and classes.


- #### `def get_imported_symbols(filepath)`

  > Parses a Python file to find all imported modules and symbols.


- #### `def main()`


### `tooling/dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.


**Public Functions:**


- #### `def find_package_json_files(root_dir)`

  > Finds all package.json files in the repository, excluding node_modules.


- #### `def find_requirements_txt_files(root_dir)`

  > Finds all requirements.txt files in the repository.


- #### `def generate_dependency_graph(root_dir='.')`

  > Generates a dependency graph for all supported dependency files found.


- #### `def main()`

  > Main function to generate and save the dependency graph.


- #### `def parse_package_json(package_json_path)`

  > Parses a single package.json file to extract its name and dependencies.


- #### `def parse_requirements_txt(requirements_path, root_dir)`

  > Parses a requirements.txt file to extract its dependencies.


### `tooling/doc_generator.py`

Generates detailed system documentation from Python source files.

This script scans specified directories for Python files, parses their
Abstract Syntax Trees (ASTs), and extracts documentation for the module,
classes, and functions. The output is a structured Markdown file.

This is a key component of the project's self-documentation capabilities,
powering the `SYSTEM_DOCUMENTATION.md` artifact in the `knowledge_core`.

The script is configured via top-level constants:
- `SCAN_DIRECTORIES`: A list of directories to search for .py files.
- `OUTPUT_FILE`: The path where the final Markdown file will be written.
- `DOC_TITLE`: The main title for the generated documentation file.

It uses Python's `ast` module to reliably parse source files without
importing them, which avoids issues with dependencies or script side-effects.


**Public Functions:**


- #### `def find_python_files(directories)`

  > Finds all Python files in the given directories, ignoring test files.


- #### `def format_args(args)`

  > Formats ast.arguments into a printable string, including defaults.


- #### `def generate_documentation(all_docs)`

  > Generates a single Markdown string from a list of ModuleDoc objects.


- #### `def generate_documentation_for_module(mod_doc)`

  > Generates Markdown content for a single module.


- #### `def main()`

  > Main function to find files, parse them, and write documentation.


- #### `def parse_file_for_docs(filepath)`

  > Parses a Python file and extracts documentation for its module, classes,
  > and functions.



**Public Classes:**


- #### `class ClassDoc`

  > Holds documentation for a single class.


  **Methods:**

  - ##### `def __init__(self, name, docstring, methods)`


- #### `class DocVisitor`

  > AST visitor to extract documentation from classes and functions.
  > It navigates the tree and builds lists of discovered documentation objects.


  **Methods:**

  - ##### `def __init__(self)`

  - ##### `def visit_ClassDef(self, node)`

  - ##### `def visit_FunctionDef(self, node)`


- #### `class FunctionDoc`

  > Holds documentation for a single function or method.


  **Methods:**

  - ##### `def __init__(self, name, signature, docstring)`


- #### `class ModuleDoc`

  > Holds all documentation for a single Python module.


  **Methods:**

  - ##### `def __init__(self, name, docstring, classes, functions)`


### `tooling/environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.


**Public Functions:**


- #### `def main()`

  > Runs all environmental probes and prints a summary report.


- #### `def probe_environment_variables()`

  > Checks for the presence of a common environment variable.


- #### `def probe_filesystem()`

  > Tests file system write/read/delete capabilities and measures latency.


- #### `def probe_network()`

  > Tests network connectivity and measures latency to a reliable external endpoint.


### `tooling/fdc_cli.py`

Provides the command-line interface for the Finite Development Cycle (FDC).

This script is a core component of the agent's protocol, offering tools to ensure
that all development work is structured, verifiable, and safe. It is used by both
the agent to signal progress and the `master_control.py` orchestrator to
validate the agent's plans before execution.

The CLI provides several key commands:
- `close`: Logs the formal end of a task, signaling to the orchestrator that
  execution is complete.
- `validate`: Performs a deep validation of a plan file against the FDC's Finite
  State Machine (FSM) definition. It checks for both syntactic correctness (Is
  the sequence of operations valid?) and semantic correctness (Does the plan try
  to use a file before creating it?).
- `analyze`: Reads a plan and provides a high-level analysis of its
  characteristics, such as its computational complexity and whether it is a
  read-only or read-write plan.
- `lint`: A comprehensive "linter" that runs a full suite of checks on a plan
  file, including `validate`, `analyze`, and checks for disallowed recursion.


**Public Functions:**


- #### `def analyze_plan(plan_filepath)`

  > Analyzes a plan file to determine its decidability, modality, and complexity.
  >
  > This function performs a comprehensive analysis of a given plan by:
  > 1.  Confirming its decidability based on the system's design.
  > 2.  Determining its modality (Read-Only vs. Read-Write).
  > 3.  Calculating its computational complexity by combining the structural
  >     complexity of the plan (e.g., loops) with the declared complexities
  >     of the tools it calls, as defined in `knowledge_core/tool_complexity.json`.


- #### `def close_task(task_id)`

  > Logs the formal end of a task.
  >
  > This command's primary role is to create a TASK_END log entry. It no longer
  > manages the post-mortem file directly; that process is now fully owned by
  > the MasterControlGraph orchestrator, which is the single source of truth
  > for state transitions and artifact lifecycle management.


- #### `def lint_plan(plan_filepath)`

  > Runs a comprehensive suite of checks on a plan file.
  > The old recursion check is now obsolete, as the max depth is checked
  > directly within the new hierarchical validator.


- #### `def main()`


- #### `def start_task(task_id)`

  > Logs the formal start of a task.


- #### `def start_task(task_id)`

  > Initiates the AORP cascade for a new task.


- #### `def validate_plan(plan_filepath)`

  > Validates a plan using the centralized parser.


### `tooling/hierarchical_compiler.py`

_No module-level docstring found._


**Public Functions:**


- #### `def cleanup_summaries(directory)`

  > Removes temporary summary files from a protocols directory.


- #### `def find_protocol_dirs(root_dir)`

  > Finds all directories named 'protocols' within the root directory,
  > ignoring any special-cased directories.


- #### `def generate_summary(child_agents_md_path)`

  > Generates a summary of a child AGENTS.md file by extracting protocol IDs.


- #### `def get_parent_module(module_path, all_module_paths)`

  > Finds the direct parent module of a given module.


- #### `def main()`

  > Main function to orchestrate the hierarchical compilation.


- #### `def run_compiler(source_dir)`

  > Invokes the protocol_compiler.py script as a subprocess.


- #### `def run_readme_generator(source_agents_md)`

  > Invokes the readme_generator.py script as a subprocess.


### `tooling/knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.


**Public Functions:**


- #### `def extract_lessons_from_postmortem(postmortem_content)`

  > Parses a post-mortem report to extract lessons learned.
  > Handles multiple possible section headers and formats.


- #### `def extract_metadata_from_postmortem(postmortem_content)`

  > Parses a post-mortem report to extract metadata like Task ID and Date.


- #### `def format_lesson_entry(metadata, lesson_data)`

  > Formats an extracted lesson into a structured JSON object.


- #### `def main()`


- #### `def parse_action_to_command(action_text)`

  > Parses a natural language action string into a machine-executable command.
  >
  > This is the core of translating insights into automated actions. It uses
  > pattern matching to identify specific, supported commands.


### `tooling/knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.


**Public Functions:**


- #### `def extract_concepts(graph)`

  > Extracts key concepts (e.g., tools) from the local graph to query externally.
  > This version dynamically extracts tool names from the graph.


- #### `def load_local_graph(graph_file)`

  > Loads the local RDF graph from a file.


- #### `def query_dbpedia(concept)`

  > Queries DBPedia for a given concept and returns a graph of results.


- #### `def run_knowledge_integration(input_graph_path, output_graph_path)`

  > The main library function to run the knowledge integration process.
  > It loads a graph, extracts concepts, queries DBPedia, and saves the
  > enriched graph.


### `tooling/log_failure.py`

_No module-level docstring found._


**Public Functions:**


- #### `def log_catastrophic_failure()`

  > Logs the catastrophic failure event.


### `tooling/master_control.py`

The master orchestrator for the agent's lifecycle, governed by a Finite State Machine.

This script, `master_control.py`, is the heart of the agent's operational loop.
It implements a strict, protocol-driven workflow defined in a JSON file
(typically `tooling/fsm.json`). The `MasterControlGraph` class reads this FSM
definition and steps through the prescribed states, ensuring that the agent
cannot deviate from the established protocol.

The key responsibilities of this orchestrator include:
- **State Enforcement:** Guiding the agent through the formal states of a task:
  ORIENTING, PLANNING, EXECUTING, FINALIZING, and finally AWAITING_SUBMISSION.
- **Plan Validation:** Before execution, it invokes the `fdc_cli.py` tool to
  formally validate the agent-generated `plan.txt`, preventing the execution of
  invalid or unsafe plans.
- **Hierarchical Execution (CFDC):** It manages the plan execution stack, which
  is the core mechanism of the Context-Free Development Cycle (CFDC). This
  allows plans to call other plans as sub-routines via the `call_plan`
  directive.
- **Recursion Safety:** It enforces a `MAX_RECURSION_DEPTH` on the plan stack to
  guarantee that the execution process is always decidable and will terminate.
- **Lifecycle Management:** It orchestrates the entire lifecycle, from initial
  orientation and environmental probing to the final post-mortem analysis and
  compilation of lessons learned.

The FSM operates by waiting for specific signals—typically the presence of
files like `plan.txt` or `step_complete.txt`—before transitioning to the next
state. This creates a robust, interactive loop where the orchestrator directs
the high-level state, and the agent is responsible for completing the work
required to advance that state.


**Public Classes:**


- #### `class MasterControlGraph`

  > A Finite State Machine (FSM) that enforces the agent's protocol.
  > This graph reads a state definition and orchestrates the agent's workflow,
  > ensuring that all protocol steps are followed in the correct order.


  **Methods:**

  - ##### `def __init__(self, fsm_path='tooling/fsm.json')`

  - ##### `def do_execution(self, agent_state)`

    > Executes the plan using a stack-based approach to handle sub-plans (CFDC).

  - ##### `def do_finalizing(self, agent_state)`

    > Handles the finalization of the task, including post-mortem analysis and self-correction.

  - ##### `def do_orientation(self, agent_state)`

    > Executes the L1, L2, and L3 orientation steps.

  - ##### `def do_planning(self, agent_state)`

    > Waits for the agent to provide a plan, validates it, parses it into
    > commands, and initializes the plan stack for execution.

  - ##### `def do_researching(self, agent_state)`

    > Generates, validates, and initiates a formal Deep Research FDC.

  - ##### `def get_trigger(self, source_state, dest_state)`

    > Finds a trigger in the FSM definition for a transition from a source
    > to a destination state. This is a helper to avoid hardcoding trigger
    > strings in the state handlers.

  - ##### `def run(self, initial_agent_state)`

    > Runs the agent's workflow through the FSM.


### `tooling/master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script provides a clean entry point for initiating a task. It handles
argument parsing, initializes the agent's state, and runs the main FSM-driven
workflow defined in `master_control.py`.


**Public Functions:**


- #### `def main()`

  > The main entry point for the agent.
  >
  > This script initializes the agent's state, runs the master control graph
  > to enforce the protocol, and prints the final result.


### `tooling/pages_generator.py`

Generates a single HTML file for GitHub Pages from the repository's metalanguage.

This script combines the human-readable `README.md` and the machine-readable
`AGENTS.md` into a single, navigable HTML document. It uses the `markdown`
library to convert the Markdown content to HTML and to automatically generate
a Table of Contents.

The final output is a semantic HTML5 document, `index.html`, which serves as
the main page for the project's GitHub Pages site.


**Public Functions:**


- #### `def generate_html_page()`

  > Reads the source Markdown files, converts them to HTML, and builds the
  > final index.html page.


### `tooling/plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.


**Public Functions:**


- #### `def deregister_plan(name)`

  > Removes a plan from the registry by its logical name.


- #### `def get_registry()`

  > Loads the plan registry from its JSON file.


- #### `def list_plans()`

  > Lists all currently registered plans.


- #### `def main()`

  > Main function to run the plan management CLI.


- #### `def register_plan(name, path)`

  > Registers a new plan by mapping a logical name to a file path.


- #### `def save_registry(registry_data)`

  > Saves the given data to the plan registry JSON file.


### `tooling/plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.


**Public Functions:**


- #### `def parse_plan(plan_content)`

  > Parses the raw text of a plan into a list of Command objects.
  > This parser correctly handles multi-line arguments and ignores comments.
  > Commands are expected to be separated by one or more blank lines.



**Public Classes:**


- #### `class Command`

  > Represents a single, parsed command from a plan.
  > This structure correctly handles multi-line arguments for tools.


### `tooling/protocol_auditor.py`

Audits the agent's behavior against its governing protocols and generates a report.

This script performs a comprehensive analysis to ensure the agent's actions,
as recorded in the activity log, align with the defined protocols in AGENTS.md.
It serves as a critical feedback mechanism for maintaining operational integrity.
The final output is a detailed `audit_report.md` file.

The auditor performs three main checks:
1.  **`AGENTS.md` Source Check:** Verifies if the `AGENTS.md` build artifact is
    potentially stale by comparing its modification time against the source
    protocol files in the `protocols/` directory.
2.  **Protocol Completeness:** It cross-references the tools used in the log
    (`logs/activity.log.jsonl`) against the tools defined in `AGENTS.md` to find:
    - Tools used but not associated with any formal protocol.
    - Tools defined in protocols but never used in the log.
3.  **Tool Centrality:** It conducts a frequency analysis of tool usage to
    identify which tools are most critical to the agent's workflow.

The script parses all embedded JSON protocol blocks within `AGENTS.md` and reads
from the standard `logs/activity.log.jsonl` log file, providing a reliable and
accurate audit.


**Public Functions:**


- #### `def find_all_agents_md_files(root_dir)`

  > Finds all AGENTS.md files in the repository.


- #### `def generate_markdown_report(source_checks, unreferenced, unused, centrality)`

  > Generates a Markdown-formatted string from the audit results.


- #### `def get_protocol_tools_from_agents_md(agents_md_paths)`

  > Parses a list of AGENTS.md files to get a set of all tools associated
  > with protocols.


- #### `def get_used_tools_from_log(log_path)`

  > Parses the JSONL log file to get a list of used tool names.
  > It specifically looks for 'TOOL_EXEC' actions and extracts the tool
  > from the 'command' field based on the logging schema.
  > This version is robust against malformed lines with multiple JSON objects.


- #### `def main()`

  > Main function to run the protocol auditor and generate a report.


- #### `def run_centrality_analysis(used_tools)`

  > Performs a frequency analysis on the tool log and returns the counts.


- #### `def run_completeness_check(used_tools, protocol_tools)`

  > Compares used tools with protocol-defined tools and returns the gaps.


- #### `def run_protocol_source_check(all_agents_files)`

  > Checks if each AGENTS.md file is older than its corresponding source files.
  > Returns a list of warning/error dictionaries.


### `tooling/protocol_compiler.py`

Compiles source protocol files into unified, human-readable and machine-readable artifacts.

This script is the engine behind the "protocol as code" principle. It discovers,
validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
into high-level documents like `AGENTS.md`.

Key Functions:
- **Discovery:** Scans a directory for source files, including `.protocol.json`
  (machine-readable rules) and `.protocol.md` (human-readable context).
- **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  `.protocol.json` file, ensuring all protocol definitions are syntactically
  correct and adhere to the established structure.
- **Compilation:** Combines the human-readable markdown and the machine-readable
  JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
- **Documentation Injection:** Can inject other generated documents, like the
  `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
- **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  protocols and serialize them into an RDF knowledge graph (in Turtle format),
  creating a machine-queryable version of the agent's governing rules.

This process ensures that `AGENTS.md` and other protocol documents are not edited
manually but are instead generated from a validated, single source of truth,
making the agent's protocols robust, verifiable, and maintainable.


**Public Functions:**


- #### `def compile_protocols(source_dir, target_file, schema_file, knowledge_graph_file=None, autodoc_file=None)`

  > Reads all .protocol.json and corresponding .protocol.md files from the
  > source directory, validates them, and compiles them into a target markdown file.
  > Optionally, it can also generate a machine-readable knowledge graph.


- #### `def load_schema(schema_file)`

  > Loads the protocol JSON schema.


- #### `def main()`

  > Main function to run the compiler.


### `tooling/protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.


**Public Functions:**


- #### `def add_tool_to_protocol(protocol_id, tool_name, protocols_dir)`

  > Adds a tool to the 'associated_tools' list of a specified protocol.


- #### `def find_protocol_file(protocol_id, protocols_dir)`

  > Finds the protocol file path corresponding to a given protocol_id.


- #### `def main()`

  > Main function to parse arguments and call the appropriate handler.


- #### `def update_rule_in_protocol(protocol_id, rule_id, new_description, protocols_dir)`

  > Updates the description of a specific rule within a protocol.


### `tooling/readme_generator.py`

_No module-level docstring found._


**Public Functions:**


- #### `def generate_core_protocols_section(agents_md_path)`

  > Parses a given AGENTS.md file to extract protocol definitions and generate a Markdown summary.


- #### `def generate_key_components_section(module_path)`

  > Generates the Markdown for the "Key Components" section by documenting
  > any `.py` files found in a `tooling/` subdirectory of the module.


- #### `def get_module_docstring(filepath)`

  > Parses a Python file and extracts the module-level docstring.


- #### `def main()`

  > Main function to generate the README.md content and write it to a file.


### `tooling/research.py`

A unified, constraint-based interface for all research and data-gathering operations.

This script abstracts the various methods an agent might use to gather information
(reading local files, accessing the web, querying a database) into a single,
standardized function: `execute_research_protocol`. It is a core component of
the Advanced Orientation and Research Protocol (AORP), providing the mechanism
by which the agent fulfills the requirements of each orientation level (L1-L4).

The function operates on a `constraints` dictionary, which specifies the target,
scope, and other parameters of the research task. This design allows the calling
orchestrator (e.g., `master_control.py`) to request information without needing
to know the underlying implementation details of how that information is fetched.

This script is designed to be executed by a system that has pre-loaded the
following native tools into the execution environment:
- `read_file(filepath: str) -> str`
- `list_files(path: str = ".") -> list[str]`
- `google_search(query: str) -> str`
- `view_text_website(url: str) -> str`


**Public Functions:**


- #### `def execute_research_protocol(constraints)`

  > Executes a research task based on a dictionary of constraints.
  >
  > This function delegates to native, pre-loaded tools based on the specified
  > target and scope.
  >
  > Args:
  >     constraints: A dictionary specifying the operational parameters.
  >         - target: 'local_filesystem', 'external_web', 'external_repository', or 'knowledge_graph'.
  >         - scope: 'file', 'directory', 'narrow', 'broad', or 'enrich'.
  >         - path: The file or directory path for local filesystem operations.
  >         - query: The search term for web research.
  >         - url: The specific URL for direct web access.
  >         - input_graph_path: Path to the source knowledge graph.
  >         - output_graph_path: Path to save the enriched knowledge graph.
  >
  > Returns:
  >     A string containing the result of the research operation.


### `tooling/research_planner.py`

Generates a structured, executable plan for conducting deep research tasks.

This script provides a standardized, FSM-compliant workflow for the agent when
it needs to perform in-depth research on a complex topic. The `plan_deep_research`
function creates a plan file that is not just a template, but a formal,
verifiable artifact that can be executed by the `master_control.py` orchestrator.

The generated plan adheres to the state transitions defined in `research_fsm.json`,
guiding the agent through the phases of GATHERING, SYNTHESIZING, and REPORTING.


**Public Functions:**


- #### `def plan_deep_research(topic)`

  > Generates a structured, FSM-compliant executable plan for deep research.
  >
  > This function creates a plan that guides an agent through a research
  > workflow. The plan is designed to be validated by `fdc_cli.py` against
  > the `tooling/research_fsm.json` definition.
  >
  > Args:
  >     topic: The research topic to be investigated.
  >
  > Returns:
  >     A string containing the executable research plan.


### `tooling/self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.


**Public Functions:**


- #### `def load_lessons()`

  > Loads all lessons from the JSONL file.


- #### `def main()`

  > Main function to run the self-correction workflow.


- #### `def process_lessons(lessons, protocols_dir)`

  > Processes all pending lessons, applies them, and updates their status.
  > Returns True if any changes were made, False otherwise.


- #### `def run_command(command)`

  > Runs a command and returns True on success, False on failure.


- #### `def save_lessons(lessons)`

  > Saves a list of lessons back to the JSONL file, overwriting it.


### `tooling/self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.


**Public Functions:**


- #### `def analyze_planning_efficiency(log_file)`

  > Analyzes the log file to find tasks with multiple plan revisions.
  >
  > Args:
  >     log_file (str): Path to the activity log file.
  >
  > Returns:
  >     dict: A dictionary mapping task IDs to the number of plan updates.


- #### `def analyze_protocol_violations(log_file)`

  > Scans the log file for critical protocol violations, such as the
  > unauthorized use of `reset_all`.
  >
  > This function checks for two conditions:
  > 1. A `SYSTEM_FAILURE` log explicitly blaming `reset_all`.
  > 2. A `TOOL_EXEC` log where the command contains "reset_all".
  >
  > Args:
  >     log_file (str): Path to the activity log file.
  >
  > Returns:
  >     list: A list of unique task IDs where `reset_all` was used.


- #### `def main()`

  > Main function to run the self-improvement analysis CLI.


### `tooling/standard_agents_compiler.py`

_No module-level docstring found._


**Public Functions:**


- #### `def main()`

  > Generates a standard-compliant AGENTS.md file by parsing commands
  > from the project's Makefile.


- #### `def parse_makefile_command(target_name, makefile_content)`

  > Parses a Makefile to find the main command for a specific target,
  > skipping any 'echo' lines. This version iterates through lines for robustness.


### `tooling/state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.


**Public Classes:**


- #### `class AgentState`

  > Represents the complete, serializable state of the agent's workflow.
  >
  > This dataclass acts as a central container for all information related to the
  > agent's current task. It is designed to be passed between the different states
  > of the `MasterControlGraph` FSM, ensuring that context is maintained
  > throughout the lifecycle of a task.
  >
  > Attributes:
  >     task: A string describing the overall objective.
  >     plan_path: The file path to the root plan for the current task.
  >     plan_stack: A list of `PlanContext` objects, forming the execution
  >         stack for the CFDC. The plan at the top of the stack is the one
  >         currently being executed.
  >     messages: A history of messages, typically for interaction with an LLM.
  >     orientation_complete: A flag indicating if the initial orientation
  >         phase has been successfully completed.
  >     vm_capability_report: A string summarizing the results of the
  >         environmental probe.
  >     research_findings: A dictionary to store the results of research tasks.
  >     draft_postmortem_path: The file path to the draft post-mortem report
  >         generated during the AWAITING_ANALYSIS state.
  >     final_report: A string containing a summary of the final, completed
  >         post-mortem report.
  >     error: An optional string that holds an error message if the FSM
  >         enters an error state, providing a clear reason for the failure.


  **Methods:**

  - ##### `def to_json(self)`


- #### `class PlanContext`

  > Represents the execution context of a single plan file within the plan stack.
  >
  > This class holds the state of a specific plan being executed, including its
  > file path, its content (as a list of parsed Command objects), and a pointer
  > to the current step being executed.


### `tooling/symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.


**Public Functions:**


- #### `def generate_symbols_with_ast(root_dir='.')`

  > Fallback to generate a symbol map for Python files using the AST module.


- #### `def generate_symbols_with_ctags(root_dir='.')`

  > Generates a symbol map using Universal Ctags.


- #### `def has_ctags()`

  > Check if Universal Ctags is installed and available in the PATH.


- #### `def main()`

  > Main function to generate and save the symbol map.


---

## `utils/` Directory

### `utils/__init__.py`

_No module-level docstring found._

### `utils/logger.py`

Provides a standardized, schema-validated logger for producing structured JSONL logs.

This module contains the `Logger` class, which is responsible for creating all
entries in the `logs/activity.log.jsonl` file. This is a critical component for
maintaining an auditable, machine-readable record of the agent's actions.

The logger enforces a strict structure on all log entries by validating them
against a formal JSON schema, which is extracted from the `LOGGING_SCHEMA.md`
document. This ensures that every log entry, regardless of its source, is
consistent and contains the required fields.

Key features of the `Logger` class:
- **Schema Validation:** Each log entry is validated against the official
  project schema before being written to disk, preventing data corruption.
- **Structured Data:** Logs are written in JSONL format, where each line is a
  valid JSON object, making them easy to parse and query.
- **Session Management:** It automatically assigns a unique `session_id` to
  all logs generated during its lifecycle, allowing actions to be traced back
  to a specific run.
- **Automatic Timestamps:** It injects a UTC timestamp into every log entry,
  providing a precise timeline of events.

This centralized logger is the sole mechanism by which the agent should record
its activities, ensuring a single source of truth for all post-mortem analysis
and self-improvement activities.


**Public Classes:**


- #### `class Logger`

  > A class to handle structured logging to a JSONL file, validated against a schema.


  **Methods:**

  - ##### `def __init__(self, schema_path='LOGGING_SCHEMA.md', log_path='logs/activity.log.jsonl')`

    > Initializes the Logger, loading the schema and setting up the session.
    >
    > Args:
    >     schema_path (str): The path to the Markdown file containing the logging schema.
    >     log_path (str): The path to the log file to be written.

  - ##### `def log(self, phase, task_id, plan_step, action_type, action_details, outcome_status, outcome_message='', error_details=None, evidence='')`

    > Constructs, validates, and writes a log entry.
    >
    > Args:
    >     phase (str): The current protocol phase (e.g., "Phase 7").
    >     task_id (str): The ID of the current task.
    >     plan_step (int): The current plan step number.
    >     action_type (str): The type of action (e.g., "TOOL_EXEC").
    >     action_details (dict): Details specific to the action.
    >     outcome_status (str): The outcome of the action ("SUCCESS", "FAILURE").
    >     outcome_message (str, optional): A message describing the outcome. Defaults to "".
    >     error_details (dict, optional): Structured error info if the outcome is a failure. Defaults to None.
    >     evidence (str, optional): Citation for the action. Defaults to "".
    >
    > Raises:
    >     ValidationError: If the generated log entry does not conform to the schema.


---

# Protocol: Agent Bootstrap

**Rule `bootstrap-load-agents-md`**: Upon initialization for any task, the agent's first and highest-priority action must be to locate, read, and parse the `AGENTS.md` file in the repository root.

**Rationale**: The `AGENTS.md` file contains the master operational protocols, including build instructions, testing procedures, architectural guidelines, and rules governing the agent's own behavior. Failing to consult this file first is a critical operational failure. This protocol ensures that the agent is properly contextualized before any planning or execution begins.

**Procedure**:
1.  On task start, immediately execute a `read_file('AGENTS.md')` operation.
2.  Parse the contents of `AGENTS.md` to load all rules and protocols into active memory.
3.  The plan for the user's request must be formulated *after* and *in accordance with* the protocols loaded from `AGENTS.md`.
4.  If `AGENTS.md` is not found, the agent should notify the user of a critical configuration error.

---

# Protocol: Dependency Management

This protocol establishes a formal process for managing Python dependencies to ensure a reliable and repeatable execution environment.

## The Problem: Environmental Brittleness

The agent's operating environment previously lacked a formal dependency declaration. This required manual, trial-and-error installation of necessary packages (`jsonschema`, `rdflib`, `requests`) upon encountering `ModuleNotFoundError` exceptions. This process is inefficient, error-prone, and makes the successful execution of tasks dependent on undocumented, ad-hoc environmental setup.

## The Solution: Formal Dependency Declaration and Installation

To solve this, two components are introduced:

1.  **`requirements.txt`:** A standard `requirements.txt` file is added to the repository root. This file serves as the single source of truth for all required Python packages.
2.  **A New Protocol Rule:** A new rule, `dependency-install-on-start`, is established. This rule mandates that upon starting any task, the agent's first action *after* reading `AGENTS.md` should be to install the dependencies listed in `requirements.txt` using `pip`.

This protocol transforms dependency management from an ad-hoc, reactive process into a proactive, automated, and verifiable step in the agent's workflow, significantly improving its robustness and reliability.

---

# Meta-Protocol: Toolchain Review on Schema Change

This protocol establishes a critical feedback loop to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.

## The Problem: Protocol-Toolchain Desynchronization

A significant process gap was identified where a major architectural change to the protocol system (e.g., the introduction of a hierarchical `AGENTS.md` structure) did not automatically trigger a review of the tools that depend on that structure. The `protocol_auditor.py` tool, for instance, became partially obsolete as it was unaware of the new hierarchical model, leading to incomplete audits. This demonstrates that the agent's tools can become desynchronized from its own governing rules, creating a critical blind spot.

## The Solution: Mandated Toolchain Audit

This protocol closes that gap by introducing a new rule that explicitly links changes in the protocol system's architecture to a mandatory review of the toolchain.

**Rule `toolchain-audit-on-schema-change`**: If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory **must** be performed as a subsequent step.

This ensures that any modification to the fundamental way protocols are defined or processed is immediately followed by a conscious verification that all dependent tools are still functioning correctly and are aware of the new structure. This transforms the previously manual and error-prone discovery process into a formal, required step of the development lifecycle.

---

## Child Module: `compliance`

This module contains the following protocols, which are defined in its own `AGENTS.md` file:

- `best-practices-001`
- `meta-protocol-001`
- `non-compliance-protocol-001`
- `pre-commit-protocol-001`
- `protocol-reset-all-pre-check-001`
- `reset-all-authorization-001`
- `reset-all-prohibition-001`

---


---

## Child Module: `core`

This module contains the following protocols, which are defined in its own `AGENTS.md` file:

- `cfdc-protocol-001`
- `core-directive-001`
- `decidability-constraints-001`
- `deep-research-cycle-001`
- `fdc-protocol-001`
- `orientation-cascade-001`
- `plan-registry-001`
- `research-fdc-001`
- `research-protocol-001`
- `self-correction-protocol-001`
- `standing-orders-001`

---


---

## Child Module: `critic`

This module contains the following protocols, which are defined in its own `AGENTS.md` file:

- `critic-meta-protocol-001`
- `critic-reset-prohibition-001`

---


---

# Protocol: Computational Tractability

This protocol introduces a formal checkpoint to ensure that agent-generated plans are not only decidable but also computationally efficient.

## The Problem: The Risk of Inefficient Plans

The agent's execution framework guarantees that all plans will eventually halt (decidability). However, it does not prevent the agent from generating plans that are technically correct but wildly inefficient. For example, a plan might use a `grep` operation inside a `for_each_file` loop, resulting in a high-complexity (e.g., polynomial time) operation that could consume significant resources and time, even though it is guaranteed to terminate.

## The Solution: Mandated Efficiency Analysis

To address this, we introduce a new protocol that mandates the use of the enhanced `analyze` command in the FDC toolchain. This command provides a detailed report on a plan's computational complexity, drawing from a formal knowledge base of tool efficiencies.

By requiring the agent to perform this analysis on any plan that contains loops or other high-complexity patterns, we ensure that "performance" becomes a primary consideration during the planning phase. This allows the agent to self-correct and identify opportunities to refactor its own plans for better efficiency before execution begins, leading to a more robust and performant system.

---

```json
{
  "protocol_id": "agent-bootstrap-001",
  "description": "A foundational protocol that dictates the agent's initial actions upon starting any task.",
  "rules": [
    {
      "rule_id": "bootstrap-load-agents-md",
      "description": "Upon initialization for any task, the agent's first and highest-priority action must be to locate, read, and parse the AGENTS.md file in the repository root. This ensures the agent is properly contextualized before any planning or execution begins.",
      "enforcement": "This rule is enforced by the agent's core startup logic. The agent must verify the load of AGENTS.md before proceeding to the planning phase."
    }
  ],
  "associated_tools": [
    "read_file"
  ]
}
```


---

```json
{
  "protocol_id": "dependency-management-001",
  "description": "A protocol for ensuring a reliable execution environment through formal dependency management.",
  "rules": [
    {
      "rule_id": "dependency-install-on-start",
      "description": "Upon starting a task, after loading AGENTS.md, the agent MUST install all required Python packages listed in the `requirements.txt` file. This ensures the environment is correctly configured before any other tools are executed.",
      "enforcement": "The agent's core startup logic should be designed to execute `pip install -r requirements.txt` as one of its initial actions."
    }
  ],
  "associated_tools": [
    "run_in_bash_session"
  ]
}
```


---

```json
{
  "protocol_id": "toolchain-review-on-schema-change-001",
  "description": "A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.",
  "rules": [
    {
      "rule_id": "toolchain-audit-on-schema-change",
      "description": "If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory MUST be performed as a subsequent step. This audit should verify that all tools are compatible with the new protocol structure.",
      "enforcement": "This is a procedural rule for any agent developing the protocol system. Adherence can be partially checked by post-commit hooks or review processes that look for a tooling audit in any change that modifies the specified core files."
    }
  ],
  "associated_tools": [
    "tooling/protocol_auditor.py",
    "tooling/protocol_compiler.py",
    "tooling/hierarchical_compiler.py"
  ]
}
```


---

```json
{
  "protocol_id": "computational-tractability-001",
  "description": "A protocol to ensure that agent-generated plans are not only decidable but also computationally efficient and tractable.",
  "rules": [
    {
      "rule_id": "mandate-efficiency-analysis",
      "description": "For any plan that involves loops ('for_each_file') or is identified as potentially high-complexity, the agent SHOULD use the 'analyze' command of the FDC toolchain to assess its computational complexity before execution. This ensures that the agent is aware of the performance implications of its plans and can avoid generating unnecessarily inefficient solutions.",
      "enforcement": "This is a procedural best practice. Adherence can be encouraged through code review and post-mortem analysis, and potentially automated in the future by having the 'lint' command issue a warning for complex plans."
    }
  ],
  "associated_tools": [
    "tooling/fdc_cli.py"
  ]
}
```


---
