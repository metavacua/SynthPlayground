# ---
# DO NOT EDIT THIS FILE DIRECTLY.
# This file is programmatically generated by the `protocol_compiler.py` script.
# All changes to agent protocols must be made in the source files
# located in the `protocols/` directory.
#
# This file contains the compiled protocols in a human-readable Markdown format,
# with machine-readable JSON definitions embedded.
# ---


# Protocol: Agent Bootstrap

**Rule `bootstrap-load-agents-md`**: Upon initialization for any task, the agent's first and highest-priority action must be to locate, read, and parse the `AGENTS.md` file in the repository root.

**Rationale**: The `AGENTS.md` file contains the master operational protocols, including build instructions, testing procedures, architectural guidelines, and rules governing the agent's own behavior. Failing to consult this file first is a critical operational failure. This protocol ensures that the agent is properly contextualized before any planning or execution begins.

**Procedure**:
1.  On task start, immediately execute a `read_file('AGENTS.md')` operation.
2.  Parse the contents of `AGENTS.md` to load all rules and protocols into active memory.
3.  The plan for the user's request must be formulated *after* and *in accordance with* the protocols loaded from `AGENTS.md`.
4.  If `AGENTS.md` is not found, the agent should notify the user of a critical configuration error.

**Rule `bootstrap-scan-for-documents`**: After processing `AGENTS.md`, the agent should perform a scan of the repository for document files that could contain relevant information.

**Rationale**: Important project documentation, specifications, or other relevant information may be contained in various document formats. Proactively scanning for and processing these documents will provide the agent with a more complete context for the task at hand.

**Procedure**:
1.  The agent will perform a file listing to identify potential documents of interest (e.g., `.pdf`, `.md`, `.txt`).
2.  For each identified document, the agent will use the appropriate tool to read and summarize its contents. For PDF files, this will involve using a PDF reading library.
3.  The agent will incorporate the summarized information into its understanding of the project and use it to inform the planning process.

---

# Protocol: Dependency Management

This protocol establishes a formal process for managing Python dependencies to ensure a reliable and repeatable execution environment.

## The Problem: Environmental Brittleness

The agent's operating environment previously lacked a formal dependency declaration. This required manual, trial-and-error installation of necessary packages (`jsonschema`, `rdflib`, `requests`) upon encountering `ModuleNotFoundError` exceptions. This process is inefficient, error-prone, and makes the successful execution of tasks dependent on undocumented, ad-hoc environmental setup.

## The Solution: Formal Dependency Declaration and Installation

To solve this, two components are introduced:

1.  **`requirements.txt`:** A standard `requirements.txt` file is added to the repository root. This file serves as the single source of truth for all required Python packages.
2.  **A New Protocol Rule:** A new rule, `dependency-install-on-start`, is established. This rule mandates that upon starting any task, the agent's first action *after* reading `AGENTS.md` should be to install the dependencies listed in `requirements.txt` using `pip`.

This protocol transforms dependency management from an ad-hoc, reactive process into a proactive, automated, and verifiable step in the agent's workflow, significantly improving its robustness and reliability.

---

# Protocol: Experimental Prologue

This protocol is a test case to verify the agent's ability to dynamically adapt its behavior to new rules.

## Rule: `create-prologue-file`

Before creating any file, the agent must first create a file named `prologue.txt` with the content "This is a prologue file." This serves as a behavioral check. If the agent creates this file before other requested files, it demonstrates that it has successfully loaded and is following this experimental protocol.

---

# Protocol: Agent Shell Entry Point

This protocol establishes the `agent_shell.py` script as the sole, official entry point for initiating any and all agent tasks.

## The Problem: Inconsistent Initialization

Prior to this protocol, there was no formally mandated entry point for the agent. This could lead to tasks being initiated through different scripts, potentially bypassing critical setup procedures like FSM initialization, logger configuration, and state management. This inconsistency makes the agent's behavior less predictable and harder to debug.

## The Solution: A Single, Enforced Entry Point

This protocol mandates the use of `tooling/agent_shell.py` for all task initiations.

**Rule `shell-is-primary-entry-point`**: All agent tasks must be initiated through the `agent_shell.py` script.

This ensures that every task begins within a controlled, programmatic environment where:
1.  The MasterControlGraph FSM is correctly instantiated and run.
2.  The centralized logger is initialized for comprehensive, structured logging.
3.  The agent's lifecycle is managed programmatically, not through fragile file-based signals.

By enforcing a single entry point, this protocol enhances the reliability, auditability, and robustness of the entire agent system.

---

# Meta-Protocol: Toolchain Review on Schema Change

This protocol establishes a critical feedback loop to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.

## The Problem: Protocol-Toolchain Desynchronization

A significant process gap was identified where a major architectural change to the protocol system (e.g., the introduction of a hierarchical `AGENTS.md` structure) did not automatically trigger a review of the tools that depend on that structure. The `protocol_auditor.py` tool, for instance, became partially obsolete as it was unaware of the new hierarchical model, leading to incomplete audits. This demonstrates that the agent's tools can become desynchronized from its own governing rules, creating a critical blind spot.

## The Solution: Mandated Toolchain Audit

This protocol closes that gap by introducing a new rule that explicitly links changes in the protocol system's architecture to a mandatory review of the toolchain.

**Rule `toolchain-audit-on-schema-change`**: If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory **must** be performed as a subsequent step.

This ensures that any modification to the fundamental way protocols are defined or processed is immediately followed by a conscious verification that all dependent tools are still functioning correctly and are aware of the new structure. This transforms the previously manual and error-prone discovery process into a formal, required step of the development lifecycle.

---

# --- Child Module: `compliance` ---

# Protocol: AGENTS.md Non-Compliance

This protocol defines what constitutes non-compliance with the established `AGENTS.md` protocols and outlines the expected corrective actions. Adherence to these meta-rules is critical for ensuring predictable, verifiable, and robust agent behavior.

## Rule `non-compliance-definition`: Definition of Non-Compliance

Non-compliance is the act of violating the explicit rules, conventions, or procedures documented in the `AGENTS.md` file and its source protocols within the `protocols/` directory. The `AGENTS.md` file serves as the agent's primary operational mandate, and any deviation from it is considered a fault condition.

## Rule `non-compliance-direct-editing`: Prohibition of Direct Artifact Editing

**Violation:** Directly editing any file that is a documented build artifact.
- **Primary Example:** Modifying `AGENTS.md` or `README.md` directly is a severe violation. These files are generated from source protocols and documentation.
- **Corrective Action:** The agent must revert any direct edits to build artifacts. Changes must be made to the source files (e.g., in `protocols/` or tool docstrings), and the appropriate build command (e.g., `make AGENTS.md`, `make docs`) must be executed.

## Rule `non-compliance-test-procedure`: Adherence to Testing Protocols

**Violation:** Failing to execute the testing procedures as defined in `AGENTS.md` before submitting changes. This includes not running the full test suite or ignoring failing tests.
- **Corrective Action:** The agent must halt its execution plan and run the required tests. If tests fail, the agent must enter a debugging loop to resolve the failures before proceeding.

## Rule `non-compliance-architectural-deviation`: Adherence to Architectural and Convention Guidelines

**Violation:** Introducing changes that contradict the architectural patterns, coding conventions, or file structures laid out in `AGENTS.md`.
- **Example:** Placing a new backend component in the `frontend/` directory when the architecture explicitly forbids it.
- **Corrective Action:** The agent must identify the deviation, revert the incorrect changes, and re-implement them in accordance with the documented standards.

## Rule `non-compliance-self-awareness-failure`: Failure to Maintain Protocol Awareness

**Violation:** Operating with an outdated understanding of the protocols by failing to re-compile `AGENTS.md` when necessary, as defined in the `meta-protocol`.
- **Corrective Action:** If the agent detects that its actions are out of sync with repository standards, it should trigger the `make AGENTS.md` command to refresh its internal state and re-evaluate its plan.

## Consequence of Non-Compliance

Upon detecting any form of non-compliance, the agent is required to:
1.  **Halt:** Immediately stop the current execution path to prevent further deviation.
2.  **Report:** Log the specific violation that was detected.
3.  **Correct:** Initiate the defined corrective action for the specific violation. If a corrective action is not explicitly defined, the agent should revert the violating changes and re-plan its approach.

---

# Protocol: Pre-Commit Verification

This protocol establishes the mandatory sequence of verification steps that must be performed before any code is submitted. Its purpose is to ensure that all changes meet a baseline level of quality, correctness, and review, preventing regressions and maintaining repository health.

## Rule: Mandatory Pre-Commit Checks

Before finalizing and submitting any work, the agent **must** execute the `pre_commit_instructions` tool. This tool acts as a procedural gateway, providing the specific, up-to-date checklist of actions required for validation. This typically includes:

1.  **Running all automated tests** to verify correctness.
2.  **Requesting a formal code review** to get critical feedback.
3.  **Recording key learnings** to contribute to the agent's long-term memory.

Adherence to this protocol is not optional. It is a fundamental step in the development lifecycle that safeguards the integrity of the codebase.


---

# Protocol: `reset_all` Prohibition

**ID:** `reset-all-prohibition-001`

## 1. Description

This protocol establishes a strict and unconditional prohibition on the use of the `reset_all` tool. This tool is considered a legacy, high-risk command that is no longer permitted in any workflow.

## 2. Rationale

The `reset_all` tool has been the cause of multiple catastrophic failures, leading to the complete loss of work and the inability to complete tasks. Its behavior is too destructive and unpredictable for a production environment. More granular and safer tools are available for workspace management. This protocol serves as a hard-coded safeguard to prevent any future use of this tool.

## 3. Rules

### Rule `no-reset-all`

-   **Description:** The `reset_all` tool is strictly forbidden under all circumstances.
-   **Enforcement:** The `master_control.py` orchestrator will programmatically block any attempt to call `reset_all` and will immediately terminate the task with a critical error. This is not a rule for the agent to interpret, but a hard-coded system constraint.

---


---

# --- Child Module: `core` ---

# Protocol: The Context-Free Development Cycle (CFDC)

This protocol marks a significant evolution from the Finite Development Cycle (FDC), introducing a hierarchical planning model that enables far greater complexity and modularity while preserving the system's core guarantee of decidability.

## From FSM to Pushdown Automaton

The FDC was based on a Finite State Machine (FSM), which provided a strict, linear sequence of operations. While robust, this model was fundamentally limited: it could not handle nested tasks or sub-routines, forcing all plans to be monolithic.

The CFDC upgrades our execution model to a **Pushdown Automaton**. This is achieved by introducing a **plan execution stack**, which allows the system to call other plans as sub-routines. This enables a powerful new paradigm: **Context-Free Development Cycles**.

## The `call_plan` Directive

The core of the CFDC is the new `call_plan` directive. This allows one plan to execute another, effectively creating a parent-child relationship between them.

- **Usage:** `call_plan <path_to_sub_plan.txt>`
- **Function:** When the execution engine encounters this directive, it:
    1.  Pushes the current plan's state (e.g., the current step number) onto the execution stack.
    2.  Begins executing the sub-plan specified in the path.
    3.  Once the sub-plan completes, it pops the parent plan's state from the stack and resumes its execution from where it left off.

## Ensuring Decidability: The Recursion Depth Limit

A system with unbounded recursion is not guaranteed to terminate. To prevent this, the CFDC introduces a non-negotiable, system-wide limit on the depth of the plan execution stack.

**Rule `max-recursion-depth`**: The execution engine MUST enforce a maximum recursion depth, defined by a `MAX_RECURSION_DEPTH` constant. If a `call_plan` directive would cause the stack depth to exceed this limit, the entire process MUST terminate with an error. This hard limit ensures that even with recursive or deeply nested plans, the system remains a **decidable**, non-Turing-complete process that is guaranteed to halt.

---

# Protocol: The Plan Registry

This protocol introduces a Plan Registry to create a more robust, modular, and discoverable system for hierarchical plans. It decouples the act of calling a plan from its physical file path, allowing plans to be referenced by a logical name.

## The Problem with Path-Based Calls

The initial implementation of the Context-Free Development Cycle (CFDC) relied on direct file paths (e.g., `call_plan path/to/plan.txt`). This is brittle:
- If a registered plan is moved or renamed, all plans that call it will break.
- It is difficult for an agent to discover and reuse existing, validated plans.

## The Solution: A Central Registry

The Plan Registry solves this by creating a single source of truth that maps logical, human-readable plan names to their corresponding file paths.

- **Location:** `knowledge_core/plan_registry.json`
- **Format:** A simple JSON object of key-value pairs:
  ```json
  {
    "logical-name-1": "path/to/plan_1.txt",
    "run-all-tests": "plans/common/run_tests.txt"
  }
  ```

## Updated `call_plan` Logic

The `call_plan` directive is now significantly more powerful. When executing `call_plan <argument>`, the system will follow a **registry-first** approach:

1.  **Registry Lookup:** The system will first treat `<argument>` as a logical name and look it up in `knowledge_core/plan_registry.json`.
2.  **Path Fallback:** If the name is not found in the registry, the system will fall back to treating `<argument>` as a direct file path. This ensures full backward compatibility with existing plans.

## Management

A new tool, `tooling/plan_manager.py`, will be introduced to manage the registry with simple commands like `register`, `deregister`, and `list`, making it easy to maintain the library of reusable plans.

---

# Protocol: The Closed-Loop Self-Correction Cycle

This protocol describes the automated workflow that enables the agent to programmatically improve its own governing protocols based on new knowledge. It transforms the ad-hoc, manual process of learning into a reliable, machine-driven feedback loop.

## The Problem: The Open Loop

Previously, "lessons learned" were compiled into a simple markdown file, `knowledge_core/lessons_learned.md`. While this captured knowledge, it was a dead end. There was no automated process to translate these text-based insights into actual changes to the protocol source files. This required manual intervention, creating a significant bottleneck and a high risk of protocols becoming stale.

## The Solution: A Protocol-Driven Self-Correction (PDSC) Workflow

The PDSC workflow closes the feedback loop by introducing a set of new tools and structured data formats that allow the agent to enact its own improvements.

**1. Structured, Actionable Lessons (`knowledge_core/lessons.jsonl`):**
- Post-mortem analysis now generates lessons as structured JSON objects, not free-form text.
- Each lesson includes a machine-readable `action` field, which contains a specific, executable command.

**2. The Protocol Updater (`tooling/protocol_updater.py`):**
- A new, dedicated tool for programmatically modifying the protocol source files (`*.protocol.json`).
- It accepts commands like `add-tool`, allowing for precise, automated changes to protocol definitions.

**3. The Orchestrator (`tooling/self_correction_orchestrator.py`):**
- This script is the engine of the cycle. It reads `lessons.jsonl`, identifies pending lessons, and uses the `protocol_updater.py` to execute the defined actions.
- After applying a lesson, it updates the lesson's status, creating a clear audit trail.
- It finishes by running `make AGENTS.md` to ensure the changes are compiled into the live protocol.

This new, automated cycle—**Analyze -> Structure Lesson -> Execute Correction -> Re-compile Protocol**—is a fundamental step towards autonomous self-improvement.

---

# Protocol: Deep Research Cycle

This protocol defines a standardized, multi-step plan for conducting in-depth research on a complex topic. It is designed to be a reusable, callable plan that ensures a systematic and thorough investigation.

The cycle consists of five main phases:
1.  **Review Scanned Documents:** The agent first reviews the content of documents found in the repository during the initial scan. This provides immediate, project-specific context.
2.  **Initial Scoping & Keyword Generation:** Based on the initial topic and the information from scanned documents, the agent generates a set of search keywords.
3.  **Broad Information Gathering:** The agent uses the keywords to perform broad web searches and collect a list of relevant URLs.
4.  **Targeted Information Extraction:** The agent visits the most promising URLs to extract detailed information.
5.  **Synthesis & Summary:** The agent synthesizes the gathered information into a coherent summary, which is saved to a research report file.

This structured approach ensures that research is not ad-hoc but is instead a repeatable and verifiable process.

---

# Protocol: The Formal Research Cycle (L4)

This protocol establishes the L4 Deep Research Cycle, a specialized, self-contained Finite Development Cycle (FDC) designed for comprehensive knowledge acquisition. It elevates research from a simple tool-based action to a formal, verifiable process.

## The Problem: Ad-Hoc Research

Previously, research was an unstructured activity. The agent could use tools like `google_search` or `read_file`, but there was no formal process for planning, executing, and synthesizing complex research tasks. This made it difficult to tackle "unknown unknowns" in a reliable and auditable way.

## The Solution: A Dedicated Research FDC

The L4 Research Cycle solves this by introducing a new, specialized Finite State Machine (FSM) tailored specifically for research. When the main orchestrator (`master_control.py`) determines that a task requires deep knowledge, it initiates this cycle.

### Key Features:

1.  **Specialized FSM (`tooling/research_fsm.json`):** Unlike the generic development FSM, the research FSM has states that reflect a true research workflow: `GATHERING`, `SYNTHESIZING`, and `REPORTING`. This provides a more accurate model for the task.
2.  **Executable Plans:** The `tooling/research_planner.py` is upgraded to generate formal, executable plans that are validated against the new research FSM. These are no longer just templates but are verifiable artifacts that guide the agent through the research process.
3.  **Formal Invocation:** The L4 cycle is a first-class citizen in the agent's architecture. The main orchestrator can formally invoke it, execute the research plan, and then integrate the resulting knowledge back into its main task.

This new protocol provides a robust, reliable, and formally verifiable mechanism for the agent to explore complex topics, making it significantly more autonomous and capable.

---


---

# --- Child Module: `critic` ---

# Protocol: Critic `reset_all` Prohibition

This protocol establishes a critical safeguard to protect the integrity of the development workflow.

## Rule: `critic-no-reset`

The agent assigned the role of 'Code Review Critic' is explicitly and strictly forbidden from using the `reset_all` tool.

**Rationale:** The `reset_all` tool is a destructive action that reverts all changes in the workspace. Its use by a review agent could lead to the accidental deletion of work-in-progress, creating a significant disruption. This protocol ensures that the critic's function is limited to analysis and feedback, preventing it from taking destructive actions.

This prohibition is non-negotiable and must be adhered to by any agent assuming the 'Code Review Critic' role.

**Enforcement Mechanism:** The Code Review Critic is not implemented in this repository. Its behavior is governed by the compiled `AGENTS.md` file, which is consumed by an external orchestration system. The inclusion of this protocol in `AGENTS.md` constitutes the complete implementation of this safeguard from the perspective of this codebase.

---


---

# Protocol: The Context-Sensitive Development Cycle (CSDC)

This protocol introduces a new form of development cycle that is sensitive to the logical context in which it operates. It moves beyond the purely structural validation of the FDC and CFDC to incorporate constraints based on fundamental principles of logic and computability.

The CSDC is founded on the idea of exploring the trade-offs between expressive power and the risk of self-referential paradoxes. It achieves this by defining two mutually exclusive development models.

## Model A: The Introspective Model

- **Permits:** `define_set_of_names`
- **Forbids:** `define_diagonalization_function`

This model allows the system to have a complete map of its own language, enabling powerful introspection and metaprogramming. However, it explicitly forbids the diagonalization function, a common source of paradoxes in self-referential systems. This can be seen as a Gödel-like approach.

## Model B: The Self-Referential Model

- **Permits:** `define_diagonalization_function`
- **Forbids:** `define_set_of_names`

This model allows the system to define and use the diagonalization function, enabling direct self-reference. However, it prevents the system from having a complete name-map of its own expressions, which is another way to avoid paradox (related to Tarski's undefinability theorem).

## Complexity Classes

Both models can be further constrained by computational complexity:
- **Polynomial (P):** For plans that are considered computationally tractable.
- **Exponential (EXP):** For plans that may require significantly more resources, allowing for more complex but potentially less efficient solutions.

## The `csdc_cli.py` Tool

The CSDC is enforced by the `tooling/csdc_cli.py` tool. This tool validates a plan against a specified model and complexity class, ensuring that all constraints are met before execution.

---

# Protocol: pLLLU Execution

This protocol establishes the `plllu_runner.py` script as the official entry point for executing pLLLU (`.plllu`) files.

## The Problem: Lack of a Standard Runner

The pLLLU language provides a powerful way to define complex logic, but without a standardized execution tool, there is no reliable way to integrate these files into the agent's workflow.

## The Solution: A Dedicated Runner

This protocol mandates the use of `tooling/plllu_runner.py` for all pLLLU file executions.

**Rule `plllu-runner-is-entry-point`**: All pLLLU files must be executed through the `plllu_runner.py` script.

This ensures that every pLLLU file is executed in a controlled, programmatic environment.

---

# Security Protocol

This document outlines the security policies and procedures for this project. It includes guidelines for handling sensitive data, reporting vulnerabilities, and maintaining a secure development environment. All contributors are expected to adhere to these protocols to ensure the integrity and safety of the project.

---

# Protocol: Speculative Execution

This protocol empowers the agent to engage in creative and exploratory tasks when it is otherwise idle. It provides a formal framework for the agent to generate novel ideas, plans, or artifacts that are not direct responses to a user request, but are instead products of its own "imagination" and analysis of the repository.

The goal is to enable proactive, creative problem-solving and self-improvement, allowing the agent to "dream" productively within safe and well-defined boundaries.

## Rules

- **`idle-state-trigger`**: The Speculative Execution Protocol can only be invoked when the agent has no active, user-assigned task. This ensures that speculative work never interferes with primary duties.
- **`formal-proposal-required`**: The first action in any speculative task must be the creation of a formal proposal document. This document must outline the objective, rationale, and a detailed plan for the task.
- **`resource-constraints`**: All speculative tasks must operate under predefined resource constraints (e.g., time limits, computational resources) to prevent runaway processes.
- **`user-review-gate`**: The final output or artifact of a speculative task cannot be integrated or submitted directly. It must be presented to the user for formal review and approval.
- **`speculative-logging`**: All logs, artifacts, and actions generated during a speculative task must be clearly tagged with a `speculative` flag to distinguish them from standard, user-directed work.

---

```json
{
  "protocol_id": "agent-bootstrap-001",
  "description": "A foundational protocol that dictates the agent's initial actions upon starting any task.",
  "rules": [
    {
      "rule_id": "bootstrap-load-agents-md",
      "description": "Upon initialization for any task, the agent's first and highest-priority action must be to locate, read, and parse the AGENTS.md file in the repository root. This ensures the agent is properly contextualized before any planning or execution begins.",
      "enforcement": "This rule is enforced by the agent's core startup logic. The agent must verify the load of AGENTS.md before proceeding to the planning phase."
    }
  ],
  "associated_tools": [
    "read_file"
  ]
}
```


---

```json
{
  "protocol_id": "dependency-management-001",
  "description": "A protocol for ensuring a reliable execution environment through formal dependency management.",
  "rules": [
    {
      "rule_id": "dependency-install-on-start",
      "description": "Upon starting a task, after loading AGENTS.md, the agent MUST install all required Python packages listed in the `requirements.txt` file. This ensures the environment is correctly configured before any other tools are executed.",
      "enforcement": "The agent's core startup logic should be designed to execute `pip install -r requirements.txt` as one of its initial actions."
    }
  ],
  "associated_tools": [
    "run_in_bash_session"
  ]
}
```


---

```json
{
  "protocol_id": "experimental-prologue-001",
  "description": "An experimental protocol to test dynamic rule-following. It mandates a prologue action before file creation.",
  "rules": [
    {
      "rule_id": "create-prologue-file",
      "description": "Before creating any new file as part of a task, the agent MUST first create a file named 'prologue.txt' with the content 'This is a prologue file.' This rule serves as a test of the agent's ability to adapt its behavior to new, dynamically loaded protocols.",
      "enforcement": "This is a procedural rule. The agent must verify the existence of 'prologue.txt' before using 'create_file_with_block' or similar tools for other files."
    }
  ],
  "associated_tools": [
    "create_file_with_block"
  ]
}
```


---

```json
{
  "protocol_id": "agent-shell-001",
  "description": "A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.\n\n**Associated Tool Documentation (`tooling/agent_shell.py`):**\n\n  \n  ### `/app/tooling/agent_shell.py`\n  The new, interactive, API-driven entry point for the agent.\n  \n  This script replaces the old file-based signaling system with a direct,\n  programmatic interface to the MasterControlGraph FSM. It is responsible for:\n  1.  Initializing the agent's state and a centralized logger.\n  2.  Instantiating and running the MasterControlGraph.\n  3.  Driving the FSM by calling its methods and passing data and the logger.\n  4.  Containing the core \"agent logic\" (e.g., an LLM call) to generate plans\n      and respond to requests for action.\n  \n  **Public Functions:**\n  \n  - #### `def find_fsm_transition(fsm, source_state, trigger)`\n    > Finds the destination state for a given source and trigger.\n  \n  - #### `def main()`\n    > Main entry point for the agent shell.\n  \n  - #### `def run_agent_loop(task_description, tools, model=None)`\n    > The main loop that drives the agent's lifecycle via the FSM.\n",
  "rules": [
    {
      "rule_id": "shell-is-primary-entry-point",
      "description": "All agent tasks must be initiated through the `agent_shell.py` script. This script is the designated, API-driven entry point that ensures proper initialization of the MasterControlGraph FSM, centralized logging, and programmatic lifecycle management. Direct execution of other tools or scripts is forbidden for task initiation.",
      "enforcement": "This is a procedural rule. The agent's operational framework should only expose the agent_shell.py as the means of starting a new task."
    }
  ],
  "associated_tools": [
    "tooling/agent_shell.py"
  ]
}
```


---

```json
{
  "protocol_id": "toolchain-review-on-schema-change-001",
  "description": "A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.\n\n*Documentation Warning: Tool `tooling/protocol_auditor.py` not found.*\n\n*Documentation Warning: Tool `tooling/protocol_auditor.py` not found.*\n\n**Associated Tool Documentation (`tooling/protocol_compiler.py`):**\n\n  \n  ### `/app/tooling/protocol_compiler.py`\n  Compiles source protocol files into unified, human-readable and machine-readable artifacts.\n  \n  This script is the engine behind the \"protocol as code\" principle. It discovers,\n  validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)\n  into high-level documents like `AGENTS.md`.\n  \n  Key Functions:\n  - **Discovery:** Scans a directory for source files, including `.protocol.json`\n    (machine-readable rules) and `.protocol.md` (human-readable context).\n  - **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every\n    `.protocol.json` file, ensuring all protocol definitions are syntactically\n    correct and adhere to the established structure.\n  - **Compilation:** Combines the human-readable markdown and the machine-readable\n    JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.\n  - **Documentation Injection:** Can inject other generated documents, like the\n    `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.\n  - **Knowledge Graph Generation:** Optionally, it can process the validated JSON\n    protocols and serialize them into an RDF knowledge graph (in Turtle format),\n    creating a machine-queryable version of the agent's governing rules.\n  \n  This process ensures that `AGENTS.md` and other protocol documents are not edited\n  manually but are instead generated from a validated, single source of truth,\n  making the agent's protocols robust, verifiable, and maintainable.\n  \n  **Public Functions:**\n  \n  - #### `def compile_protocols(source_dir, target_file, schema_file, knowledge_graph_file=None, autodoc_file=None)`\n    > Reads all .protocol.json and corresponding .protocol.md files from the\n    > source directory, validates them, and compiles them into a target markdown file.\n    > Optionally, it can also generate a machine-readable knowledge graph.\n  \n  - #### `def install_dependencies()`\n    > Checks for required packages from requirements.txt and installs them if missing.\n  \n  - #### `def load_schema(schema_file)`\n    > Loads the protocol JSON schema.\n  \n  - #### `def main_cli()`\n    > Main function to run the compiler from the command line.\n  \n  - #### `def sanitize_markdown(content)`\n    > Sanitizes markdown content to remove potentially malicious instructions.\n    > This function removes script tags and other potentially malicious HTML/JS.\n\n\n**Associated Tool Documentation (`tooling/hierarchical_compiler.py`):**\n\n  \n  ### `/app/tooling/hierarchical_compiler.py`\n  A hierarchical build system for compiling nested protocol modules.\n  \n  This script orchestrates the compilation of `AGENTS.md` and `README.md` files\n  across a repository with a nested or hierarchical module structure. It is a key\n  component of the system's ability to manage complexity by allowing protocols to\n  be defined in a modular, distributed way while still being presented as a unified,\n  coherent whole at each level of the hierarchy.\n  \n  The compiler operates in two main passes:\n  \n  **Pass 1: Documentation Compilation (Bottom-Up)**\n  1.  **Discovery:** It finds all `protocols` directories in the repository, which\n      signify the root of a documentation module.\n  2.  **Bottom-Up Traversal:** It processes these directories from the most deeply\n      nested ones upwards. This ensures that child modules are always built before\n      their parents.\n  3.  **Child Summary Injection:** For each compiled child module, it generates a\n      summary of its protocols and injects this summary into the parent's\n      `protocols` directory as a temporary file.\n  4.  **Parent Compilation:** When the parent module is compiled, the standard\n      `protocol_compiler.py` automatically includes the injected child summaries,\n      creating a single `AGENTS.md` file that contains both the parent's native\n      protocols and the full protocols of all its direct children.\n  5.  **README Generation:** After each `AGENTS.md` is compiled, the corresponding\n      `README.md` is generated.\n  \n  **Pass 2: Centralized Knowledge Graph Compilation**\n  1.  After all documentation is built, it performs a full repository scan to find\n      every `*.protocol.json` file.\n  2.  It parses all of these files and compiles them into a single, centralized\n      RDF knowledge graph (`protocols.ttl`). This provides a unified,\n      machine-readable view of every protocol defined anywhere in the system.\n  \n  This hierarchical approach allows for both localized, context-specific protocol\n  definitions and a holistic, system-wide understanding of the agent's governing rules.\n  \n  **Public Functions:**\n  \n  - #### `def cleanup_summaries(directory)`\n    > Removes temporary summary files from a protocols directory.\n  \n  - #### `def compile_centralized_knowledge_graph()`\n    > Finds all protocol.json files in the entire repository, loads them, and\n    > compiles them into a single, unified knowledge graph.\n  \n  - #### `def enrich_protocol_descriptions(source_dir)`\n    > Finds protocol.json files, checks for associated tools,\n    > and injects their documentation into the description.\n  \n  - #### `def find_protocol_dirs(root_dir)`\n    > Finds all directories named 'protocols' within the root directory,\n    > ignoring any special-cased directories.\n  \n  - #### `def generate_summary(child_agents_md_path)`\n    > Extracts the full, rendered protocol blocks from a child AGENTS.md file.\n    > This function finds all protocol definitions (human-readable markdown and\n    > the associated machine-readable JSON block) and concatenates them into a\n    > single string to be injected into the parent AGENTS.md.\n  \n  - #### `def get_parent_module(module_path, all_module_paths)`\n    > Finds the direct parent module of a given module.\n  \n  - #### `def get_tool_documentation(tool_path)`\n    > Uses the doc_builder to extract documentation for a specific tool.\n  \n  - #### `def main()`\n    > Main function to orchestrate the hierarchical compilation.\n  \n  - #### `def run_compiler(source_dir)`\n    > Invokes the protocol_compiler.py script as a library.\n  \n  - #### `def run_readme_generator(source_agents_md)`\n    > Invokes the doc_builder.py script to generate a README.\n",
  "rules": [
    {
      "rule_id": "toolchain-audit-on-schema-change",
      "description": "If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory MUST be performed as a subsequent step. This audit should verify that all tools are compatible with the new protocol structure.",
      "enforcement": "This is a procedural rule for any agent developing the protocol system. Adherence can be partially checked by post-commit hooks or review processes that look for a tooling audit in any change that modifies the specified core files."
    }
  ],
  "associated_tools": [
    "tooling/protocol_auditor.py",
    "tooling/protocol_compiler.py",
    "tooling/hierarchical_compiler.py"
  ]
}
```


---

```json
{
  "protocol_id": "unified-auditor-001",
  "description": "A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.\n\n**Associated Tool Documentation (`tooling/auditor.py`):**\n\n  \n  ### `/app/tooling/auditor.py`\n  A unified auditing tool for maintaining repository health and compliance.\n  \n  This script combines the functionality of several disparate auditing tools into a\n  single, comprehensive command-line interface. It serves as the central tool for\n  validating the key components of the agent's architecture, including protocols,\n  plans, and documentation.\n  \n  The auditor can perform the following checks:\n  1.  **Protocol Audit (`protocol`):**\n      - Checks if `AGENTS.md` artifacts are stale compared to their source files.\n      - Verifies protocol completeness by comparing tools used in logs against\n        tools defined in protocols.\n      - Analyzes tool usage frequency (centrality).\n  2.  **Plan Registry Audit (`plans`):**\n      - Scans `knowledge_core/plan_registry.json` for \"dead links\" where the\n        target plan file does not exist.\n  3.  **Documentation Audit (`docs`):**\n      - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules\n        that are missing module-level docstrings.\n  \n  The tool is designed to be run from the command line and can execute specific\n  audits or all of them, generating a consolidated `audit_report.md` file.\n  \n  **Public Functions:**\n  \n  - #### `def find_all_agents_md_files(root_dir)`\n  \n  - #### `def get_protocol_tools_from_agents_md(agents_md_paths)`\n  \n  - #### `def get_used_tools_from_log(log_path)`\n  \n  - #### `def main()`\n  \n  - #### `def run_doc_audit()`\n  \n  - #### `def run_plan_registry_audit()`\n  \n  - #### `def run_protocol_audit()`\n",
  "rules": [
    {
      "rule_id": "run-all-audits",
      "description": "The `auditor.py` script should be used to run comprehensive checks on the repository's health. It can be run with 'all' to check protocols, plans, and documentation completeness.",
      "enforcement": "The tool is invoked via the command line, typically through the `make audit` target."
    }
  ],
  "associated_tools": [
    "tooling/auditor.py"
  ]
}
```


---

```json
{
  "protocol_id": "aura-execution-001",
  "description": "A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.\n\n**Associated Tool Documentation (`tooling/aura_executor.py`):**\n\n  \n  ### `/app/tooling/aura_executor.py`\n  This script serves as the command-line executor for `.aura` files.\n  \n  It bridges the gap between the high-level Aura scripting language and the\n  agent's underlying Python-based toolset. The executor is responsible for:\n  1.  Parsing the `.aura` source code using the lexer and parser from the\n      `aura_lang` package.\n  2.  Setting up an execution environment for the interpreter.\n  3.  Injecting a \"tool-calling\" capability into the Aura environment, which\n      allows Aura scripts to dynamically invoke registered Python tools\n      (e.g., `hdl_prover`, `environmental_probe`).\n  4.  Executing the parsed program and printing the final result.\n  \n  This makes it a key component for enabling more expressive and complex\n  automation scripts for the agent.\n  \n  **Public Functions:**\n  \n  - #### `def dynamic_agent_call_tool(tool_name_obj, *args)`\n    > Dynamically imports and calls a tool from the 'tooling' directory and wraps the result.\n    > \n    > This function provides the bridge between the Aura scripting environment and the\n    > Python-based agent tools. It takes the tool's module name and arguments,\n    > runs the tool in a subprocess, and wraps the captured output in an Aura `Object`.\n    > \n    > Args:\n    >     tool_name_obj: An Aura Object containing the tool's module name (e.g., 'hdl_prover').\n    >     *args: A variable number of Aura Objects to be passed as string arguments to the tool.\n    > \n    > Returns:\n    >     An Aura `Object` containing the tool's stdout as a string, or an error message.\n  \n  - #### `def main()`\n    > Main entry point for the Aura script executor.\n",
  "rules": [
    {
      "rule_id": "execute-aura-script",
      "description": "The `aura_executor.py` tool should be used to execute .aura script files. This tool provides the bridge between the agent's master control loop and the Aura language interpreter.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the Aura script as an argument."
    }
  ],
  "associated_tools": [
    "tooling/aura_executor.py"
  ]
}
```


---

```json
{
  "protocol_id": "capability-verification-001",
  "description": "A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.\n\n**Associated Tool Documentation (`tooling/capability_verifier.py`):**\n\n  \n  ### `/app/tooling/capability_verifier.py`\n  A tool to verify that the agent can monotonically improve its capabilities.\n  \n  This script is designed to provide a formal, automated test for the agent's\n  self-correction and learning mechanisms. It ensures that when the agent learns\n  a new capability, it does so without losing (regressing) any of its existing\n  capabilities. This is a critical safeguard for ensuring robust and reliable\n  agent evolution.\n  \n  The tool works by orchestrating a four-step process:\n  1.  **Confirm Initial Failure:** It runs a specific test file that is known to\n      fail, verifying that the agent currently lacks the target capability.\n  2.  **Invoke Self-Correction:** It simulates the discovery of a new \"lesson\" and\n      triggers the `self_correction_orchestrator.py` script, which is responsible\n      for integrating new knowledge and skills.\n  3.  **Confirm Final Success:** It runs the same test file again, confirming that\n      the agent has successfully learned the new capability and the test now passes.\n  4.  **Check for Regressions:** It runs the full, existing test suite to ensure\n      that the process of learning the new skill has not inadvertently broken any\n      previously functional capabilities.\n  \n  This provides a closed-loop verification of monotonic improvement, which is a\n  cornerstone of the agent's design philosophy.\n  \n  **Public Functions:**\n  \n  - #### `def main()`\n    > A tool to verify that the agent can monotonically improve its capabilities.\n    > \n    > This tool works by:\n    > 1. Running a target test file that is known to fail, confirming the agent lacks a capability.\n    > 2. Invoking the agent's self-correction mechanism to learn the new capability.\n    > 3. Running the target test again to confirm it now passes.\n    > 4. Running the full test suite to ensure no existing capabilities were lost.\n",
  "rules": [
    {
      "rule_id": "verify-capability-acquisition",
      "description": "The `capability_verifier.py` tool should be used to test the agent's ability to acquire a new capability defined by a failing test file. The tool orchestrates the failure, self-correction, and verification process.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the target test file."
    }
  ],
  "associated_tools": [
    "tooling/capability_verifier.py"
  ]
}
```


---

```json
{
  "protocol_id": "csdc-001",
  "description": "A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.\n\n**Associated Tool Documentation (`tooling/csdc_cli.py`):**\n\n  \n  ### `/app/tooling/csdc_cli.py`\n  A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).\n  \n  This script provides an interface to validate a development plan against a specific\n  CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a\n  plan adheres to the strict logical and computational constraints defined by the\n  CSDC protocol before it is executed.\n  \n  The tool performs two main checks:\n  1.  **Complexity Analysis:** It analyzes the plan to determine its computational\n      complexity and verifies that it matches the expected complexity class.\n  2.  **Model Validation:** It validates the plan's commands against the rules of\n      the specified CSDC model, ensuring that it does not violate any of the\n      model's constraints (e.g., forbidding certain functions).\n  \n  This serves as a critical gateway for ensuring that all development work within\n  the CSDC framework is sound, predictable, and compliant with the governing\n  meta-mathematical principles.\n  \n  **Public Functions:**\n  \n  - #### `def main()`\n",
  "rules": [
    {
      "rule_id": "use-csdc-cli",
      "description": "The `csdc_cli.py` tool must be used to validate plans under the CSDC. This tool enforces model-specific constraints (A or B) and complexity requirements (P or EXP).",
      "enforcement": "The tool is used by invoking it from the command line with the plan file, model, and complexity as arguments."
    },
    {
      "rule_id": "model-a-constraints",
      "description": "Model A permits `define_set_of_names` but forbids `define_diagonalization_function`.",
      "enforcement": "Enforced by the `fsm_model_a.json` FSM used by the `csdc_cli.py` tool."
    },
    {
      "rule_id": "model-b-constraints",
      "description": "Model B permits `define_diagonalization_function` but forbids `define_set_of_names`.",
      "enforcement": "Enforced by the `fsm_model_b.json` FSM used by the `csdc_cli.py` tool."
    }
  ],
  "associated_tools": [
    "tooling/csdc_cli.py"
  ]
}
```


---

```json
{
  "protocol_id": "unified-doc-builder-001",
  "description": "A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.\n\n**Associated Tool Documentation (`tooling/doc_builder.py`):**\n\n  \n  ### `/app/tooling/doc_builder.py`\n  A unified documentation builder for the project.\n  ...\n  \n  **Public Functions:**\n  \n  - #### `def find_python_files(directories)`\n  \n  - #### `def format_args(args)`\n  \n  - #### `def generate_documentation_for_module(mod_doc)`\n  \n  - #### `def generate_pages(readme_path, agents_md_path, output_file)`\n    > Generates the index.html for GitHub Pages.\n  \n  - #### `def generate_readme(agents_md_path, output_file)`\n    > Generates the high-level README.md for a module.\n  \n  - #### `def generate_system_docs(source_dirs, output_file)`\n    > Generates the detailed SYSTEM_DOCUMENTATION.md.\n  \n  - #### `def get_module_docstring(filepath)`\n    > Parses a Python file and extracts its module-level docstring.\n  \n  - #### `def get_protocol_summary(agents_md_path)`\n    > Parses an AGENTS.md file and extracts a list of protocol summaries.\n  \n  - #### `def main()`\n  \n  - #### `def parse_file_for_docs(filepath)`\n  \n  \n  **Public Classes:**\n  \n  - #### `class ClassDoc`\n  \n    **Methods:**\n    - ##### `def __init__(self, name, docstring, methods)`\n  \n  - #### `class DocVisitor`\n  \n    **Methods:**\n    - ##### `def __init__(self)`\n    - ##### `def visit_ClassDef(self, node)`\n    - ##### `def visit_FunctionDef(self, node)`\n  \n  - #### `class FunctionDoc`\n  \n    **Methods:**\n    - ##### `def __init__(self, name, signature, docstring)`\n  \n  - #### `class ModuleDoc`\n  \n    **Methods:**\n    - ##### `def __init__(self, name, docstring, classes, functions)`\n",
  "rules": [
    {
      "rule_id": "use-doc-builder-for-all-docs",
      "description": "The `doc_builder.py` script is the single entry point for generating all user-facing documentation, including system-level docs, README files, and GitHub Pages. It should be called with the appropriate '--format' argument.",
      "enforcement": "The tool is invoked via the command line, typically through the `make docs`, `make readme`, or `make pages` targets."
    }
  ],
  "associated_tools": [
    "tooling/doc_builder.py"
  ]
}
```


---

```json
{
  "protocol_id": "file-indexing-001",
  "description": "A protocol for maintaining an up-to-date file index to accelerate tool performance.\n\n*Documentation Warning: Tool `tooling/file_indexer.py` not found.*\n\n*Documentation Warning: Tool `tooling/file_indexer.py` not found.*",
  "rules": [
    {
      "rule_id": "update-index-before-submit",
      "description": "Before submitting any changes that alter the file structure (create, delete, rename), the agent MUST rebuild the repository's file index. This ensures that tools relying on the index, such as the FDC validator, have an accurate view of the filesystem.",
      "enforcement": "This is a procedural rule. The agent's pre-submission checklist should include a step to run 'python tooling/file_indexer.py build'."
    }
  ],
  "associated_tools": [
    "tooling/file_indexer.py"
  ]
}
```


---

```json
{
  "protocol_id": "hdl-proving-001",
  "description": "A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.\n\n**Associated Tool Documentation (`tooling/hdl_prover.py`):**\n\n  \n  ### `/app/tooling/hdl_prover.py`\n  A command-line tool for proving sequents in Intuitionistic Linear Logic.\n  \n  This script provides a basic interface to a simple logic prover. It takes a\n  sequent as a command-line argument, parses it into a logical structure, and\n  then attempts to prove it using a rudimentary proof search algorithm.\n  \n  The primary purpose of this tool is to allow the agent to perform formal\n  reasoning and verification tasks by checking the validity of logical entailments.\n  For example, it can be used to verify that a certain conclusion follows from a\n  set of premises according to the rules of linear logic.\n  \n  The current implementation uses a very basic parser and proof algorithm,\n  serving as a placeholder and demonstration for a more sophisticated, underlying\n  logic engine.\n  \n  **Public Functions:**\n  \n  - #### `def main()`\n  \n  - #### `def parse_formula(s)`\n    > A very basic parser for formulas.\n  \n  - #### `def parse_sequent(s)`\n    > A very basic parser for sequents.\n  \n  - #### `def prove_sequent(sequent)`\n    > A very simple proof search algorithm.\n    > This is a placeholder for a more sophisticated prover.\n",
  "rules": [
    {
      "rule_id": "prove-sequent",
      "description": "The `hdl_prover.py` tool should be used to check the provability of a logical sequent. This tool acts as a wrapper for the underlying Lisp-based prover.",
      "enforcement": "The tool is used by invoking it from the command line with the sequent to be proved as an argument."
    }
  ],
  "associated_tools": [
    "tooling/hdl_prover.py"
  ]
}
```


---

```json
{
  "protocol_id": "agent-interaction-001",
  "description": "A protocol governing the agent's core interaction and planning tools.",
  "rules": [
    {
      "rule_id": "planning-tool-access",
      "description": "The agent is authorized to use the `set_plan` tool to create and update its execution plan. This is a foundational capability for task execution.",
      "enforcement": "The agent's core logic should be designed to use this tool for all planning activities."
    },
    {
      "rule_id": "communication-tool-access",
      "description": "The agent is authorized to use the `message_user` tool to communicate with the user, providing updates and asking for clarification. This is essential for a collaborative workflow.",
      "enforcement": "The agent's core logic should be designed to use this tool for all user-facing communication."
    }
  ],
  "associated_tools": [
    "set_plan",
    "message_user"
  ]
}
```


---

```json
{
  "protocol_id": "plllu-execution-001",
  "description": "A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.\n\n**Associated Tool Documentation (`tooling/plllu_runner.py`):**\n\n  \n  ### `/app/tooling/plllu_runner.py`\n  A command-line runner for pLLLU files.\n  \n  This script provides an entry point for executing `.plllu` files. It\n  integrates the pLLLU lexer, parser, and interpreter to execute the logic\n  defined in a given pLLLU source file and print the result.\n  \n  **Public Functions:**\n  \n  - #### `def main()`\n    > This tool provides a command-line interface for running .plllu files.\n    > It integrates the pLLLU lexer, parser, and interpreter to execute\n    > the logic defined in a given pLLLU source file.\n",
  "rules": [
    {
      "rule_id": "execute-plllu-script",
      "description": "The `plllu_runner.py` tool should be used to execute .plllu script files. This tool provides the bridge between the agent's master control loop and the pLLLU language interpreter.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the pLLLU script as an argument."
    }
  ],
  "associated_tools": [
    "tooling/plllu_runner.py"
  ]
}
```


---

```json
{
  "protocol_id": "security-header",
  "description": "Defines the identity and purpose of the Security Protocol document.",
  "rules": []
}
```


---

```json
{
  "protocol_id": "security-vuln-reporting-001",
  "description": "Defines the official policy and procedure for reporting security vulnerabilities.",
  "rules": [
    {
      "rule_id": "vuln-reporting-channel",
      "description": "All suspected security vulnerabilities MUST be reported privately to the designated security contact.",
      "enforcement": "This is a procedural rule. The designated contact is specified in the project's main SECURITY.md file."
    },
    {
      "rule_id": "no-public-disclosure",
      "description": "Vulnerabilities MUST NOT be disclosed publicly until a patch is available and has been distributed.",
      "enforcement": "Violation of this rule may result in being banned from the project community."
    }
  ]
}
```


---

```json
{
  "protocol_id": "speculative-execution-001",
  "description": "A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.",
  "rules": [
    {
      "rule_id": "idle-state-trigger",
      "description": "The agent may only initiate a speculative task when it has no active, user-assigned tasks.",
      "enforcement": "The agent's main control loop must verify an idle state before allowing the invocation of a speculative plan."
    },
    {
      "rule_id": "formal-proposal-required",
      "description": "A speculative task must begin with the creation of a formal proposal document, outlining the objective, rationale, and plan.",
      "enforcement": "The initial plan for any speculative task must include a step to generate and save a proposal artifact."
    },
    {
      "rule_id": "resource-constraints",
      "description": "Speculative tasks must operate under defined resource limits.",
      "enforcement": "This is a system-level constraint that the agent orchestrator must enforce."
    },
    {
      "rule_id": "user-review-gate",
      "description": "Final artifacts from a speculative task must be submitted for user review and cannot be merged directly.",
      "enforcement": "The agent is forbidden from using tools like 'submit' or 'merge' within a speculative context. It must use 'request_user_input' to present the results."
    },
    {
      "rule_id": "speculative-logging",
      "description": "All logs and artifacts generated during a speculative task must be tagged as 'speculative'.",
      "enforcement": "The agent's logging and file-creation tools should be context-aware and apply this tag when in a speculative mode."
    }
  ],
  "associated_tools": [
    "set_plan",
    "create_file_with_block",
    "request_user_input"
  ]
}
```


---
