# ---
# DO NOT EDIT THIS FILE DIRECTLY.
# This file is programmatically generated by the `protocol_compiler.py` script.
# All changes to agent protocols must be made in the source files
# located in the `core/` directory.
#
# This file contains the compiled protocols in a human-readable Markdown format,
# with machine-readable JSON definitions embedded.
# ---

# Protocol: Agent Shell Entry Point

This protocol establishes the `agent_shell.py` script as the sole, official entry point for initiating any and all agent tasks.

## The Problem: Inconsistent Initialization

Prior to this protocol, there was no formally mandated entry point for the agent. This could lead to tasks being initiated through different scripts, potentially bypassing critical setup procedures like FSM initialization, logger configuration, and state management. This inconsistency makes the agent's behavior less predictable and harder to debug.

## The Solution: A Single, Enforced Entry Point

This protocol mandates the use of `tooling/agent_shell.py` for all task initiations.

**Rule `shell-is-primary-entry-point`**: All agent tasks must be initiated through the `agent_shell.py` script.

This ensures that every task begins within a controlled, programmatic environment where:
1.  The MasterControlGraph FSM is correctly instantiated and run.
2.  The centralized logger is initialized for comprehensive, structured logging.
3.  The agent's lifecycle is managed programmatically, not through fragile file-based signals.

By enforcing a single entry point, this protocol enhances the reliability, auditability, and robustness of the entire agent system.

---

# Meta-Protocol: Toolchain Review on Schema Change

This protocol establishes a critical feedback loop to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.

## The Problem: Protocol-Toolchain Desynchronization

A significant process gap was identified where a major architectural change to the protocol system (e.g., the introduction of a hierarchical `AGENTS.md` structure) did not automatically trigger a review of the tools that depend on that structure. The `protocol_auditor.py` tool, for instance, became partially obsolete as it was unaware of the new hierarchical model, leading to incomplete audits. This demonstrates that the agent's tools can become desynchronized from its own governing rules, creating a critical blind spot.

## The Solution: Mandated Toolchain Audit

This protocol closes that gap by introducing a new rule that explicitly links changes in the protocol system's architecture to a mandatory review of the toolchain.

**Rule `toolchain-audit-on-schema-change`**: If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory **must** be performed as a subsequent step.

This ensures that any modification to the fundamental way protocols are defined or processed is immediately followed by a conscious verification that all dependent tools are still functioning correctly and are aware of the new structure. This transforms the previously manual and error-prone discovery process into a formal, required step of the development lifecycle.

---

# --- Child Module: `core` ---

# Protocol: The Context-Free Development Cycle (CFDC)

This protocol marks a significant evolution from the Finite Development Cycle (FDC), introducing a hierarchical planning model that enables far greater complexity and modularity while preserving the system's core guarantee of decidability.

## From FSM to Pushdown Automaton

The FDC was based on a Finite State Machine (FSM), which provided a strict, linear sequence of operations. While robust, this model was fundamentally limited: it could not handle nested tasks or sub-routines, forcing all plans to be monolithic.

The CFDC upgrades our execution model to a **Pushdown Automaton**. This is achieved by introducing a **plan execution stack**, which allows the system to call other plans as sub-routines. This enables a powerful new paradigm: **Context-Free Development Cycles**.

## The `call_plan` Directive

The core of the CFDC is the new `call_plan` directive. This allows one plan to execute another, effectively creating a parent-child relationship between them.

- **Usage:** `call_plan <path_to_sub_plan.txt>`
- **Function:** When the execution engine encounters this directive, it:
    1.  Pushes the current plan's state (e.g., the current step number) onto the execution stack.
    2.  Begins executing the sub-plan specified in the path.
    3.  Once the sub-plan completes, it pops the parent plan's state from the stack and resumes its execution from where it left off.

## Ensuring Decidability: The Recursion Depth Limit

A system with unbounded recursion is not guaranteed to terminate. To prevent this, the CFDC introduces a non-negotiable, system-wide limit on the depth of the plan execution stack.

**Rule `max-recursion-depth`**: The execution engine MUST enforce a maximum recursion depth, defined by a `MAX_RECURSION_DEPTH` constant. If a `call_plan` directive would cause the stack depth to exceed this limit, the entire process MUST terminate with an error. This hard limit ensures that even with recursive or deeply nested plans, the system remains a **decidable**, non-Turing-complete process that is guaranteed to halt.

---

# Protocol: The Plan Registry

This protocol introduces a Plan Registry to create a more robust, modular, and discoverable system for hierarchical plans. It decouples the act of calling a plan from its physical file path, allowing plans to be referenced by a logical name.

## The Problem with Path-Based Calls

The initial implementation of the Context-Free Development Cycle (CFDC) relied on direct file paths (e.g., `call_plan path/to/plan.txt`). This is brittle:
- If a registered plan is moved or renamed, all plans that call it will break.
- It is difficult for an agent to discover and reuse existing, validated plans.

## The Solution: A Central Registry

The Plan Registry solves this by creating a single source of truth that maps logical, human-readable plan names to their corresponding file paths.

- **Location:** `knowledge_core/plan_registry.json`
- **Format:** A simple JSON object of key-value pairs:
  ```json
  {
    "logical-name-1": "path/to/plan_1.txt",
    "run-all-tests": "plans/common/run_tests.txt"
  }
  ```

## Updated `call_plan` Logic

The `call_plan` directive is now significantly more powerful. When executing `call_plan <argument>`, the system will follow a **registry-first** approach:

1.  **Registry Lookup:** The system will first treat `<argument>` as a logical name and look it up in `knowledge_core/plan_registry.json`.
2.  **Path Fallback:** If the name is not found in the registry, the system will fall back to treating `<argument>` as a direct file path. This ensures full backward compatibility with existing plans.

## Management

A new tool, `tooling/plan_manager.py`, will be introduced to manage the registry with simple commands like `register`, `deregister`, and `list`, making it easy to maintain the library of reusable plans.

---

# Protocol: The Closed-Loop Self-Correction Cycle

This protocol describes the automated workflow that enables the agent to programmatically improve its own governing protocols based on new knowledge. It transforms the ad-hoc, manual process of learning into a reliable, machine-driven feedback loop.

## The Problem: The Open Loop

Previously, "lessons learned" were compiled into a simple markdown file, `knowledge_core/lessons_learned.md`. While this captured knowledge, it was a dead end. There was no automated process to translate these text-based insights into actual changes to the protocol source files. This required manual intervention, creating a significant bottleneck and a high risk of protocols becoming stale.

## The Solution: A Protocol-Driven Self-Correction (PDSC) Workflow

The PDSC workflow closes the feedback loop by introducing a set of new tools and structured data formats that allow the agent to enact its own improvements.

**1. Structured, Actionable Lessons (`knowledge_core/lessons.jsonl`):**
- Post-mortem analysis now generates lessons as structured JSON objects, not free-form text.
- Each lesson includes a machine-readable `action` field, which contains a specific, executable command.

**2. The Protocol Updater (`tooling/protocol_updater.py`):**
- A new, dedicated tool for programmatically modifying the protocol source files (`*.protocol.json`).
- It accepts commands like `add-tool`, allowing for precise, automated changes to protocol definitions.

**3. The Orchestrator (`tooling/self_correction_orchestrator.py`):**
- This script is the engine of the cycle. It reads `lessons.jsonl`, identifies pending lessons, and uses the `protocol_updater.py` to execute the defined actions.
- After applying a lesson, it updates the lesson's status, creating a clear audit trail.
- It finishes by running `make AGENTS.md` to ensure the changes are compiled into the live protocol.

This new, automated cycle—**Analyze -> Structure Lesson -> Execute Correction -> Re-compile Protocol**—is a fundamental step towards autonomous self-improvement.

---

# Protocol: Deep Research Cycle

This protocol defines a standardized, multi-step plan for conducting in-depth research on a complex topic. It is designed to be a reusable, callable plan that ensures a systematic and thorough investigation.

The cycle consists of five main phases:
1.  **Review Scanned Documents:** The agent first reviews the content of documents found in the repository during the initial scan. This provides immediate, project-specific context.
2.  **Initial Scoping & Keyword Generation:** Based on the initial topic and the information from scanned documents, the agent generates a set of search keywords.
3.  **Broad Information Gathering:** The agent uses the keywords to perform broad web searches and collect a list of relevant URLs.
4.  **Targeted Information Extraction:** The agent visits the most promising URLs to extract detailed information.
5.  **Synthesis & Summary:** The agent synthesizes the gathered information into a coherent summary, which is saved to a research report file.

This structured approach ensures that research is not ad-hoc but is instead a repeatable and verifiable process.

---

# Protocol: The Formal Research Cycle (L4)

This protocol establishes the L4 Deep Research Cycle, a specialized, self-contained Finite Development Cycle (FDC) designed for comprehensive knowledge acquisition. It elevates research from a simple tool-based action to a formal, verifiable process.

## The Problem: Ad-Hoc Research

Previously, research was an unstructured activity. The agent could use tools like `google_search` or `read_file`, but there was no formal process for planning, executing, and synthesizing complex research tasks. This made it difficult to tackle "unknown unknowns" in a reliable and auditable way.

## The Solution: A Dedicated Research FDC

The L4 Research Cycle solves this by introducing a new, specialized Finite State Machine (FSM) tailored specifically for research. When the main orchestrator (`master_control.py`) determines that a task requires deep knowledge, it initiates this cycle.

### Key Features:

1.  **Specialized FSM (`tooling/research_fsm.json`):** Unlike the generic development FSM, the research FSM has states that reflect a true research workflow: `GATHERING`, `SYNTHESIZING`, and `REPORTING`. This provides a more accurate model for the task.
2.  **Executable Plans:** The `tooling/research_planner.py` is upgraded to generate formal, executable plans that are validated against the new research FSM. These are no longer just templates but are verifiable artifacts that guide the agent through the research process.
3.  **Formal Invocation:** The L4 cycle is a first-class citizen in the agent's architecture. The main orchestrator can formally invoke it, execute the research plan, and then integrate the resulting knowledge back into its main task.

This new protocol provides a robust, reliable, and formally verifiable mechanism for the agent to explore complex topics, making it significantly more autonomous and capable.

---


---

# Protocol: The Context-Sensitive Development Cycle (CSDC)

This protocol introduces a new form of development cycle that is sensitive to the logical context in which it operates. It moves beyond the purely structural validation of the FDC and CFDC to incorporate constraints based on fundamental principles of logic and computability.

The CSDC is founded on the idea of exploring the trade-offs between expressive power and the risk of self-referential paradoxes. It achieves this by defining two mutually exclusive development models.

## Model A: The Introspective Model

- **Permits:** `define_set_of_names`
- **Forbids:** `define_diagonalization_function`

This model allows the system to have a complete map of its own language, enabling powerful introspection and metaprogramming. However, it explicitly forbids the diagonalization function, a common source of paradoxes in self-referential systems. This can be seen as a Gödel-like approach.

## Model B: The Self-Referential Model

- **Permits:** `define_diagonalization_function`
- **Forbids:** `define_set_of_names`

This model allows the system to define and use the diagonalization function, enabling direct self-reference. However, it prevents the system from having a complete name-map of its own expressions, which is another way to avoid paradox (related to Tarski's undefinability theorem).

## Complexity Classes

Both models can be further constrained by computational complexity:
- **Polynomial (P):** For plans that are considered computationally tractable.
- **Exponential (EXP):** For plans that may require significantly more resources, allowing for more complex but potentially less efficient solutions.

## The `csdc_cli.py` Tool

The CSDC is enforced by the `tooling/csdc_cli.py` tool. This tool validates a plan against a specified model and complexity class, ensuring that all constraints are met before execution.

---

# Protocol: pLLLU Execution

This protocol establishes the `plllu_runner.py` script as the official entry point for executing pLLLU (`.plllu`) files.

## The Problem: Lack of a Standard Runner

The pLLLU language provides a powerful way to define complex logic, but without a standardized execution tool, there is no reliable way to integrate these files into the agent's workflow.

## The Solution: A Dedicated Runner

This protocol mandates the use of `tooling/plllu_runner.py` for all pLLLU file executions.

**Rule `plllu-runner-is-entry-point`**: All pLLLU files must be executed through the `plllu_runner.py` script.

This ensures that every pLLLU file is executed in a controlled, programmatic environment.

---

# Protocol: Speculative Execution

This protocol empowers the agent to engage in creative and exploratory tasks when it is otherwise idle. It provides a formal framework for the agent to generate novel ideas, plans, or artifacts that are not direct responses to a user request, but are instead products of its own "imagination" and analysis of the repository.

The goal is to enable proactive, creative problem-solving and self-improvement, allowing the agent to "dream" productively within safe and well-defined boundaries.

## Rules

- **`idle-state-trigger`**: The Speculative Execution Protocol can only be invoked when the agent has no active, user-assigned task. This ensures that speculative work never interferes with primary duties.
- **`formal-proposal-required`**: The first action in any speculative task must be the creation of a formal proposal document. This document must outline the objective, rationale, and a detailed plan for the task.
- **`resource-constraints`**: All speculative tasks must operate under predefined resource constraints (e.g., time limits, computational resources) to prevent runaway processes.
- **`user-review-gate`**: The final output or artifact of a speculative task cannot be integrated or submitted directly. It must be presented to the user for formal review and approval.
- **`speculative-logging`**: All logs, artifacts, and actions generated during a speculative task must be clearly tagged with a `speculative` flag to distinguish them from standard, user-directed work.

---

```json
{
  "protocol_id": "agent-shell-001",
  "description": "A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.",
  "rules": [
    {
      "rule_id": "shell-is-primary-entry-point",
      "description": "All agent tasks must be initiated through the `agent_shell.py` script. This script is the designated, API-driven entry point that ensures proper initialization of the MasterControlGraph FSM, centralized logging, and programmatic lifecycle management. Direct execution of other tools or scripts is forbidden for task initiation.",
      "enforcement": "This is a procedural rule. The agent's operational framework should only expose the agent_shell.py as the means of starting a new task."
    }
  ],
  "associated_tools": [
    "tooling/agent_shell.py"
  ]
}
```


---

```json
{
  "protocol_id": "toolchain-review-on-schema-change-001",
  "description": "A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.",
  "rules": [
    {
      "rule_id": "toolchain-audit-on-schema-change",
      "description": "If a change is made to the core protocol schema (`protocol.schema.json`) or to the compilers that process it (`protocol_compiler.py`, `hierarchical_compiler.py`), a formal audit of the entire `tooling/` directory MUST be performed as a subsequent step. This audit should verify that all tools are compatible with the new protocol structure.",
      "enforcement": "This is a procedural rule for any agent developing the protocol system. Adherence can be partially checked by post-commit hooks or review processes that look for a tooling audit in any change that modifies the specified core files."
    }
  ],
  "associated_tools": [
    "tooling/protocol_auditor.py",
    "tooling/protocol_compiler.py",
    "tooling/hierarchical_compiler.py"
  ]
}
```


---

```json
{
  "protocol_id": "unified-auditor-001",
  "description": "A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.",
  "rules": [
    {
      "rule_id": "run-all-audits",
      "description": "The `auditor.py` script should be used to run comprehensive checks on the repository's health. It can be run with 'all' to check protocols, plans, and documentation completeness.",
      "enforcement": "The tool is invoked via the command line, typically through the `make audit` target."
    }
  ],
  "associated_tools": [
    "tooling/auditor.py"
  ]
}
```


---

```json
{
  "protocol_id": "aura-execution-001",
  "description": "A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.",
  "rules": [
    {
      "rule_id": "execute-aura-script",
      "description": "The `aura_executor.py` tool should be used to execute .aura script files. This tool provides the bridge between the agent's master control loop and the Aura language interpreter.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the Aura script as an argument."
    }
  ],
  "associated_tools": [
    "tooling/aura_executor.py"
  ]
}
```


---

```json
{
  "protocol_id": "capability-verification-001",
  "description": "A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.",
  "rules": [
    {
      "rule_id": "verify-capability-acquisition",
      "description": "The `capability_verifier.py` tool should be used to test the agent's ability to acquire a new capability defined by a failing test file. The tool orchestrates the failure, self-correction, and verification process.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the target test file."
    }
  ],
  "associated_tools": [
    "tooling/capability_verifier.py"
  ]
}
```


---

```json
{
  "protocol_id": "csdc-001",
  "description": "A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.",
  "rules": [
    {
      "rule_id": "use-csdc-cli",
      "description": "The `csdc_cli.py` tool must be used to validate plans under the CSDC. This tool enforces model-specific constraints (A or B) and complexity requirements (P or EXP).",
      "enforcement": "The tool is used by invoking it from the command line with the plan file, model, and complexity as arguments."
    },
    {
      "rule_id": "model-a-constraints",
      "description": "Model A permits `define_set_of_names` but forbids `define_diagonalization_function`.",
      "enforcement": "Enforced by the `fsm_model_a.json` FSM used by the `csdc_cli.py` tool."
    },
    {
      "rule_id": "model-b-constraints",
      "description": "Model B permits `define_diagonalization_function` but forbids `define_set_of_names`.",
      "enforcement": "Enforced by the `fsm_model_b.json` FSM used by the `csdc_cli.py` tool."
    }
  ],
  "associated_tools": [
    "tooling/csdc_cli.py"
  ]
}
```


---

```json
{
  "protocol_id": "unified-doc-builder-001",
  "description": "A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.",
  "rules": [
    {
      "rule_id": "use-doc-builder-for-all-docs",
      "description": "The `doc_builder.py` script is the single entry point for generating all user-facing documentation, including system-level docs, README files, and GitHub Pages. It should be called with the appropriate '--format' argument.",
      "enforcement": "The tool is invoked via the command line, typically through the `make docs`, `make readme`, or `make pages` targets."
    }
  ],
  "associated_tools": [
    "tooling/doc_builder.py"
  ]
}
```


---

```json
{
  "protocol_id": "file-indexing-001",
  "description": "A protocol for maintaining an up-to-date file index to accelerate tool performance.",
  "rules": [
    {
      "rule_id": "update-index-before-submit",
      "description": "Before submitting any changes that alter the file structure (create, delete, rename), the agent MUST rebuild the repository's file index. This ensures that tools relying on the index, such as the FDC validator, have an accurate view of the filesystem.",
      "enforcement": "This is a procedural rule. The agent's pre-submission checklist should include a step to run 'python tooling/file_indexer.py build'."
    }
  ],
  "associated_tools": [
    "tooling/file_indexer.py"
  ]
}
```


---

```json
{
  "protocol_id": "hdl-proving-001",
  "description": "A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.",
  "rules": [
    {
      "rule_id": "prove-sequent",
      "description": "The `hdl_prover.py` tool should be used to check the provability of a logical sequent. This tool acts as a wrapper for the underlying Lisp-based prover.",
      "enforcement": "The tool is used by invoking it from the command line with the sequent to be proved as an argument."
    }
  ],
  "associated_tools": [
    "tooling/hdl_prover.py"
  ]
}
```


---

```json
{
  "protocol_id": "agent-interaction-001",
  "description": "A protocol governing the agent's core interaction and planning tools.",
  "rules": [
    {
      "rule_id": "planning-tool-access",
      "description": "The agent is authorized to use the `set_plan` tool to create and update its execution plan. This is a foundational capability for task execution.",
      "enforcement": "The agent's core logic should be designed to use this tool for all planning activities."
    },
    {
      "rule_id": "communication-tool-access",
      "description": "The agent is authorized to use the `message_user` tool to communicate with the user, providing updates and asking for clarification. This is essential for a collaborative workflow.",
      "enforcement": "The agent's core logic should be designed to use this tool for all user-facing communication."
    }
  ],
  "associated_tools": [
    "set_plan",
    "message_user"
  ]
}
```


---

```json
{
  "protocol_id": "plllu-execution-001",
  "description": "A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.",
  "rules": [
    {
      "rule_id": "execute-plllu-script",
      "description": "The `plllu_runner.py` tool should be used to execute .plllu script files. This tool provides the bridge between the agent's master control loop and the pLLLU language interpreter.",
      "enforcement": "The tool is used by invoking it from the command line with the path to the pLLLU script as an argument."
    }
  ],
  "associated_tools": [
    "tooling/plllu_runner.py"
  ]
}
```


---

```json
{
  "protocol_id": "speculative-execution-001",
  "description": "A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.",
  "rules": [
    {
      "rule_id": "idle-state-trigger",
      "description": "The agent may only initiate a speculative task when it has no active, user-assigned tasks.",
      "enforcement": "The agent's main control loop must verify an idle state before allowing the invocation of a speculative plan."
    },
    {
      "rule_id": "formal-proposal-required",
      "description": "A speculative task must begin with the creation of a formal proposal document, outlining the objective, rationale, and plan.",
      "enforcement": "The initial plan for any speculative task must include a step to generate and save a proposal artifact."
    },
    {
      "rule_id": "resource-constraints",
      "description": "Speculative tasks must operate under defined resource limits.",
      "enforcement": "This is a system-level constraint that the agent orchestrator must enforce."
    },
    {
      "rule_id": "user-review-gate",
      "description": "Final artifacts from a speculative task must be submitted for user review and cannot be merged directly.",
      "enforcement": "The agent is forbidden from using tools like 'submit' or 'merge' within a speculative context. It must use 'request_user_input' to present the results."
    },
    {
      "rule_id": "speculative-logging",
      "description": "All logs and artifacts generated during a speculative task must be tagged as 'speculative'.",
      "enforcement": "The agent's logging and file-creation tools should be context-aware and apply this tag when in a speculative mode."
    }
  ],
  "associated_tools": [
    "set_plan",
    "create_file_with_block",
    "request_user_input"
  ]
}
```


---



# --- Associated Tool Documentation ---

# Module Documentation

## Overview

This document provides a human-readable summary of the protocols and key components defined within this module. It is automatically generated.

## Core Protocols

- **`dependency-management-001`**: A protocol for ensuring a reliable execution environment through formal dependency management.
- **`experimental-prologue-001`**: An experimental protocol to test dynamic rule-following. It mandates a prologue action before file creation.
- **`agent-shell-001`**: A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.
- **`toolchain-review-on-schema-change-001`**: A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.
- **`unified-auditor-001`**: A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.
- **`aura-execution-001`**: A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`capability-verification-001`**: A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.
- **`csdc-001`**: A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.
- **`unified-doc-builder-001`**: A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.
- **`file-indexing-001`**: A protocol for maintaining an up-to-date file index to accelerate tool performance.
- **`hdl-proving-001`**: A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.
- **`agent-interaction-001`**: A protocol governing the agent's core interaction and planning tools.
- **`plllu-execution-001`**: A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`security-header`**: Defines the identity and purpose of the Security Protocol document.
- **`security-vuln-reporting-001`**: Defines the official policy and procedure for reporting security vulnerabilities.
- **`speculative-execution-001`**: A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.

## Key Components

- **`tooling/__init__.py`**:

  > This module contains the various tools and utilities that support the agent's
  > development, testing, and operational workflows.
  >
  > The tools in this package are the building blocks of the agent's capabilities,
  > ranging from code analysis and refactoring to protocol compilation and
  > self-correction. Each script is designed to be a self-contained unit of
  > functionality that can be invoked either from the command line or programmatically
  > by the agent's master control system.
  >
  > This __init__.py file marks the 'tooling' directory as a Python package,
  > allowing for the organized import of its various modules.

- **`tooling/agent_shell.py`**:

  > The new, interactive, API-driven entry point for the agent.
  >
  > This script replaces the old file-based signaling system with a direct,
  > programmatic interface to the MasterControlGraph FSM. It is responsible for:
  > 1.  Initializing the agent's state and a centralized logger.
  > 2.  Instantiating and running the MasterControlGraph.
  > 3.  Driving the FSM by calling its methods and passing data and the logger.
  > 4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
  >     and respond to requests for action.

- **`tooling/__init__.py`**:

  > _No module-level docstring found._

- **`tooling/generate_and_test.py`**:

  > _No module-level docstring found._

- **`tooling/appl_runner.py`**:

  > A command-line tool for executing APPL files.
  >
  > This script provides a simple interface to run APPL files using the main
  > `run.py` interpreter. It captures and prints the output of the execution,
  > and provides detailed error reporting if the execution fails.

- **`tooling/appl_to_lfi_ill.py`**:

  > A compiler that translates APPL (a simple functional language) to LFI-ILL.
  >
  > This script takes a Python file containing an APPL AST, and compiles it into
  > an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/auditor.py`**:

  > A unified auditing tool for maintaining repository health and compliance.
  >
  > This script combines the functionality of several disparate auditing tools into a
  > single, comprehensive command-line interface. It serves as the central tool for
  > validating the key components of the agent's architecture, including protocols,
  > plans, and documentation.
  >
  > The auditor can perform the following checks:
  > 1.  **Protocol Audit (`protocol`):**
  >     - Checks if `AGENTS.md` artifacts are stale compared to their source files.
  >     - Verifies protocol completeness by comparing tools used in logs against
  >       tools defined in protocols.
  >     - Analyzes tool usage frequency (centrality).
  > 2.  **Plan Registry Audit (`plans`):**
  >     - Scans `knowledge_core/plan_registry.json` for "dead links" where the
  >       target plan file does not exist.
  > 3.  **Documentation Audit (`docs`):**
  >     - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
  >       that are missing module-level docstrings.
  >
  > The tool is designed to be run from the command line and can execute specific
  > audits or all of them, generating a consolidated `audit_report.md` file.

- **`tooling/aura_executor.py`**:

  > This script serves as the command-line executor for `.aura` files.
  >
  > It bridges the gap between the high-level Aura scripting language and the
  > agent's underlying Python-based toolset. The executor is responsible for:
  > 1.  Parsing the `.aura` source code using the lexer and parser from the
  >     `aura_lang` package.
  > 2.  Setting up an execution environment for the interpreter.
  > 3.  Injecting a "tool-calling" capability into the Aura environment, which
  >     allows Aura scripts to dynamically invoke registered Python tools
  >     (e.g., `hdl_prover`, `environmental_probe`).
  > 4.  Executing the parsed program and printing the final result.
  >
  > This makes it a key component for enabling more expressive and complex
  > automation scripts for the agent.

- **`tooling/aura_to_lfi_ill.py`**:

  > A compiler that translates AURA code to LFI-ILL.
  >
  > This script takes an AURA file, parses it, and compiles it into an LFI-ILL
  > AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/background_researcher.py`**:

  > This script performs a simulated research task in the background.
  > It takes a task ID as a command-line argument and writes its findings
  > to a temporary file that the main agent can poll.

- **`tooling/builder.py`**:

  > A unified, configuration-driven build script for the project.
  >
  > This script serves as the central entry point for all build-related tasks, such
  > as generating documentation, compiling protocols, and running code quality checks.
  > It replaces a traditional Makefile's direct command execution with a more
  > structured, maintainable, and introspectable approach.
  >
  > The core logic is driven by a `build_config.json` file, which defines a series
  > of "targets." Each target specifies:
  > - The `type` of target: "compiler" or "command".
  > - For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
  > - For "command" types: the `command` to execute.
  >
  > The configuration also defines "build_groups", which are ordered collections of
  > targets (e.g., "all", "quality").
  >
  > This centralized builder provides several advantages:
  > - **Single Source of Truth:** The `build_config.json` file is the definitive
  >   source for all build logic.
  > - **Consistency:** Ensures all build tasks are executed in a uniform way.
  > - **Extensibility:** New build targets can be added by simply updating the
  >   configuration file.
  > - **Discoverability:** The script can list all available targets and groups.

- **`tooling/capability_verifier.py`**:

  > A tool to verify that the agent can monotonically improve its capabilities.
  >
  > This script is designed to provide a formal, automated test for the agent's
  > self-correction and learning mechanisms. It ensures that when the agent learns
  > a new capability, it does so without losing (regressing) any of its existing
  > capabilities. This is a critical safeguard for ensuring robust and reliable
  > agent evolution.
  >
  > The tool works by orchestrating a four-step process:
  > 1.  **Confirm Initial Failure:** It runs a specific test file that is known to
  >     fail, verifying that the agent currently lacks the target capability.
  > 2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
  >     triggers the `self_correction_orchestrator.py` script, which is responsible
  >     for integrating new knowledge and skills.
  > 3.  **Confirm Final Success:** It runs the same test file again, confirming that
  >     the agent has successfully learned the new capability and the test now passes.
  > 4.  **Check for Regressions:** It runs the full, existing test suite to ensure
  >     that the process of learning the new skill has not inadvertently broken any
  >     previously functional capabilities.
  >
  > This provides a closed-loop verification of monotonic improvement, which is a
  > cornerstone of the agent's design philosophy.

- **`tooling/code_suggester.py`**:

  > Handles the generation and application of autonomous code change suggestions.
  >
  > This tool is a key component of the advanced self-correction loop. It is
  > designed to be invoked by the self-correction orchestrator when a lesson
  > contains a 'propose-code-change' action.
  >
  > For its initial implementation, this tool acts as a structured executor. It
  > takes a lesson where the 'details' field contains a fully-formed git-style
  > merge diff and applies it to the target file. It does this by generating a
  > temporary, single-step plan file and signaling its location for the master
  > controller to execute.
  >
  > This establishes the fundamental workflow for autonomous code modification,
  > decoupling the suggestion logic from the execution logic. Future iterations
  > can enhance this tool with more sophisticated code generation capabilities
  > (e.g., using an LLM to generate the diff from a natural language description)
  > without altering the core orchestration process.

- **`tooling/context_awareness_scanner.py`**:

  > A tool for performing static analysis on a Python file to understand its context.
  >
  > This script provides a "contextual awareness" scan of a specified Python file
  > to help an agent (or a human) understand its role, dependencies, and connections
  > within a larger codebase. This is crucial for planning complex changes or
  > refactoring efforts, as it provides a snapshot of the potential impact of
  > modifying a file.
  >
  > The scanner performs three main functions:
  > 1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
  >     module to parse the target file and identify all the functions and classes
  >     that are defined within it.
  > 2.  **Import Analysis:** It also uses the AST to find all modules and symbols
  >     that the target file imports, revealing its dependencies on other parts of
  >     the codebase or external libraries.
  > 3.  **Reference Finding:** It performs a repository-wide search to find all other
  >     files that reference the symbols defined in the target file. This helps to
  >     understand how the file is used by the rest of the system.
  >
  > The final output is a detailed JSON report containing all of this information,
  > which can be used as a foundational artifact for automated planning or human review.

- **`tooling/csdc_cli.py`**:

  > A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).
  >
  > This script provides an interface to validate a development plan against a specific
  > CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
  > plan adheres to the strict logical and computational constraints defined by the
  > CSDC protocol before it is executed.
  >
  > The tool performs two main checks:
  > 1.  **Complexity Analysis:** It analyzes the plan to determine its computational
  >     complexity and verifies that it matches the expected complexity class.
  > 2.  **Model Validation:** It validates the plan's commands against the rules of
  >     the specified CSDC model, ensuring that it does not violate any of the
  >     model's constraints (e.g., forbidding certain functions).
  >
  > This serves as a critical gateway for ensuring that all development work within
  > the CSDC framework is sound, predictable, and compliant with the governing
  > meta-mathematical principles.

- **`tooling/dependency_graph_generator.py`**:

  > Scans the repository for dependency files and generates a unified dependency graph.
  >
  > This script is a crucial component of the agent's environmental awareness,
  > providing a clear map of the software supply chain. It recursively searches the
  > entire repository for common dependency management files, specifically:
  > - `package.json` (for JavaScript/Node.js projects)
  > - `requirements.txt` (for Python projects)
  >
  > It parses these files to identify two key types of relationships:
  > 1.  **Internal Dependencies:** Links between different projects within this repository.
  > 2.  **External Dependencies:** Links to third-party libraries and packages.
  >
  > The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
  > represents these relationships as a graph structure with nodes (projects and
  > dependencies) and edges (the dependency links). This artifact is a primary
  > input for the agent's orientation and planning phases, allowing it to reason
  > about the potential impact of its changes.

- **`tooling/doc_builder.py`**:

  > A unified documentation builder for the project.
  > ...

- **`tooling/document_scanner.py`**:

  > A tool for scanning the repository for human-readable documents and extracting their text content.
  >
  > This script is a crucial component of the agent's initial information-gathering
  > and orientation phase. It allows the agent to ingest knowledge from unstructured
  > or semi-structured documents that are not part of the formal codebase, but which
  > may contain critical context, requirements, or specifications.
  >
  > The scanner searches a given directory for files with common document extensions:
  > - `.pdf`: Uses the `pypdf` library to extract text from PDF files.
  > - `.md`: Reads Markdown files.
  > - `.txt`: Reads plain text files.
  >
  > The output is a dictionary where the keys are the file paths of the discovered
  > documents and the values are their extracted text content. This data can then
  > be used by the agent to inform its planning and execution process. This tool
  > is essential for bridging the gap between human-written documentation and the
  > agent's operational awareness.

- **`tooling/environmental_probe.py`**:

  > Performs a series of checks to assess the capabilities of the execution environment.
  >
  > This script is a critical diagnostic tool run at the beginning of a task to
  > ensure the agent understands its operational sandbox. It verifies fundamental
  > capabilities required for most software development tasks:
  >
  > 1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
  >     and delete files. It also provides a basic latency measurement for these
  >     operations.
  > 2.  **Network Connectivity:** Checks for external network access by attempting to
  >     connect to a highly-available public endpoint (google.com). This is crucial
  >     for tasks requiring `git` operations, package downloads, or API calls.
  > 3.  **Environment Variables:** Verifies that standard environment variables are
  >     accessible, which is a prerequisite for many command-line tools.
  >
  > The script generates a human-readable report summarizing the results of these
  > probes, allowing the agent to quickly identify any environmental constraints
  > that might impact its ability to complete a task.

- **`tooling/fdc_cli.py`**:

  > This script provides a command-line interface (CLI) for managing the Finite
  > Development Cycle (FDC).
  >
  > The FDC is a structured workflow for agent-driven software development. This CLI
  > is the primary human interface for interacting with that cycle, providing
  > commands to:
  > - **start:** Initiates a new development task, triggering the "Advanced
  >   Orientation and Research Protocol" (AORP) to ensure the agent is fully
  >   contextualized.
  > - **close:** Formally concludes a task, creating a post-mortem template for
  >   analysis and lesson-learning.
  > - **validate:** Checks a given plan file for both syntactic and semantic
  >   correctness against the FDC's governing Finite State Machine (FSM). This
  >   ensures that a plan is executable and will not violate protocol.
  > - **analyze:** Examines a plan to determine its computational complexity (e.g.,
  >   Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  >   Read-Write), providing insight into the plan's potential impact.

- **`tooling/filesystem_lister.py`**:

  > A tool for listing files and directories in a repository, with an option to respect .gitignore.

- **`tooling/halting_heuristic_analyzer.py`**:

  > A static analysis tool to estimate the termination risk of a UDC plan.
  >
  > This script reads a `.udc` plan file, parses its instructions, and uses a
  > series of heuristics to identify potential infinite loops. It is not a
  > formal decider (as the halting problem is undecidable), but rather a
  > practical tool to flag common patterns that lead to non-termination.
  >
  > The analysis focuses on:
  > 1.  Detecting backward jumps, which are the primary indicator of loops.
  > 2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
  > 3.  Checking if the registers involved in the exit conditions are modified
  >     within the loop body in a way that is likely to lead to termination.
  >
  > The tool outputs a JSON report detailing the estimated risk level (LOW,
  > MEDIUM, HIGH) and the specific loops that were identified.

- **`tooling/hdl_prover.py`**:

  > A command-line tool for proving sequents in Intuitionistic Linear Logic.
  >
  > This script provides a basic interface to a simple logic prover. It takes a
  > sequent as a command-line argument, parses it into a logical structure, and
  > then attempts to prove it using a rudimentary proof search algorithm.
  >
  > The primary purpose of this tool is to allow the agent to perform formal
  > reasoning and verification tasks by checking the validity of logical entailments.
  > For example, it can be used to verify that a certain conclusion follows from a
  > set of premises according to the rules of linear logic.
  >
  > The current implementation uses a very basic parser and proof algorithm,
  > serving as a placeholder and demonstration for a more sophisticated, underlying
  > logic engine.

- **`tooling/hierarchical_compiler.py`**:

  > A hierarchical build system for compiling nested protocol modules.
  >
  > This script orchestrates the compilation of `AGENTS.md` and `README.md` files
  > across a repository with a nested or hierarchical module structure. It is a key
  > component of the system's ability to manage complexity by allowing protocols to
  > be defined in a modular, distributed way while still being presented as a unified,
  > coherent whole at each level of the hierarchy.
  >
  > The compiler operates in two main passes:
  >
  > **Pass 1: Documentation Compilation (Bottom-Up)**
  > 1.  **Discovery:** It finds all `protocols` directories in the repository, which
  >     signify the root of a documentation module.
  > 2.  **Bottom-Up Traversal:** It processes these directories from the most deeply
  >     nested ones upwards. This ensures that child modules are always built before
  >     their parents.
  > 3.  **Child Summary Injection:** For each compiled child module, it generates a
  >     summary of its protocols and injects this summary into the parent's
  >     `protocols` directory as a temporary file.
  > 4.  **Parent Compilation:** When the parent module is compiled, the standard
  >     `protocol_compiler.py` automatically includes the injected child summaries,
  >     creating a single `AGENTS.md` file that contains both the parent's native
  >     protocols and the full protocols of all its direct children.
  > 5.  **README Generation:** After each `AGENTS.md` is compiled, the corresponding
  >     `README.md` is generated.
  >
  > **Pass 2: Centralized Knowledge Graph Compilation**
  > 1.  After all documentation is built, it performs a full repository scan to find
  >     every `*.protocol.json` file.
  > 2.  It parses all of these files and compiles them into a single, centralized
  >     RDF knowledge graph (`protocols.ttl`). This provides a unified,
  >     machine-readable view of every protocol defined anywhere in the system.
  >
  > This hierarchical approach allows for both localized, context-specific protocol
  > definitions and a holistic, system-wide understanding of the agent's governing rules.

- **`tooling/knowledge_compiler.py`**:

  > Extracts structured lessons from post-mortem reports and compiles them into a
  > centralized, long-term knowledge base.
  >
  > This script is a core component of the agent's self-improvement feedback loop.
  > After a task is completed, a post-mortem report is generated that includes a
  > section for "Corrective Actions & Lessons Learned." This script automates the
  > process of parsing that section to extract key insights.
  >
  > It identifies pairs of "Lesson" and "Action" statements and transforms them
  > into a standardized, machine-readable format. These formatted entries are then
  > appended to the `knowledge_core/lessons.jsonl` file, which serves as the
  > agent's persistent memory of what has worked, what has failed, and what can be
  > improved in future tasks.
  >
  > The script is executed via the command line, taking the path to a completed
  > post-mortem file as its primary argument.

- **`tooling/knowledge_integrator.py`**:

  > Enriches the local knowledge graph with data from external sources like DBPedia.
  >
  > This script loads the RDF graph generated from the project's protocols,
  > identifies key concepts (like tools and rules), queries the DBPedia SPARQL
  > endpoint to find related information, and merges the external data into a new,
  > enriched knowledge graph.

- **`tooling/lba_validator.py`**:

  > A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.
  >
  > This module implements a validator that enforces the context-sensitive rules of the CSDC.
  > Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
  > validation decisions. This is necessary to enforce rules where the validity of one
  > command depends on the presence or absence of another command elsewhere in the plan.
  >
  > The CSDC defines two mutually exclusive models:
  > - Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
  > - Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.
  >
  > This validator checks for these co-occurrence constraints.

- **`tooling/lfi_ill_halting_decider.py`**:

  > A tool for analyzing the termination of LFI-ILL programs.
  >
  > This script takes an LFI-ILL file, interprets it in a paraconsistent logic
  > environment, and reports on its halting status. It does this by setting up
  > a paradoxical initial state and observing how the program resolves it.

- **`tooling/lfi_udc_model.py`**:

  > A paraconsistent execution model for UDC plans.
  >
  > This module provides the classes necessary to interpret a UDC (Un-decidable
  > Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
  > concrete values, the state of the machine (registers, tape, etc.) is modeled
  > using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).
  >
  > This allows the system to reason about paradoxical programs, such as a program
  > that halts if and only if it does not halt. By executing the program under
  > paraconsistent semantics, the model can arrive at a final state of `BOTH`,
  > effectively demonstrating the paradoxical nature of the input without crashing.
  >
  > Key classes:
  > - `ParaconsistentTruth`: An enum for the four truth values.
  > - `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
  > - `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
  > - `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
  > - `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  >   analysis of a UDC plan.

- **`tooling/log_failure.py`**:

  > A dedicated script to log a catastrophic failure event to the main activity log.
  >
  > This tool is designed to be invoked in the rare case of a severe, unrecoverable
  > error that violates a core protocol. Its primary purpose is to ensure that such
  > a critical event is formally and structurally documented in the standard agent
  > activity log (`logs/activity.log.jsonl`), even if the main agent loop has
  > crashed or been terminated.
  >
  > The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
  > attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
  > permanent, machine-readable record of the failure, which is essential for
  > post-mortem analysis, debugging, and the development of future safeguards.
  >
  > By using the standard `Logger` class, it ensures that the failure log entry
  > conforms to the established `LOGGING_SCHEMA.md`, making it processable by
  > auditing and analysis tools.

- **`tooling/master_control.py`**:

  > The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).
  >
  > This script, master_control.py, is the heart of the agent's operational loop.
  > It implements the CFDC, a hierarchical planning and execution model based on a
  > Pushdown Automaton. This allows the agent to execute complex tasks by calling
  > plans as sub-routines.
  >
  > Core Responsibilities:
  > - **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  >   plans to call other plans via the `call_plan` directive. This allows for
  >   modular, reusable, and complex task decomposition. A maximum recursion depth
  >   is enforced to guarantee decidability.
  > - **Plan Validation:** Contains the in-memory plan validator. Before execution,
  >   it parses a plan and simulates its execution against a Finite State Machine
  >   (FSM) to ensure it complies with the agent's operational protocols.
  > - **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  >   it first attempts to look up the plan by its logical name in the
  >   `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  >   the argument as a direct file path.
  > - **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  >   finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  >   to ensure predictable and auditable behavior.
  >
  > This module is designed as a library to be controlled by an external shell
  > (e.g., `agent_shell.py`), making its interaction purely programmatic.

- **`tooling/master_control_cli.py`**:

  > The official command-line interface for the agent's master control loop.
  >
  > This script is now a lightweight wrapper that passes control to the new,
  > API-driven `agent_shell.py`. It preserves the command-line interface while
  > decoupling the entry point from the FSM implementation.

- **`tooling/message_user.py`**:

  > A dummy tool that prints its arguments to simulate the message_user tool.
  >
  > This script is a simple command-line utility that takes a string as an
  > argument and prints it to standard output, prefixed with "[Message User]:".
  > Its purpose is to serve as a stand-in or mock for the actual `message_user`
  > tool in testing environments where the full agent framework is not required.
  >
  > This allows for the testing of scripts or workflows that call the
  > `message_user` tool without needing to invoke the entire agent messaging
  > subsystem.

- **`tooling/pda_parser.py`**:

  > A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.
  >
  > This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
  > parser for a simple, string-based representation of pLLLU formulas. It can
  > handle basic atomic formulas, unary operators (like negation and consistency),
  > and binary operators (like implication and conjunction).
  >
  > The main function `parse_formula` takes a string and returns a simple AST
  > (Abstract Syntax Tree) represented as nested tuples.

- **`tooling/plan_executor.py`**:

  > A simple plan executor for simulating agent behavior.
  >
  > This script reads a plan file, parses it, and executes the commands in a
  > simplified, simulated environment. It supports a limited set of tools
  > (`message_user` and `run_in_bash_session`) to provide a basic demonstration
  > of how an agent would execute a plan.

- **`tooling/plan_manager.py`**:

  > Provides a command-line interface for managing the agent's Plan Registry.
  >
  > This script is the administrative tool for the Plan Registry, a key component
  > of the Context-Free Development Cycle (CFDC) that enables hierarchical and
  > modular planning. The registry, located at `knowledge_core/plan_registry.json`,
  > maps human-readable, logical names to the file paths of specific plans. This
  > decouples the `call_plan` directive from hardcoded file paths, making plans
  > more reusable and the system more robust.
  >
  > This CLI provides three essential functions:
  > - **register**: Associates a new logical name with a plan file path, adding it
  >   to the central registry.
  > - **deregister**: Removes an existing logical name and its associated path from
  >   the registry.
  > - **list**: Displays all current name-to-path mappings in the registry.
  >
  > By providing a simple, standardized interface for managing this library of
  > reusable plans, this tool improves the agent's ability to compose complex
  > workflows from smaller, validated sub-plans.

- **`tooling/plan_parser.py`**:

  > Parses a plan file into a structured list of commands.
  >
  > This module provides the `parse_plan` function and the `Command` dataclass,
  > which are central to the agent's ability to understand and execute plans.
  > The parser correctly handles multi-line arguments and ignores comments,
  > allowing for robust and readable plan files.

- **`tooling/plllu_interpreter.py`**:

  > A resource-sensitive, four-valued interpreter for pLLLU formulas.
  >
  > This script implements an interpreter for the pLLLU language. It operates on
  > an AST generated by the `pda_parser.py` script. The interpreter is designed
  > to be resource-sensitive, meaning that each atomic formula in the initial
  > context must be consumed exactly once during the evaluation of the proof.
  >
  > The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
  > it to reason about paraconsistent and paracomplete states.
  >
  > The core of the interpreter is the `FourValuedInterpreter` class, which
  > recursively walks the AST, consuming resources from a context (a Counter of
  > available atoms) and returning the resulting logical value.

- **`tooling/plllu_runner.py`**:

  > A command-line runner for pLLLU files.
  >
  > This script provides an entry point for executing `.plllu` files. It
  > integrates the pLLLU lexer, parser, and interpreter to execute the logic
  > defined in a given pLLLU source file and print the result.

- **`tooling/pre_submit_check.py`**:

  > _No module-level docstring found._

- **`tooling/protocol_compiler.py`**:

  > Compiles source protocol files into unified, human-readable and machine-readable artifacts.
  >
  > This script is the engine behind the "protocol as code" principle. It discovers,
  > validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
  > into high-level documents like `AGENTS.md`.
  >
  > Key Functions:
  > - **Discovery:** Scans a directory for source files, including `.protocol.json`
  >   (machine-readable rules) and `.protocol.md` (human-readable context).
  > - **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  >   `.protocol.json` file, ensuring all protocol definitions are syntactically
  >   correct and adhere to the established structure.
  > - **Compilation:** Combines the human-readable markdown and the machine-readable
  >   JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
  > - **Documentation Injection:** Can inject other generated documents, like the
  >   `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
  > - **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  >   protocols and serialize them into an RDF knowledge graph (in Turtle format),
  >   creating a machine-queryable version of the agent's governing rules.
  >
  > This process ensures that `AGENTS.md` and other protocol documents are not edited
  > manually but are instead generated from a validated, single source of truth,
  > making the agent's protocols robust, verifiable, and maintainable.

- **`tooling/protocol_updater.py`**:

  > A command-line tool for programmatically updating protocol source files.
  >
  > This script provides the mechanism for the agent to perform self-correction
  > by modifying its own governing protocols based on structured, actionable
  > lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
  > workflow.
  >
  > The tool operates on the .protocol.json files located in the `protocols/`
  > directory, performing targeted updates based on command-line arguments.

- **`tooling/refactor.py`**:

  > A tool for performing automated symbol renaming in Python code.
  >
  > This script provides a command-line interface to find a specific symbol
  > (a function or a class) in a given Python file and rename it, along with all of
  > its textual references throughout the entire repository. This provides a safe
  > and automated way to perform a common refactoring task, reducing the risk of
  > manual errors.
  >
  > The tool operates in three main stages:
  > 1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
  >     to parse the source file and precisely locate the definition of the target
  >     symbol. This ensures that the tool is targeting the correct code construct.
  > 2.  **Reference Finding:** It performs a text-based search across the specified
  >     search path (defaulting to the entire repository) to find all files that
  >     mention the symbol's old name.
  > 3.  **Plan Generation:** Instead of modifying files directly, it generates a
  >     refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
  >     commands, one for each file that needs to be changed. The path to this
  >     generated plan file is printed to standard output.
  >
  > This plan-based approach allows the agent's master controller to execute the
  > refactoring in a controlled, verifiable, and atomic way, consistent with its
  > standard operational procedures.

- **`tooling/reliable_ls.py`**:

  > A tool for reliably listing files and directories.
  >
  > This script provides a consistent, sorted, and recursive listing of files and
  > directories, excluding the `.git` directory. It is intended to be a more
  > reliable alternative to the standard `ls` command for agent use cases.

- **`tooling/reorientation_manager.py`**:

  > Re-orientation Manager
  >
  > This script is the core of the automated re-orientation process. It is
  > designed to be triggered by the build system whenever the agent's core
  > protocols (`AGENTS.md`) are re-compiled.
  >
  > The manager performs the following key functions:
  > 1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
  >     version to identify new protocols, tools, or other key concepts that have
  >     been introduced.
  > 2.  **Temporal Orientation (Shallow Research):** For each new concept, it
  >     invokes the `temporal_orienter.py` tool to fetch a high-level summary from
  >     an external knowledge base like DBpedia. This ensures the agent has a
  >     baseline understanding of new terms.
  > 3.  **Knowledge Storage:** The summaries from the temporal orientation are
  >     stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
  >     creating a persistent, queryable knowledge artifact.
  > 4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
  >     change is deemed significant (e.g., the addition of a new core
  >     architectural protocol), it programmatically triggers a formal L4 Deep
  >     Research Cycle by creating a `deep_research_required.json` file.
  >
  > This automated workflow ensures that the agent never operates with an outdated
  > understanding of its own protocols. It closes the loop between protocol
  > modification and the agent's self-awareness, making the system more robust,
  > adaptive, and reliable.

- **`tooling/research.py`**:

  > This module contains the logic for executing research tasks based on a set of
  > constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
  > read_file, google_search) based on the specified target and scope.

- **`tooling/research_planner.py`**:

  > This module is responsible for generating a formal, FSM-compliant research plan
  > for a given topic. The output is a string that can be executed by the agent's
  > master controller.

- **`tooling/self_correction_orchestrator.py`**:

  > Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.
  >
  > This script is the engine of the automated feedback loop. It reads structured,
  > actionable lessons from `knowledge_core/lessons.jsonl` and uses the
  > `protocol_updater.py` tool to apply them to the source protocol files.

- **`tooling/self_improvement_cli.py`**:

  > Analyzes agent activity logs to identify opportunities for self-improvement.
  >
  > This script is a command-line tool that serves as a key part of the agent's
  > meta-cognitive loop. It parses the structured activity log
  > (`logs/activity.log.jsonl`) to identify patterns that may indicate
  > inefficiencies or errors in the agent's workflow.
  >
  > The primary analysis currently implemented is:
  > - **Planning Efficiency Analysis:** It scans the logs for tasks that required
  >   multiple `set_plan` actions. A high number of plan revisions for a single
  >   task can suggest that the initial planning phase was insufficient, the task
  >   was poorly understood, or the agent struggled to adapt to unforeseen
  >   challenges.
  >
  > By flagging these tasks, the script provides a starting point for a deeper
  > post-mortem analysis, helping the agent (or its developers) to understand the
  > root causes of the planning churn and to develop strategies for more effective
  > upfront planning in the future.
  >
  > The tool is designed to be extensible, with future analyses (such as error
  > rate tracking or tool usage anti-patterns) to be added as the system evolves.

- **`tooling/standard_agents_compiler.py`**:

  > A compiler that generates a simplified, standard-compliant `AGENTS.md` file.
  >
  > This script acts as an "adapter" to make the repository more accessible to
  > third-party AI agents that expect a conventional set of instructions. While the
  > repository's primary `AGENTS.md` is a complex, hierarchical, and
  > machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
  > file produced by this script offers a simple, human-readable summary of the
  > most common development commands.
  >
  > The script works by:
  > 1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
  >     which is the single source of truth for high-level commands. It specifically
  >     extracts the exact commands for common targets like `install`, `test`,
  >     `lint`, and `format`. This ensures the generated instructions are never
  >     stale.
  > 2.  **Injecting into a Template:** It injects these extracted commands into a
  >     pre-defined, user-friendly Markdown template.
  > 3.  **Generating the Artifact:** The final output is written to
  >     `AGENTS.standard.md`, providing a simple, stable, and conventional entry
  >     point for external tools, effectively bridging the gap between the complex
  >     internal protocol system and the broader agent ecosystem.

- **`tooling/state.py`**:

  > Defines the core data structures for managing the agent's state.
  >
  > This module provides the `AgentState` and `PlanContext` dataclasses, which are
  > fundamental to the operation of the Context-Free Development Cycle (CFDC). These
  > structures allow the `master_control.py` orchestrator to maintain a complete,
  > snapshot-able representation of the agent's progress through a task.
  >
  > - `AgentState`: The primary container for all information related to the current
  >   task, including the plan execution stack, message history, and error states.
  > - `PlanContext`: A specific structure that holds the state of a single plan
  >   file, including its content and the current execution step. This is the
  >   element that gets pushed onto the `plan_stack` in `AgentState`.
  >
  > Together, these classes enable the hierarchical, stack-based planning and
  > execution that is the hallmark of the CFDC.

- **`tooling/symbol_map_generator.py`**:

  > Generates a code symbol map for the repository to aid in contextual understanding.
  >
  > This script creates a `symbols.json` file in the `knowledge_core` directory,
  > which acts as a high-level index of the codebase. This map contains information
  > about key programming constructs like classes and functions, including their
  > name, location (file path and line number), and language.
  >
  > The script employs a two-tiered approach for symbol generation:
  > 1.  **Universal Ctags (Preferred):** It first checks for the presence of the
  >     `ctags` command-line tool. If available, it uses `ctags` to perform a
  >     comprehensive, multi-language scan of the repository. This is the most
  >     robust and accurate method.
  > 2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
  >     back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
  >     method parses all `.py` files and extracts symbol information for Python
  >     code. While less comprehensive than `ctags`, it ensures that a baseline
  >     symbol map is always available.
  >
  > The resulting `symbols.json` artifact is a critical input for the agent's
  > orientation and planning phases, allowing it to quickly locate relevant code
  > and understand the structure of the repository without having to read every file.

- **`tooling/udc_orchestrator.py`**:

  > An orchestrator for executing Unrestricted Development Cycle (UDC) plans.
  >
  > This script provides a sandboxed environment for running UDC plans, which are
  > low-level assembly-like programs that can perform Turing-complete computations.
  > The orchestrator acts as a virtual machine with a tape-based memory model,
  > registers, and a set of simple instructions.
  >
  > To prevent non-termination and other resource-exhaustion issues, the
  > orchestrator imposes strict limits on the number of instructions executed,
  > the amount of memory used, and the total wall-clock time.

## Experimental Framework

The `experiments/` directory contains a framework for testing the agent's behavior in response to changes in its governing protocols (`AGENTS.md`). Each subdirectory within `experiments/` represents a self-contained experiment.

### Running an Experiment

To run an existing experiment (e.g., `scoped_protocol_override`):

1.  **Review the Experiment:** Read the `README.md` inside the experiment's directory (e.g., `experiments/scoped_protocol_override/README.md`) to understand its hypothesis, procedure, and expected outcome.
2.  **Perform the Baseline Run:** Follow the instructions in the experiment's `README.md` to establish the agent's baseline behavior. This usually involves performing a task in the root directory.
3.  **Perform the Experimental Run:** Follow the instructions to run the agent against the mutated protocol. This typically involves:
    a. Copying the `mutation.md` file to a new `AGENTS.md` file within the experiment's directory.
    b. Instructing the agent to perform the task specified in `task.md`, targeting the experiment's directory.
4.  **Compare the Results:** Observe the difference in the agent's behavior between the baseline and experimental runs to verify the hypothesis.

### Creating a New Experiment

1.  Create a new subdirectory in `experiments/`.
2.  Add a `README.md` file explaining the new experiment's hypothesis and procedure.
3.  Add a `mutation.md` file containing the altered `AGENTS.md` content.
4.  Add a `task.md` file describing the task the agent should perform.

---

# Module Documentation

## Overview

This document provides a human-readable summary of the protocols and key components defined within this module. It is automatically generated.

## Core Protocols

- **`dependency-management-001`**: A protocol for ensuring a reliable execution environment through formal dependency management.
- **`experimental-prologue-001`**: An experimental protocol to test dynamic rule-following. It mandates a prologue action before file creation.
- **`agent-shell-001`**: A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.
- **`toolchain-review-on-schema-change-001`**: A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.
- **`unified-auditor-001`**: A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.
- **`aura-execution-001`**: A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`capability-verification-001`**: A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.
- **`csdc-001`**: A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.
- **`unified-doc-builder-001`**: A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.
- **`file-indexing-001`**: A protocol for maintaining an up-to-date file index to accelerate tool performance.
- **`hdl-proving-001`**: A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.
- **`agent-interaction-001`**: A protocol governing the agent's core interaction and planning tools.
- **`plllu-execution-001`**: A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`security-header`**: Defines the identity and purpose of the Security Protocol document.
- **`security-vuln-reporting-001`**: Defines the official policy and procedure for reporting security vulnerabilities.
- **`speculative-execution-001`**: A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.

## Key Components

- **`tooling/__init__.py`**:

  > This module contains the various tools and utilities that support the agent's
  > development, testing, and operational workflows.
  >
  > The tools in this package are the building blocks of the agent's capabilities,
  > ranging from code analysis and refactoring to protocol compilation and
  > self-correction. Each script is designed to be a self-contained unit of
  > functionality that can be invoked either from the command line or programmatically
  > by the agent's master control system.
  >
  > This __init__.py file marks the 'tooling' directory as a Python package,
  > allowing for the organized import of its various modules.

- **`tooling/agent_shell.py`**:

  > The new, interactive, API-driven entry point for the agent.
  >
  > This script replaces the old file-based signaling system with a direct,
  > programmatic interface to the MasterControlGraph FSM. It is responsible for:
  > 1.  Initializing the agent's state and a centralized logger.
  > 2.  Instantiating and running the MasterControlGraph.
  > 3.  Driving the FSM by calling its methods and passing data and the logger.
  > 4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
  >     and respond to requests for action.

- **`tooling/__init__.py`**:

  > _No module-level docstring found._

- **`tooling/generate_and_test.py`**:

  > _No module-level docstring found._

- **`tooling/appl_runner.py`**:

  > A command-line tool for executing APPL files.
  >
  > This script provides a simple interface to run APPL files using the main
  > `run.py` interpreter. It captures and prints the output of the execution,
  > and provides detailed error reporting if the execution fails.

- **`tooling/appl_to_lfi_ill.py`**:

  > A compiler that translates APPL (a simple functional language) to LFI-ILL.
  >
  > This script takes a Python file containing an APPL AST, and compiles it into
  > an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/auditor.py`**:

  > A unified auditing tool for maintaining repository health and compliance.
  >
  > This script combines the functionality of several disparate auditing tools into a
  > single, comprehensive command-line interface. It serves as the central tool for
  > validating the key components of the agent's architecture, including protocols,
  > plans, and documentation.
  >
  > The auditor can perform the following checks:
  > 1.  **Protocol Audit (`protocol`):**
  >     - Checks if `AGENTS.md` artifacts are stale compared to their source files.
  >     - Verifies protocol completeness by comparing tools used in logs against
  >       tools defined in protocols.
  >     - Analyzes tool usage frequency (centrality).
  > 2.  **Plan Registry Audit (`plans`):**
  >     - Scans `knowledge_core/plan_registry.json` for "dead links" where the
  >       target plan file does not exist.
  > 3.  **Documentation Audit (`docs`):**
  >     - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
  >       that are missing module-level docstrings.
  >
  > The tool is designed to be run from the command line and can execute specific
  > audits or all of them, generating a consolidated `audit_report.md` file.

- **`tooling/aura_executor.py`**:

  > This script serves as the command-line executor for `.aura` files.
  >
  > It bridges the gap between the high-level Aura scripting language and the
  > agent's underlying Python-based toolset. The executor is responsible for:
  > 1.  Parsing the `.aura` source code using the lexer and parser from the
  >     `aura_lang` package.
  > 2.  Setting up an execution environment for the interpreter.
  > 3.  Injecting a "tool-calling" capability into the Aura environment, which
  >     allows Aura scripts to dynamically invoke registered Python tools
  >     (e.g., `hdl_prover`, `environmental_probe`).
  > 4.  Executing the parsed program and printing the final result.
  >
  > This makes it a key component for enabling more expressive and complex
  > automation scripts for the agent.

- **`tooling/aura_to_lfi_ill.py`**:

  > A compiler that translates AURA code to LFI-ILL.
  >
  > This script takes an AURA file, parses it, and compiles it into an LFI-ILL
  > AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/background_researcher.py`**:

  > This script performs a simulated research task in the background.
  > It takes a task ID as a command-line argument and writes its findings
  > to a temporary file that the main agent can poll.

- **`tooling/builder.py`**:

  > A unified, configuration-driven build script for the project.
  >
  > This script serves as the central entry point for all build-related tasks, such
  > as generating documentation, compiling protocols, and running code quality checks.
  > It replaces a traditional Makefile's direct command execution with a more
  > structured, maintainable, and introspectable approach.
  >
  > The core logic is driven by a `build_config.json` file, which defines a series
  > of "targets." Each target specifies:
  > - The `type` of target: "compiler" or "command".
  > - For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
  > - For "command" types: the `command` to execute.
  >
  > The configuration also defines "build_groups", which are ordered collections of
  > targets (e.g., "all", "quality").
  >
  > This centralized builder provides several advantages:
  > - **Single Source of Truth:** The `build_config.json` file is the definitive
  >   source for all build logic.
  > - **Consistency:** Ensures all build tasks are executed in a uniform way.
  > - **Extensibility:** New build targets can be added by simply updating the
  >   configuration file.
  > - **Discoverability:** The script can list all available targets and groups.

- **`tooling/capability_verifier.py`**:

  > A tool to verify that the agent can monotonically improve its capabilities.
  >
  > This script is designed to provide a formal, automated test for the agent's
  > self-correction and learning mechanisms. It ensures that when the agent learns
  > a new capability, it does so without losing (regressing) any of its existing
  > capabilities. This is a critical safeguard for ensuring robust and reliable
  > agent evolution.
  >
  > The tool works by orchestrating a four-step process:
  > 1.  **Confirm Initial Failure:** It runs a specific test file that is known to
  >     fail, verifying that the agent currently lacks the target capability.
  > 2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
  >     triggers the `self_correction_orchestrator.py` script, which is responsible
  >     for integrating new knowledge and skills.
  > 3.  **Confirm Final Success:** It runs the same test file again, confirming that
  >     the agent has successfully learned the new capability and the test now passes.
  > 4.  **Check for Regressions:** It runs the full, existing test suite to ensure
  >     that the process of learning the new skill has not inadvertently broken any
  >     previously functional capabilities.
  >
  > This provides a closed-loop verification of monotonic improvement, which is a
  > cornerstone of the agent's design philosophy.

- **`tooling/code_suggester.py`**:

  > Handles the generation and application of autonomous code change suggestions.
  >
  > This tool is a key component of the advanced self-correction loop. It is
  > designed to be invoked by the self-correction orchestrator when a lesson
  > contains a 'propose-code-change' action.
  >
  > For its initial implementation, this tool acts as a structured executor. It
  > takes a lesson where the 'details' field contains a fully-formed git-style
  > merge diff and applies it to the target file. It does this by generating a
  > temporary, single-step plan file and signaling its location for the master
  > controller to execute.
  >
  > This establishes the fundamental workflow for autonomous code modification,
  > decoupling the suggestion logic from the execution logic. Future iterations
  > can enhance this tool with more sophisticated code generation capabilities
  > (e.g., using an LLM to generate the diff from a natural language description)
  > without altering the core orchestration process.

- **`tooling/context_awareness_scanner.py`**:

  > A tool for performing static analysis on a Python file to understand its context.
  >
  > This script provides a "contextual awareness" scan of a specified Python file
  > to help an agent (or a human) understand its role, dependencies, and connections
  > within a larger codebase. This is crucial for planning complex changes or
  > refactoring efforts, as it provides a snapshot of the potential impact of
  > modifying a file.
  >
  > The scanner performs three main functions:
  > 1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
  >     module to parse the target file and identify all the functions and classes
  >     that are defined within it.
  > 2.  **Import Analysis:** It also uses the AST to find all modules and symbols
  >     that the target file imports, revealing its dependencies on other parts of
  >     the codebase or external libraries.
  > 3.  **Reference Finding:** It performs a repository-wide search to find all other
  >     files that reference the symbols defined in the target file. This helps to
  >     understand how the file is used by the rest of the system.
  >
  > The final output is a detailed JSON report containing all of this information,
  > which can be used as a foundational artifact for automated planning or human review.

- **`tooling/csdc_cli.py`**:

  > A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).
  >
  > This script provides an interface to validate a development plan against a specific
  > CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
  > plan adheres to the strict logical and computational constraints defined by the
  > CSDC protocol before it is executed.
  >
  > The tool performs two main checks:
  > 1.  **Complexity Analysis:** It analyzes the plan to determine its computational
  >     complexity and verifies that it matches the expected complexity class.
  > 2.  **Model Validation:** It validates the plan's commands against the rules of
  >     the specified CSDC model, ensuring that it does not violate any of the
  >     model's constraints (e.g., forbidding certain functions).
  >
  > This serves as a critical gateway for ensuring that all development work within
  > the CSDC framework is sound, predictable, and compliant with the governing
  > meta-mathematical principles.

- **`tooling/dependency_graph_generator.py`**:

  > Scans the repository for dependency files and generates a unified dependency graph.
  >
  > This script is a crucial component of the agent's environmental awareness,
  > providing a clear map of the software supply chain. It recursively searches the
  > entire repository for common dependency management files, specifically:
  > - `package.json` (for JavaScript/Node.js projects)
  > - `requirements.txt` (for Python projects)
  >
  > It parses these files to identify two key types of relationships:
  > 1.  **Internal Dependencies:** Links between different projects within this repository.
  > 2.  **External Dependencies:** Links to third-party libraries and packages.
  >
  > The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
  > represents these relationships as a graph structure with nodes (projects and
  > dependencies) and edges (the dependency links). This artifact is a primary
  > input for the agent's orientation and planning phases, allowing it to reason
  > about the potential impact of its changes.

- **`tooling/doc_builder.py`**:

  > A unified documentation builder for the project.
  > ...

- **`tooling/document_scanner.py`**:

  > A tool for scanning the repository for human-readable documents and extracting their text content.
  >
  > This script is a crucial component of the agent's initial information-gathering
  > and orientation phase. It allows the agent to ingest knowledge from unstructured
  > or semi-structured documents that are not part of the formal codebase, but which
  > may contain critical context, requirements, or specifications.
  >
  > The scanner searches a given directory for files with common document extensions:
  > - `.pdf`: Uses the `pypdf` library to extract text from PDF files.
  > - `.md`: Reads Markdown files.
  > - `.txt`: Reads plain text files.
  >
  > The output is a dictionary where the keys are the file paths of the discovered
  > documents and the values are their extracted text content. This data can then
  > be used by the agent to inform its planning and execution process. This tool
  > is essential for bridging the gap between human-written documentation and the
  > agent's operational awareness.

- **`tooling/environmental_probe.py`**:

  > Performs a series of checks to assess the capabilities of the execution environment.
  >
  > This script is a critical diagnostic tool run at the beginning of a task to
  > ensure the agent understands its operational sandbox. It verifies fundamental
  > capabilities required for most software development tasks:
  >
  > 1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
  >     and delete files. It also provides a basic latency measurement for these
  >     operations.
  > 2.  **Network Connectivity:** Checks for external network access by attempting to
  >     connect to a highly-available public endpoint (google.com). This is crucial
  >     for tasks requiring `git` operations, package downloads, or API calls.
  > 3.  **Environment Variables:** Verifies that standard environment variables are
  >     accessible, which is a prerequisite for many command-line tools.
  >
  > The script generates a human-readable report summarizing the results of these
  > probes, allowing the agent to quickly identify any environmental constraints
  > that might impact its ability to complete a task.

- **`tooling/fdc_cli.py`**:

  > This script provides a command-line interface (CLI) for managing the Finite
  > Development Cycle (FDC).
  >
  > The FDC is a structured workflow for agent-driven software development. This CLI
  > is the primary human interface for interacting with that cycle, providing
  > commands to:
  > - **start:** Initiates a new development task, triggering the "Advanced
  >   Orientation and Research Protocol" (AORP) to ensure the agent is fully
  >   contextualized.
  > - **close:** Formally concludes a task, creating a post-mortem template for
  >   analysis and lesson-learning.
  > - **validate:** Checks a given plan file for both syntactic and semantic
  >   correctness against the FDC's governing Finite State Machine (FSM). This
  >   ensures that a plan is executable and will not violate protocol.
  > - **analyze:** Examines a plan to determine its computational complexity (e.g.,
  >   Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  >   Read-Write), providing insight into the plan's potential impact.

- **`tooling/filesystem_lister.py`**:

  > A tool for listing files and directories in a repository, with an option to respect .gitignore.

- **`tooling/halting_heuristic_analyzer.py`**:

  > A static analysis tool to estimate the termination risk of a UDC plan.
  >
  > This script reads a `.udc` plan file, parses its instructions, and uses a
  > series of heuristics to identify potential infinite loops. It is not a
  > formal decider (as the halting problem is undecidable), but rather a
  > practical tool to flag common patterns that lead to non-termination.
  >
  > The analysis focuses on:
  > 1.  Detecting backward jumps, which are the primary indicator of loops.
  > 2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
  > 3.  Checking if the registers involved in the exit conditions are modified
  >     within the loop body in a way that is likely to lead to termination.
  >
  > The tool outputs a JSON report detailing the estimated risk level (LOW,
  > MEDIUM, HIGH) and the specific loops that were identified.

- **`tooling/hdl_prover.py`**:

  > A command-line tool for proving sequents in Intuitionistic Linear Logic.
  >
  > This script provides a basic interface to a simple logic prover. It takes a
  > sequent as a command-line argument, parses it into a logical structure, and
  > then attempts to prove it using a rudimentary proof search algorithm.
  >
  > The primary purpose of this tool is to allow the agent to perform formal
  > reasoning and verification tasks by checking the validity of logical entailments.
  > For example, it can be used to verify that a certain conclusion follows from a
  > set of premises according to the rules of linear logic.
  >
  > The current implementation uses a very basic parser and proof algorithm,
  > serving as a placeholder and demonstration for a more sophisticated, underlying
  > logic engine.

- **`tooling/hierarchical_compiler.py`**:

  > A hierarchical build system for compiling nested protocol modules.
  >
  > This script orchestrates the compilation of `AGENTS.md` and `README.md` files
  > across a repository with a nested or hierarchical module structure. It is a key
  > component of the system's ability to manage complexity by allowing protocols to
  > be defined in a modular, distributed way while still being presented as a unified,
  > coherent whole at each level of the hierarchy.
  >
  > The compiler operates in two main passes:
  >
  > **Pass 1: Documentation Compilation (Bottom-Up)**
  > 1.  **Discovery:** It finds all `protocols` directories in the repository, which
  >     signify the root of a documentation module.
  > 2.  **Bottom-Up Traversal:** It processes these directories from the most deeply
  >     nested ones upwards. This ensures that child modules are always built before
  >     their parents.
  > 3.  **Child Summary Injection:** For each compiled child module, it generates a
  >     summary of its protocols and injects this summary into the parent's
  >     `protocols` directory as a temporary file.
  > 4.  **Parent Compilation:** When the parent module is compiled, the standard
  >     `protocol_compiler.py` automatically includes the injected child summaries,
  >     creating a single `AGENTS.md` file that contains both the parent's native
  >     protocols and the full protocols of all its direct children.
  > 5.  **README Generation:** After each `AGENTS.md` is compiled, the corresponding
  >     `README.md` is generated.
  >
  > **Pass 2: Centralized Knowledge Graph Compilation**
  > 1.  After all documentation is built, it performs a full repository scan to find
  >     every `*.protocol.json` file.
  > 2.  It parses all of these files and compiles them into a single, centralized
  >     RDF knowledge graph (`protocols.ttl`). This provides a unified,
  >     machine-readable view of every protocol defined anywhere in the system.
  >
  > This hierarchical approach allows for both localized, context-specific protocol
  > definitions and a holistic, system-wide understanding of the agent's governing rules.

- **`tooling/knowledge_compiler.py`**:

  > Extracts structured lessons from post-mortem reports and compiles them into a
  > centralized, long-term knowledge base.
  >
  > This script is a core component of the agent's self-improvement feedback loop.
  > After a task is completed, a post-mortem report is generated that includes a
  > section for "Corrective Actions & Lessons Learned." This script automates the
  > process of parsing that section to extract key insights.
  >
  > It identifies pairs of "Lesson" and "Action" statements and transforms them
  > into a standardized, machine-readable format. These formatted entries are then
  > appended to the `knowledge_core/lessons.jsonl` file, which serves as the
  > agent's persistent memory of what has worked, what has failed, and what can be
  > improved in future tasks.
  >
  > The script is executed via the command line, taking the path to a completed
  > post-mortem file as its primary argument.

- **`tooling/knowledge_integrator.py`**:

  > Enriches the local knowledge graph with data from external sources like DBPedia.
  >
  > This script loads the RDF graph generated from the project's protocols,
  > identifies key concepts (like tools and rules), queries the DBPedia SPARQL
  > endpoint to find related information, and merges the external data into a new,
  > enriched knowledge graph.

- **`tooling/lba_validator.py`**:

  > A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.
  >
  > This module implements a validator that enforces the context-sensitive rules of the CSDC.
  > Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
  > validation decisions. This is necessary to enforce rules where the validity of one
  > command depends on the presence or absence of another command elsewhere in the plan.
  >
  > The CSDC defines two mutually exclusive models:
  > - Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
  > - Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.
  >
  > This validator checks for these co-occurrence constraints.

- **`tooling/lfi_ill_halting_decider.py`**:

  > A tool for analyzing the termination of LFI-ILL programs.
  >
  > This script takes an LFI-ILL file, interprets it in a paraconsistent logic
  > environment, and reports on its halting status. It does this by setting up
  > a paradoxical initial state and observing how the program resolves it.

- **`tooling/lfi_udc_model.py`**:

  > A paraconsistent execution model for UDC plans.
  >
  > This module provides the classes necessary to interpret a UDC (Un-decidable
  > Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
  > concrete values, the state of the machine (registers, tape, etc.) is modeled
  > using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).
  >
  > This allows the system to reason about paradoxical programs, such as a program
  > that halts if and only if it does not halt. By executing the program under
  > paraconsistent semantics, the model can arrive at a final state of `BOTH`,
  > effectively demonstrating the paradoxical nature of the input without crashing.
  >
  > Key classes:
  > - `ParaconsistentTruth`: An enum for the four truth values.
  > - `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
  > - `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
  > - `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
  > - `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  >   analysis of a UDC plan.

- **`tooling/log_failure.py`**:

  > A dedicated script to log a catastrophic failure event to the main activity log.
  >
  > This tool is designed to be invoked in the rare case of a severe, unrecoverable
  > error that violates a core protocol. Its primary purpose is to ensure that such
  > a critical event is formally and structurally documented in the standard agent
  > activity log (`logs/activity.log.jsonl`), even if the main agent loop has
  > crashed or been terminated.
  >
  > The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
  > attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
  > permanent, machine-readable record of the failure, which is essential for
  > post-mortem analysis, debugging, and the development of future safeguards.
  >
  > By using the standard `Logger` class, it ensures that the failure log entry
  > conforms to the established `LOGGING_SCHEMA.md`, making it processable by
  > auditing and analysis tools.

- **`tooling/master_control.py`**:

  > The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).
  >
  > This script, master_control.py, is the heart of the agent's operational loop.
  > It implements the CFDC, a hierarchical planning and execution model based on a
  > Pushdown Automaton. This allows the agent to execute complex tasks by calling
  > plans as sub-routines.
  >
  > Core Responsibilities:
  > - **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  >   plans to call other plans via the `call_plan` directive. This allows for
  >   modular, reusable, and complex task decomposition. A maximum recursion depth
  >   is enforced to guarantee decidability.
  > - **Plan Validation:** Contains the in-memory plan validator. Before execution,
  >   it parses a plan and simulates its execution against a Finite State Machine
  >   (FSM) to ensure it complies with the agent's operational protocols.
  > - **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  >   it first attempts to look up the plan by its logical name in the
  >   `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  >   the argument as a direct file path.
  > - **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  >   finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  >   to ensure predictable and auditable behavior.
  >
  > This module is designed as a library to be controlled by an external shell
  > (e.g., `agent_shell.py`), making its interaction purely programmatic.

- **`tooling/master_control_cli.py`**:

  > The official command-line interface for the agent's master control loop.
  >
  > This script is now a lightweight wrapper that passes control to the new,
  > API-driven `agent_shell.py`. It preserves the command-line interface while
  > decoupling the entry point from the FSM implementation.

- **`tooling/message_user.py`**:

  > A dummy tool that prints its arguments to simulate the message_user tool.
  >
  > This script is a simple command-line utility that takes a string as an
  > argument and prints it to standard output, prefixed with "[Message User]:".
  > Its purpose is to serve as a stand-in or mock for the actual `message_user`
  > tool in testing environments where the full agent framework is not required.
  >
  > This allows for the testing of scripts or workflows that call the
  > `message_user` tool without needing to invoke the entire agent messaging
  > subsystem.

- **`tooling/pda_parser.py`**:

  > A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.
  >
  > This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
  > parser for a simple, string-based representation of pLLLU formulas. It can
  > handle basic atomic formulas, unary operators (like negation and consistency),
  > and binary operators (like implication and conjunction).
  >
  > The main function `parse_formula` takes a string and returns a simple AST
  > (Abstract Syntax Tree) represented as nested tuples.

- **`tooling/plan_executor.py`**:

  > A simple plan executor for simulating agent behavior.
  >
  > This script reads a plan file, parses it, and executes the commands in a
  > simplified, simulated environment. It supports a limited set of tools
  > (`message_user` and `run_in_bash_session`) to provide a basic demonstration
  > of how an agent would execute a plan.

- **`tooling/plan_manager.py`**:

  > Provides a command-line interface for managing the agent's Plan Registry.
  >
  > This script is the administrative tool for the Plan Registry, a key component
  > of the Context-Free Development Cycle (CFDC) that enables hierarchical and
  > modular planning. The registry, located at `knowledge_core/plan_registry.json`,
  > maps human-readable, logical names to the file paths of specific plans. This
  > decouples the `call_plan` directive from hardcoded file paths, making plans
  > more reusable and the system more robust.
  >
  > This CLI provides three essential functions:
  > - **register**: Associates a new logical name with a plan file path, adding it
  >   to the central registry.
  > - **deregister**: Removes an existing logical name and its associated path from
  >   the registry.
  > - **list**: Displays all current name-to-path mappings in the registry.
  >
  > By providing a simple, standardized interface for managing this library of
  > reusable plans, this tool improves the agent's ability to compose complex
  > workflows from smaller, validated sub-plans.

- **`tooling/plan_parser.py`**:

  > Parses a plan file into a structured list of commands.
  >
  > This module provides the `parse_plan` function and the `Command` dataclass,
  > which are central to the agent's ability to understand and execute plans.
  > The parser correctly handles multi-line arguments and ignores comments,
  > allowing for robust and readable plan files.

- **`tooling/plllu_interpreter.py`**:

  > A resource-sensitive, four-valued interpreter for pLLLU formulas.
  >
  > This script implements an interpreter for the pLLLU language. It operates on
  > an AST generated by the `pda_parser.py` script. The interpreter is designed
  > to be resource-sensitive, meaning that each atomic formula in the initial
  > context must be consumed exactly once during the evaluation of the proof.
  >
  > The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
  > it to reason about paraconsistent and paracomplete states.
  >
  > The core of the interpreter is the `FourValuedInterpreter` class, which
  > recursively walks the AST, consuming resources from a context (a Counter of
  > available atoms) and returning the resulting logical value.

- **`tooling/plllu_runner.py`**:

  > A command-line runner for pLLLU files.
  >
  > This script provides an entry point for executing `.plllu` files. It
  > integrates the pLLLU lexer, parser, and interpreter to execute the logic
  > defined in a given pLLLU source file and print the result.

- **`tooling/pre_submit_check.py`**:

  > _No module-level docstring found._

- **`tooling/protocol_compiler.py`**:

  > Compiles source protocol files into unified, human-readable and machine-readable artifacts.
  >
  > This script is the engine behind the "protocol as code" principle. It discovers,
  > validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
  > into high-level documents like `AGENTS.md`.
  >
  > Key Functions:
  > - **Discovery:** Scans a directory for source files, including `.protocol.json`
  >   (machine-readable rules) and `.protocol.md` (human-readable context).
  > - **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  >   `.protocol.json` file, ensuring all protocol definitions are syntactically
  >   correct and adhere to the established structure.
  > - **Compilation:** Combines the human-readable markdown and the machine-readable
  >   JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
  > - **Documentation Injection:** Can inject other generated documents, like the
  >   `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
  > - **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  >   protocols and serialize them into an RDF knowledge graph (in Turtle format),
  >   creating a machine-queryable version of the agent's governing rules.
  >
  > This process ensures that `AGENTS.md` and other protocol documents are not edited
  > manually but are instead generated from a validated, single source of truth,
  > making the agent's protocols robust, verifiable, and maintainable.

- **`tooling/protocol_updater.py`**:

  > A command-line tool for programmatically updating protocol source files.
  >
  > This script provides the mechanism for the agent to perform self-correction
  > by modifying its own governing protocols based on structured, actionable
  > lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
  > workflow.
  >
  > The tool operates on the .protocol.json files located in the `protocols/`
  > directory, performing targeted updates based on command-line arguments.

- **`tooling/refactor.py`**:

  > A tool for performing automated symbol renaming in Python code.
  >
  > This script provides a command-line interface to find a specific symbol
  > (a function or a class) in a given Python file and rename it, along with all of
  > its textual references throughout the entire repository. This provides a safe
  > and automated way to perform a common refactoring task, reducing the risk of
  > manual errors.
  >
  > The tool operates in three main stages:
  > 1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
  >     to parse the source file and precisely locate the definition of the target
  >     symbol. This ensures that the tool is targeting the correct code construct.
  > 2.  **Reference Finding:** It performs a text-based search across the specified
  >     search path (defaulting to the entire repository) to find all files that
  >     mention the symbol's old name.
  > 3.  **Plan Generation:** Instead of modifying files directly, it generates a
  >     refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
  >     commands, one for each file that needs to be changed. The path to this
  >     generated plan file is printed to standard output.
  >
  > This plan-based approach allows the agent's master controller to execute the
  > refactoring in a controlled, verifiable, and atomic way, consistent with its
  > standard operational procedures.

- **`tooling/reliable_ls.py`**:

  > A tool for reliably listing files and directories.
  >
  > This script provides a consistent, sorted, and recursive listing of files and
  > directories, excluding the `.git` directory. It is intended to be a more
  > reliable alternative to the standard `ls` command for agent use cases.

- **`tooling/reorientation_manager.py`**:

  > Re-orientation Manager
  >
  > This script is the core of the automated re-orientation process. It is
  > designed to be triggered by the build system whenever the agent's core
  > protocols (`AGENTS.md`) are re-compiled.
  >
  > The manager performs the following key functions:
  > 1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
  >     version to identify new protocols, tools, or other key concepts that have
  >     been introduced.
  > 2.  **Temporal Orientation (Shallow Research):** For each new concept, it
  >     invokes the `temporal_orienter.py` tool to fetch a high-level summary from
  >     an external knowledge base like DBpedia. This ensures the agent has a
  >     baseline understanding of new terms.
  > 3.  **Knowledge Storage:** The summaries from the temporal orientation are
  >     stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
  >     creating a persistent, queryable knowledge artifact.
  > 4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
  >     change is deemed significant (e.g., the addition of a new core
  >     architectural protocol), it programmatically triggers a formal L4 Deep
  >     Research Cycle by creating a `deep_research_required.json` file.
  >
  > This automated workflow ensures that the agent never operates with an outdated
  > understanding of its own protocols. It closes the loop between protocol
  > modification and the agent's self-awareness, making the system more robust,
  > adaptive, and reliable.

- **`tooling/research.py`**:

  > This module contains the logic for executing research tasks based on a set of
  > constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
  > read_file, google_search) based on the specified target and scope.

- **`tooling/research_planner.py`**:

  > This module is responsible for generating a formal, FSM-compliant research plan
  > for a given topic. The output is a string that can be executed by the agent's
  > master controller.

- **`tooling/self_correction_orchestrator.py`**:

  > Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.
  >
  > This script is the engine of the automated feedback loop. It reads structured,
  > actionable lessons from `knowledge_core/lessons.jsonl` and uses the
  > `protocol_updater.py` tool to apply them to the source protocol files.

- **`tooling/self_improvement_cli.py`**:

  > Analyzes agent activity logs to identify opportunities for self-improvement.
  >
  > This script is a command-line tool that serves as a key part of the agent's
  > meta-cognitive loop. It parses the structured activity log
  > (`logs/activity.log.jsonl`) to identify patterns that may indicate
  > inefficiencies or errors in the agent's workflow.
  >
  > The primary analysis currently implemented is:
  > - **Planning Efficiency Analysis:** It scans the logs for tasks that required
  >   multiple `set_plan` actions. A high number of plan revisions for a single
  >   task can suggest that the initial planning phase was insufficient, the task
  >   was poorly understood, or the agent struggled to adapt to unforeseen
  >   challenges.
  >
  > By flagging these tasks, the script provides a starting point for a deeper
  > post-mortem analysis, helping the agent (or its developers) to understand the
  > root causes of the planning churn and to develop strategies for more effective
  > upfront planning in the future.
  >
  > The tool is designed to be extensible, with future analyses (such as error
  > rate tracking or tool usage anti-patterns) to be added as the system evolves.

- **`tooling/standard_agents_compiler.py`**:

  > A compiler that generates a simplified, standard-compliant `AGENTS.md` file.
  >
  > This script acts as an "adapter" to make the repository more accessible to
  > third-party AI agents that expect a conventional set of instructions. While the
  > repository's primary `AGENTS.md` is a complex, hierarchical, and
  > machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
  > file produced by this script offers a simple, human-readable summary of the
  > most common development commands.
  >
  > The script works by:
  > 1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
  >     which is the single source of truth for high-level commands. It specifically
  >     extracts the exact commands for common targets like `install`, `test`,
  >     `lint`, and `format`. This ensures the generated instructions are never
  >     stale.
  > 2.  **Injecting into a Template:** It injects these extracted commands into a
  >     pre-defined, user-friendly Markdown template.
  > 3.  **Generating the Artifact:** The final output is written to
  >     `AGENTS.standard.md`, providing a simple, stable, and conventional entry
  >     point for external tools, effectively bridging the gap between the complex
  >     internal protocol system and the broader agent ecosystem.

- **`tooling/state.py`**:

  > Defines the core data structures for managing the agent's state.
  >
  > This module provides the `AgentState` and `PlanContext` dataclasses, which are
  > fundamental to the operation of the Context-Free Development Cycle (CFDC). These
  > structures allow the `master_control.py` orchestrator to maintain a complete,
  > snapshot-able representation of the agent's progress through a task.
  >
  > - `AgentState`: The primary container for all information related to the current
  >   task, including the plan execution stack, message history, and error states.
  > - `PlanContext`: A specific structure that holds the state of a single plan
  >   file, including its content and the current execution step. This is the
  >   element that gets pushed onto the `plan_stack` in `AgentState`.
  >
  > Together, these classes enable the hierarchical, stack-based planning and
  > execution that is the hallmark of the CFDC.

- **`tooling/symbol_map_generator.py`**:

  > Generates a code symbol map for the repository to aid in contextual understanding.
  >
  > This script creates a `symbols.json` file in the `knowledge_core` directory,
  > which acts as a high-level index of the codebase. This map contains information
  > about key programming constructs like classes and functions, including their
  > name, location (file path and line number), and language.
  >
  > The script employs a two-tiered approach for symbol generation:
  > 1.  **Universal Ctags (Preferred):** It first checks for the presence of the
  >     `ctags` command-line tool. If available, it uses `ctags` to perform a
  >     comprehensive, multi-language scan of the repository. This is the most
  >     robust and accurate method.
  > 2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
  >     back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
  >     method parses all `.py` files and extracts symbol information for Python
  >     code. While less comprehensive than `ctags`, it ensures that a baseline
  >     symbol map is always available.
  >
  > The resulting `symbols.json` artifact is a critical input for the agent's
  > orientation and planning phases, allowing it to quickly locate relevant code
  > and understand the structure of the repository without having to read every file.

- **`tooling/udc_orchestrator.py`**:

  > An orchestrator for executing Unrestricted Development Cycle (UDC) plans.
  >
  > This script provides a sandboxed environment for running UDC plans, which are
  > low-level assembly-like programs that can perform Turing-complete computations.
  > The orchestrator acts as a virtual machine with a tape-based memory model,
  > registers, and a set of simple instructions.
  >
  > To prevent non-termination and other resource-exhaustion issues, the
  > orchestrator imposes strict limits on the number of instructions executed,
  > the amount of memory used, and the total wall-clock time.

## Experimental Framework

The `experiments/` directory contains a framework for testing the agent's behavior in response to changes in its governing protocols (`AGENTS.md`). Each subdirectory within `experiments/` represents a self-contained experiment.

### Running an Experiment

To run an existing experiment (e.g., `scoped_protocol_override`):

1.  **Review the Experiment:** Read the `README.md` inside the experiment's directory (e.g., `experiments/scoped_protocol_override/README.md`) to understand its hypothesis, procedure, and expected outcome.
2.  **Perform the Baseline Run:** Follow the instructions in the experiment's `README.md` to establish the agent's baseline behavior. This usually involves performing a task in the root directory.
3.  **Perform the Experimental Run:** Follow the instructions to run the agent against the mutated protocol. This typically involves:
    a. Copying the `mutation.md` file to a new `AGENTS.md` file within the experiment's directory.
    b. Instructing the agent to perform the task specified in `task.md`, targeting the experiment's directory.
4.  **Compare the Results:** Observe the difference in the agent's behavior between the baseline and experimental runs to verify the hypothesis.

### Creating a New Experiment

1.  Create a new subdirectory in `experiments/`.
2.  Add a `README.md` file explaining the new experiment's hypothesis and procedure.
3.  Add a `mutation.md` file containing the altered `AGENTS.md` content.
4.  Add a `task.md` file describing the task the agent should perform.

---

# Module Documentation

## Overview

This document provides a human-readable summary of the protocols and key components defined within this module. It is automatically generated.

## Core Protocols

- **`dependency-management-001`**: A protocol for ensuring a reliable execution environment through formal dependency management.
- **`experimental-prologue-001`**: An experimental protocol to test dynamic rule-following. It mandates a prologue action before file creation.
- **`agent-shell-001`**: A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.
- **`toolchain-review-on-schema-change-001`**: A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.
- **`unified-auditor-001`**: A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.
- **`aura-execution-001`**: A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`capability-verification-001`**: A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.
- **`csdc-001`**: A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.
- **`unified-doc-builder-001`**: A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.
- **`file-indexing-001`**: A protocol for maintaining an up-to-date file index to accelerate tool performance.
- **`hdl-proving-001`**: A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.
- **`agent-interaction-001`**: A protocol governing the agent's core interaction and planning tools.
- **`plllu-execution-001`**: A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`security-header`**: Defines the identity and purpose of the Security Protocol document.
- **`security-vuln-reporting-001`**: Defines the official policy and procedure for reporting security vulnerabilities.
- **`speculative-execution-001`**: A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.

## Key Components

- **`tooling/__init__.py`**:

  > This module contains the various tools and utilities that support the agent's
  > development, testing, and operational workflows.
  >
  > The tools in this package are the building blocks of the agent's capabilities,
  > ranging from code analysis and refactoring to protocol compilation and
  > self-correction. Each script is designed to be a self-contained unit of
  > functionality that can be invoked either from the command line or programmatically
  > by the agent's master control system.
  >
  > This __init__.py file marks the 'tooling' directory as a Python package,
  > allowing for the organized import of its various modules.

- **`tooling/agent_shell.py`**:

  > The new, interactive, API-driven entry point for the agent.
  >
  > This script replaces the old file-based signaling system with a direct,
  > programmatic interface to the MasterControlGraph FSM. It is responsible for:
  > 1.  Initializing the agent's state and a centralized logger.
  > 2.  Instantiating and running the MasterControlGraph.
  > 3.  Driving the FSM by calling its methods and passing data and the logger.
  > 4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
  >     and respond to requests for action.

- **`tooling/__init__.py`**:

  > _No module-level docstring found._

- **`tooling/generate_and_test.py`**:

  > _No module-level docstring found._

- **`tooling/appl_runner.py`**:

  > A command-line tool for executing APPL files.
  >
  > This script provides a simple interface to run APPL files using the main
  > `run.py` interpreter. It captures and prints the output of the execution,
  > and provides detailed error reporting if the execution fails.

- **`tooling/appl_to_lfi_ill.py`**:

  > A compiler that translates APPL (a simple functional language) to LFI-ILL.
  >
  > This script takes a Python file containing an APPL AST, and compiles it into
  > an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/auditor.py`**:

  > A unified auditing tool for maintaining repository health and compliance.
  >
  > This script combines the functionality of several disparate auditing tools into a
  > single, comprehensive command-line interface. It serves as the central tool for
  > validating the key components of the agent's architecture, including protocols,
  > plans, and documentation.
  >
  > The auditor can perform the following checks:
  > 1.  **Protocol Audit (`protocol`):**
  >     - Checks if `AGENTS.md` artifacts are stale compared to their source files.
  >     - Verifies protocol completeness by comparing tools used in logs against
  >       tools defined in protocols.
  >     - Analyzes tool usage frequency (centrality).
  > 2.  **Plan Registry Audit (`plans`):**
  >     - Scans `knowledge_core/plan_registry.json` for "dead links" where the
  >       target plan file does not exist.
  > 3.  **Documentation Audit (`docs`):**
  >     - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
  >       that are missing module-level docstrings.
  >
  > The tool is designed to be run from the command line and can execute specific
  > audits or all of them, generating a consolidated `audit_report.md` file.

- **`tooling/aura_executor.py`**:

  > This script serves as the command-line executor for `.aura` files.
  >
  > It bridges the gap between the high-level Aura scripting language and the
  > agent's underlying Python-based toolset. The executor is responsible for:
  > 1.  Parsing the `.aura` source code using the lexer and parser from the
  >     `aura_lang` package.
  > 2.  Setting up an execution environment for the interpreter.
  > 3.  Injecting a "tool-calling" capability into the Aura environment, which
  >     allows Aura scripts to dynamically invoke registered Python tools
  >     (e.g., `hdl_prover`, `environmental_probe`).
  > 4.  Executing the parsed program and printing the final result.
  >
  > This makes it a key component for enabling more expressive and complex
  > automation scripts for the agent.

- **`tooling/aura_to_lfi_ill.py`**:

  > A compiler that translates AURA code to LFI-ILL.
  >
  > This script takes an AURA file, parses it, and compiles it into an LFI-ILL
  > AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/background_researcher.py`**:

  > This script performs a simulated research task in the background.
  > It takes a task ID as a command-line argument and writes its findings
  > to a temporary file that the main agent can poll.

- **`tooling/builder.py`**:

  > A unified, configuration-driven build script for the project.
  >
  > This script serves as the central entry point for all build-related tasks, such
  > as generating documentation, compiling protocols, and running code quality checks.
  > It replaces a traditional Makefile's direct command execution with a more
  > structured, maintainable, and introspectable approach.
  >
  > The core logic is driven by a `build_config.json` file, which defines a series
  > of "targets." Each target specifies:
  > - The `type` of target: "compiler" or "command".
  > - For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
  > - For "command" types: the `command` to execute.
  >
  > The configuration also defines "build_groups", which are ordered collections of
  > targets (e.g., "all", "quality").
  >
  > This centralized builder provides several advantages:
  > - **Single Source of Truth:** The `build_config.json` file is the definitive
  >   source for all build logic.
  > - **Consistency:** Ensures all build tasks are executed in a uniform way.
  > - **Extensibility:** New build targets can be added by simply updating the
  >   configuration file.
  > - **Discoverability:** The script can list all available targets and groups.

- **`tooling/capability_verifier.py`**:

  > A tool to verify that the agent can monotonically improve its capabilities.
  >
  > This script is designed to provide a formal, automated test for the agent's
  > self-correction and learning mechanisms. It ensures that when the agent learns
  > a new capability, it does so without losing (regressing) any of its existing
  > capabilities. This is a critical safeguard for ensuring robust and reliable
  > agent evolution.
  >
  > The tool works by orchestrating a four-step process:
  > 1.  **Confirm Initial Failure:** It runs a specific test file that is known to
  >     fail, verifying that the agent currently lacks the target capability.
  > 2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
  >     triggers the `self_correction_orchestrator.py` script, which is responsible
  >     for integrating new knowledge and skills.
  > 3.  **Confirm Final Success:** It runs the same test file again, confirming that
  >     the agent has successfully learned the new capability and the test now passes.
  > 4.  **Check for Regressions:** It runs the full, existing test suite to ensure
  >     that the process of learning the new skill has not inadvertently broken any
  >     previously functional capabilities.
  >
  > This provides a closed-loop verification of monotonic improvement, which is a
  > cornerstone of the agent's design philosophy.

- **`tooling/code_suggester.py`**:

  > Handles the generation and application of autonomous code change suggestions.
  >
  > This tool is a key component of the advanced self-correction loop. It is
  > designed to be invoked by the self-correction orchestrator when a lesson
  > contains a 'propose-code-change' action.
  >
  > For its initial implementation, this tool acts as a structured executor. It
  > takes a lesson where the 'details' field contains a fully-formed git-style
  > merge diff and applies it to the target file. It does this by generating a
  > temporary, single-step plan file and signaling its location for the master
  > controller to execute.
  >
  > This establishes the fundamental workflow for autonomous code modification,
  > decoupling the suggestion logic from the execution logic. Future iterations
  > can enhance this tool with more sophisticated code generation capabilities
  > (e.g., using an LLM to generate the diff from a natural language description)
  > without altering the core orchestration process.

- **`tooling/context_awareness_scanner.py`**:

  > A tool for performing static analysis on a Python file to understand its context.
  >
  > This script provides a "contextual awareness" scan of a specified Python file
  > to help an agent (or a human) understand its role, dependencies, and connections
  > within a larger codebase. This is crucial for planning complex changes or
  > refactoring efforts, as it provides a snapshot of the potential impact of
  > modifying a file.
  >
  > The scanner performs three main functions:
  > 1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
  >     module to parse the target file and identify all the functions and classes
  >     that are defined within it.
  > 2.  **Import Analysis:** It also uses the AST to find all modules and symbols
  >     that the target file imports, revealing its dependencies on other parts of
  >     the codebase or external libraries.
  > 3.  **Reference Finding:** It performs a repository-wide search to find all other
  >     files that reference the symbols defined in the target file. This helps to
  >     understand how the file is used by the rest of the system.
  >
  > The final output is a detailed JSON report containing all of this information,
  > which can be used as a foundational artifact for automated planning or human review.

- **`tooling/csdc_cli.py`**:

  > A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).
  >
  > This script provides an interface to validate a development plan against a specific
  > CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
  > plan adheres to the strict logical and computational constraints defined by the
  > CSDC protocol before it is executed.
  >
  > The tool performs two main checks:
  > 1.  **Complexity Analysis:** It analyzes the plan to determine its computational
  >     complexity and verifies that it matches the expected complexity class.
  > 2.  **Model Validation:** It validates the plan's commands against the rules of
  >     the specified CSDC model, ensuring that it does not violate any of the
  >     model's constraints (e.g., forbidding certain functions).
  >
  > This serves as a critical gateway for ensuring that all development work within
  > the CSDC framework is sound, predictable, and compliant with the governing
  > meta-mathematical principles.

- **`tooling/dependency_graph_generator.py`**:

  > Scans the repository for dependency files and generates a unified dependency graph.
  >
  > This script is a crucial component of the agent's environmental awareness,
  > providing a clear map of the software supply chain. It recursively searches the
  > entire repository for common dependency management files, specifically:
  > - `package.json` (for JavaScript/Node.js projects)
  > - `requirements.txt` (for Python projects)
  >
  > It parses these files to identify two key types of relationships:
  > 1.  **Internal Dependencies:** Links between different projects within this repository.
  > 2.  **External Dependencies:** Links to third-party libraries and packages.
  >
  > The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
  > represents these relationships as a graph structure with nodes (projects and
  > dependencies) and edges (the dependency links). This artifact is a primary
  > input for the agent's orientation and planning phases, allowing it to reason
  > about the potential impact of its changes.

- **`tooling/doc_builder.py`**:

  > A unified documentation builder for the project.
  > ...

- **`tooling/document_scanner.py`**:

  > A tool for scanning the repository for human-readable documents and extracting their text content.
  >
  > This script is a crucial component of the agent's initial information-gathering
  > and orientation phase. It allows the agent to ingest knowledge from unstructured
  > or semi-structured documents that are not part of the formal codebase, but which
  > may contain critical context, requirements, or specifications.
  >
  > The scanner searches a given directory for files with common document extensions:
  > - `.pdf`: Uses the `pypdf` library to extract text from PDF files.
  > - `.md`: Reads Markdown files.
  > - `.txt`: Reads plain text files.
  >
  > The output is a dictionary where the keys are the file paths of the discovered
  > documents and the values are their extracted text content. This data can then
  > be used by the agent to inform its planning and execution process. This tool
  > is essential for bridging the gap between human-written documentation and the
  > agent's operational awareness.

- **`tooling/environmental_probe.py`**:

  > Performs a series of checks to assess the capabilities of the execution environment.
  >
  > This script is a critical diagnostic tool run at the beginning of a task to
  > ensure the agent understands its operational sandbox. It verifies fundamental
  > capabilities required for most software development tasks:
  >
  > 1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
  >     and delete files. It also provides a basic latency measurement for these
  >     operations.
  > 2.  **Network Connectivity:** Checks for external network access by attempting to
  >     connect to a highly-available public endpoint (google.com). This is crucial
  >     for tasks requiring `git` operations, package downloads, or API calls.
  > 3.  **Environment Variables:** Verifies that standard environment variables are
  >     accessible, which is a prerequisite for many command-line tools.
  >
  > The script generates a human-readable report summarizing the results of these
  > probes, allowing the agent to quickly identify any environmental constraints
  > that might impact its ability to complete a task.

- **`tooling/fdc_cli.py`**:

  > This script provides a command-line interface (CLI) for managing the Finite
  > Development Cycle (FDC).
  >
  > The FDC is a structured workflow for agent-driven software development. This CLI
  > is the primary human interface for interacting with that cycle, providing
  > commands to:
  > - **start:** Initiates a new development task, triggering the "Advanced
  >   Orientation and Research Protocol" (AORP) to ensure the agent is fully
  >   contextualized.
  > - **close:** Formally concludes a task, creating a post-mortem template for
  >   analysis and lesson-learning.
  > - **validate:** Checks a given plan file for both syntactic and semantic
  >   correctness against the FDC's governing Finite State Machine (FSM). This
  >   ensures that a plan is executable and will not violate protocol.
  > - **analyze:** Examines a plan to determine its computational complexity (e.g.,
  >   Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  >   Read-Write), providing insight into the plan's potential impact.

- **`tooling/filesystem_lister.py`**:

  > A tool for listing files and directories in a repository, with an option to respect .gitignore.

- **`tooling/halting_heuristic_analyzer.py`**:

  > A static analysis tool to estimate the termination risk of a UDC plan.
  >
  > This script reads a `.udc` plan file, parses its instructions, and uses a
  > series of heuristics to identify potential infinite loops. It is not a
  > formal decider (as the halting problem is undecidable), but rather a
  > practical tool to flag common patterns that lead to non-termination.
  >
  > The analysis focuses on:
  > 1.  Detecting backward jumps, which are the primary indicator of loops.
  > 2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
  > 3.  Checking if the registers involved in the exit conditions are modified
  >     within the loop body in a way that is likely to lead to termination.
  >
  > The tool outputs a JSON report detailing the estimated risk level (LOW,
  > MEDIUM, HIGH) and the specific loops that were identified.

- **`tooling/hdl_prover.py`**:

  > A command-line tool for proving sequents in Intuitionistic Linear Logic.
  >
  > This script provides a basic interface to a simple logic prover. It takes a
  > sequent as a command-line argument, parses it into a logical structure, and
  > then attempts to prove it using a rudimentary proof search algorithm.
  >
  > The primary purpose of this tool is to allow the agent to perform formal
  > reasoning and verification tasks by checking the validity of logical entailments.
  > For example, it can be used to verify that a certain conclusion follows from a
  > set of premises according to the rules of linear logic.
  >
  > The current implementation uses a very basic parser and proof algorithm,
  > serving as a placeholder and demonstration for a more sophisticated, underlying
  > logic engine.

- **`tooling/hierarchical_compiler.py`**:

  > A hierarchical build system for compiling nested protocol modules.
  >
  > This script orchestrates the compilation of `AGENTS.md` and `README.md` files
  > across a repository with a nested or hierarchical module structure. It is a key
  > component of the system's ability to manage complexity by allowing protocols to
  > be defined in a modular, distributed way while still being presented as a unified,
  > coherent whole at each level of the hierarchy.
  >
  > The compiler operates in two main passes:
  >
  > **Pass 1: Documentation Compilation (Bottom-Up)**
  > 1.  **Discovery:** It finds all `protocols` directories in the repository, which
  >     signify the root of a documentation module.
  > 2.  **Bottom-Up Traversal:** It processes these directories from the most deeply
  >     nested ones upwards. This ensures that child modules are always built before
  >     their parents.
  > 3.  **Child Summary Injection:** For each compiled child module, it generates a
  >     summary of its protocols and injects this summary into the parent's
  >     `protocols` directory as a temporary file.
  > 4.  **Parent Compilation:** When the parent module is compiled, the standard
  >     `protocol_compiler.py` automatically includes the injected child summaries,
  >     creating a single `AGENTS.md` file that contains both the parent's native
  >     protocols and the full protocols of all its direct children.
  > 5.  **README Generation:** After each `AGENTS.md` is compiled, the corresponding
  >     `README.md` is generated.
  >
  > **Pass 2: Centralized Knowledge Graph Compilation**
  > 1.  After all documentation is built, it performs a full repository scan to find
  >     every `*.protocol.json` file.
  > 2.  It parses all of these files and compiles them into a single, centralized
  >     RDF knowledge graph (`protocols.ttl`). This provides a unified,
  >     machine-readable view of every protocol defined anywhere in the system.
  >
  > This hierarchical approach allows for both localized, context-specific protocol
  > definitions and a holistic, system-wide understanding of the agent's governing rules.

- **`tooling/knowledge_compiler.py`**:

  > Extracts structured lessons from post-mortem reports and compiles them into a
  > centralized, long-term knowledge base.
  >
  > This script is a core component of the agent's self-improvement feedback loop.
  > After a task is completed, a post-mortem report is generated that includes a
  > section for "Corrective Actions & Lessons Learned." This script automates the
  > process of parsing that section to extract key insights.
  >
  > It identifies pairs of "Lesson" and "Action" statements and transforms them
  > into a standardized, machine-readable format. These formatted entries are then
  > appended to the `knowledge_core/lessons.jsonl` file, which serves as the
  > agent's persistent memory of what has worked, what has failed, and what can be
  > improved in future tasks.
  >
  > The script is executed via the command line, taking the path to a completed
  > post-mortem file as its primary argument.

- **`tooling/knowledge_integrator.py`**:

  > Enriches the local knowledge graph with data from external sources like DBPedia.
  >
  > This script loads the RDF graph generated from the project's protocols,
  > identifies key concepts (like tools and rules), queries the DBPedia SPARQL
  > endpoint to find related information, and merges the external data into a new,
  > enriched knowledge graph.

- **`tooling/lba_validator.py`**:

  > A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.
  >
  > This module implements a validator that enforces the context-sensitive rules of the CSDC.
  > Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
  > validation decisions. This is necessary to enforce rules where the validity of one
  > command depends on the presence or absence of another command elsewhere in the plan.
  >
  > The CSDC defines two mutually exclusive models:
  > - Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
  > - Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.
  >
  > This validator checks for these co-occurrence constraints.

- **`tooling/lfi_ill_halting_decider.py`**:

  > A tool for analyzing the termination of LFI-ILL programs.
  >
  > This script takes an LFI-ILL file, interprets it in a paraconsistent logic
  > environment, and reports on its halting status. It does this by setting up
  > a paradoxical initial state and observing how the program resolves it.

- **`tooling/lfi_udc_model.py`**:

  > A paraconsistent execution model for UDC plans.
  >
  > This module provides the classes necessary to interpret a UDC (Un-decidable
  > Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
  > concrete values, the state of the machine (registers, tape, etc.) is modeled
  > using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).
  >
  > This allows the system to reason about paradoxical programs, such as a program
  > that halts if and only if it does not halt. By executing the program under
  > paraconsistent semantics, the model can arrive at a final state of `BOTH`,
  > effectively demonstrating the paradoxical nature of the input without crashing.
  >
  > Key classes:
  > - `ParaconsistentTruth`: An enum for the four truth values.
  > - `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
  > - `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
  > - `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
  > - `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  >   analysis of a UDC plan.

- **`tooling/log_failure.py`**:

  > A dedicated script to log a catastrophic failure event to the main activity log.
  >
  > This tool is designed to be invoked in the rare case of a severe, unrecoverable
  > error that violates a core protocol. Its primary purpose is to ensure that such
  > a critical event is formally and structurally documented in the standard agent
  > activity log (`logs/activity.log.jsonl`), even if the main agent loop has
  > crashed or been terminated.
  >
  > The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
  > attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
  > permanent, machine-readable record of the failure, which is essential for
  > post-mortem analysis, debugging, and the development of future safeguards.
  >
  > By using the standard `Logger` class, it ensures that the failure log entry
  > conforms to the established `LOGGING_SCHEMA.md`, making it processable by
  > auditing and analysis tools.

- **`tooling/master_control.py`**:

  > The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).
  >
  > This script, master_control.py, is the heart of the agent's operational loop.
  > It implements the CFDC, a hierarchical planning and execution model based on a
  > Pushdown Automaton. This allows the agent to execute complex tasks by calling
  > plans as sub-routines.
  >
  > Core Responsibilities:
  > - **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  >   plans to call other plans via the `call_plan` directive. This allows for
  >   modular, reusable, and complex task decomposition. A maximum recursion depth
  >   is enforced to guarantee decidability.
  > - **Plan Validation:** Contains the in-memory plan validator. Before execution,
  >   it parses a plan and simulates its execution against a Finite State Machine
  >   (FSM) to ensure it complies with the agent's operational protocols.
  > - **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  >   it first attempts to look up the plan by its logical name in the
  >   `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  >   the argument as a direct file path.
  > - **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  >   finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  >   to ensure predictable and auditable behavior.
  >
  > This module is designed as a library to be controlled by an external shell
  > (e.g., `agent_shell.py`), making its interaction purely programmatic.

- **`tooling/master_control_cli.py`**:

  > The official command-line interface for the agent's master control loop.
  >
  > This script is now a lightweight wrapper that passes control to the new,
  > API-driven `agent_shell.py`. It preserves the command-line interface while
  > decoupling the entry point from the FSM implementation.

- **`tooling/message_user.py`**:

  > A dummy tool that prints its arguments to simulate the message_user tool.
  >
  > This script is a simple command-line utility that takes a string as an
  > argument and prints it to standard output, prefixed with "[Message User]:".
  > Its purpose is to serve as a stand-in or mock for the actual `message_user`
  > tool in testing environments where the full agent framework is not required.
  >
  > This allows for the testing of scripts or workflows that call the
  > `message_user` tool without needing to invoke the entire agent messaging
  > subsystem.

- **`tooling/pda_parser.py`**:

  > A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.
  >
  > This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
  > parser for a simple, string-based representation of pLLLU formulas. It can
  > handle basic atomic formulas, unary operators (like negation and consistency),
  > and binary operators (like implication and conjunction).
  >
  > The main function `parse_formula` takes a string and returns a simple AST
  > (Abstract Syntax Tree) represented as nested tuples.

- **`tooling/plan_executor.py`**:

  > A simple plan executor for simulating agent behavior.
  >
  > This script reads a plan file, parses it, and executes the commands in a
  > simplified, simulated environment. It supports a limited set of tools
  > (`message_user` and `run_in_bash_session`) to provide a basic demonstration
  > of how an agent would execute a plan.

- **`tooling/plan_manager.py`**:

  > Provides a command-line interface for managing the agent's Plan Registry.
  >
  > This script is the administrative tool for the Plan Registry, a key component
  > of the Context-Free Development Cycle (CFDC) that enables hierarchical and
  > modular planning. The registry, located at `knowledge_core/plan_registry.json`,
  > maps human-readable, logical names to the file paths of specific plans. This
  > decouples the `call_plan` directive from hardcoded file paths, making plans
  > more reusable and the system more robust.
  >
  > This CLI provides three essential functions:
  > - **register**: Associates a new logical name with a plan file path, adding it
  >   to the central registry.
  > - **deregister**: Removes an existing logical name and its associated path from
  >   the registry.
  > - **list**: Displays all current name-to-path mappings in the registry.
  >
  > By providing a simple, standardized interface for managing this library of
  > reusable plans, this tool improves the agent's ability to compose complex
  > workflows from smaller, validated sub-plans.

- **`tooling/plan_parser.py`**:

  > Parses a plan file into a structured list of commands.
  >
  > This module provides the `parse_plan` function and the `Command` dataclass,
  > which are central to the agent's ability to understand and execute plans.
  > The parser correctly handles multi-line arguments and ignores comments,
  > allowing for robust and readable plan files.

- **`tooling/plllu_interpreter.py`**:

  > A resource-sensitive, four-valued interpreter for pLLLU formulas.
  >
  > This script implements an interpreter for the pLLLU language. It operates on
  > an AST generated by the `pda_parser.py` script. The interpreter is designed
  > to be resource-sensitive, meaning that each atomic formula in the initial
  > context must be consumed exactly once during the evaluation of the proof.
  >
  > The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
  > it to reason about paraconsistent and paracomplete states.
  >
  > The core of the interpreter is the `FourValuedInterpreter` class, which
  > recursively walks the AST, consuming resources from a context (a Counter of
  > available atoms) and returning the resulting logical value.

- **`tooling/plllu_runner.py`**:

  > A command-line runner for pLLLU files.
  >
  > This script provides an entry point for executing `.plllu` files. It
  > integrates the pLLLU lexer, parser, and interpreter to execute the logic
  > defined in a given pLLLU source file and print the result.

- **`tooling/pre_submit_check.py`**:

  > _No module-level docstring found._

- **`tooling/protocol_compiler.py`**:

  > Compiles source protocol files into unified, human-readable and machine-readable artifacts.
  >
  > This script is the engine behind the "protocol as code" principle. It discovers,
  > validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
  > into high-level documents like `AGENTS.md`.
  >
  > Key Functions:
  > - **Discovery:** Scans a directory for source files, including `.protocol.json`
  >   (machine-readable rules) and `.protocol.md` (human-readable context).
  > - **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  >   `.protocol.json` file, ensuring all protocol definitions are syntactically
  >   correct and adhere to the established structure.
  > - **Compilation:** Combines the human-readable markdown and the machine-readable
  >   JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
  > - **Documentation Injection:** Can inject other generated documents, like the
  >   `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
  > - **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  >   protocols and serialize them into an RDF knowledge graph (in Turtle format),
  >   creating a machine-queryable version of the agent's governing rules.
  >
  > This process ensures that `AGENTS.md` and other protocol documents are not edited
  > manually but are instead generated from a validated, single source of truth,
  > making the agent's protocols robust, verifiable, and maintainable.

- **`tooling/protocol_updater.py`**:

  > A command-line tool for programmatically updating protocol source files.
  >
  > This script provides the mechanism for the agent to perform self-correction
  > by modifying its own governing protocols based on structured, actionable
  > lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
  > workflow.
  >
  > The tool operates on the .protocol.json files located in the `protocols/`
  > directory, performing targeted updates based on command-line arguments.

- **`tooling/refactor.py`**:

  > A tool for performing automated symbol renaming in Python code.
  >
  > This script provides a command-line interface to find a specific symbol
  > (a function or a class) in a given Python file and rename it, along with all of
  > its textual references throughout the entire repository. This provides a safe
  > and automated way to perform a common refactoring task, reducing the risk of
  > manual errors.
  >
  > The tool operates in three main stages:
  > 1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
  >     to parse the source file and precisely locate the definition of the target
  >     symbol. This ensures that the tool is targeting the correct code construct.
  > 2.  **Reference Finding:** It performs a text-based search across the specified
  >     search path (defaulting to the entire repository) to find all files that
  >     mention the symbol's old name.
  > 3.  **Plan Generation:** Instead of modifying files directly, it generates a
  >     refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
  >     commands, one for each file that needs to be changed. The path to this
  >     generated plan file is printed to standard output.
  >
  > This plan-based approach allows the agent's master controller to execute the
  > refactoring in a controlled, verifiable, and atomic way, consistent with its
  > standard operational procedures.

- **`tooling/reliable_ls.py`**:

  > A tool for reliably listing files and directories.
  >
  > This script provides a consistent, sorted, and recursive listing of files and
  > directories, excluding the `.git` directory. It is intended to be a more
  > reliable alternative to the standard `ls` command for agent use cases.

- **`tooling/reorientation_manager.py`**:

  > Re-orientation Manager
  >
  > This script is the core of the automated re-orientation process. It is
  > designed to be triggered by the build system whenever the agent's core
  > protocols (`AGENTS.md`) are re-compiled.
  >
  > The manager performs the following key functions:
  > 1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
  >     version to identify new protocols, tools, or other key concepts that have
  >     been introduced.
  > 2.  **Temporal Orientation (Shallow Research):** For each new concept, it
  >     invokes the `temporal_orienter.py` tool to fetch a high-level summary from
  >     an external knowledge base like DBpedia. This ensures the agent has a
  >     baseline understanding of new terms.
  > 3.  **Knowledge Storage:** The summaries from the temporal orientation are
  >     stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
  >     creating a persistent, queryable knowledge artifact.
  > 4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
  >     change is deemed significant (e.g., the addition of a new core
  >     architectural protocol), it programmatically triggers a formal L4 Deep
  >     Research Cycle by creating a `deep_research_required.json` file.
  >
  > This automated workflow ensures that the agent never operates with an outdated
  > understanding of its own protocols. It closes the loop between protocol
  > modification and the agent's self-awareness, making the system more robust,
  > adaptive, and reliable.

- **`tooling/research.py`**:

  > This module contains the logic for executing research tasks based on a set of
  > constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
  > read_file, google_search) based on the specified target and scope.

- **`tooling/research_planner.py`**:

  > This module is responsible for generating a formal, FSM-compliant research plan
  > for a given topic. The output is a string that can be executed by the agent's
  > master controller.

- **`tooling/self_correction_orchestrator.py`**:

  > Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.
  >
  > This script is the engine of the automated feedback loop. It reads structured,
  > actionable lessons from `knowledge_core/lessons.jsonl` and uses the
  > `protocol_updater.py` tool to apply them to the source protocol files.

- **`tooling/self_improvement_cli.py`**:

  > Analyzes agent activity logs to identify opportunities for self-improvement.
  >
  > This script is a command-line tool that serves as a key part of the agent's
  > meta-cognitive loop. It parses the structured activity log
  > (`logs/activity.log.jsonl`) to identify patterns that may indicate
  > inefficiencies or errors in the agent's workflow.
  >
  > The primary analysis currently implemented is:
  > - **Planning Efficiency Analysis:** It scans the logs for tasks that required
  >   multiple `set_plan` actions. A high number of plan revisions for a single
  >   task can suggest that the initial planning phase was insufficient, the task
  >   was poorly understood, or the agent struggled to adapt to unforeseen
  >   challenges.
  >
  > By flagging these tasks, the script provides a starting point for a deeper
  > post-mortem analysis, helping the agent (or its developers) to understand the
  > root causes of the planning churn and to develop strategies for more effective
  > upfront planning in the future.
  >
  > The tool is designed to be extensible, with future analyses (such as error
  > rate tracking or tool usage anti-patterns) to be added as the system evolves.

- **`tooling/standard_agents_compiler.py`**:

  > A compiler that generates a simplified, standard-compliant `AGENTS.md` file.
  >
  > This script acts as an "adapter" to make the repository more accessible to
  > third-party AI agents that expect a conventional set of instructions. While the
  > repository's primary `AGENTS.md` is a complex, hierarchical, and
  > machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
  > file produced by this script offers a simple, human-readable summary of the
  > most common development commands.
  >
  > The script works by:
  > 1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
  >     which is the single source of truth for high-level commands. It specifically
  >     extracts the exact commands for common targets like `install`, `test`,
  >     `lint`, and `format`. This ensures the generated instructions are never
  >     stale.
  > 2.  **Injecting into a Template:** It injects these extracted commands into a
  >     pre-defined, user-friendly Markdown template.
  > 3.  **Generating the Artifact:** The final output is written to
  >     `AGENTS.standard.md`, providing a simple, stable, and conventional entry
  >     point for external tools, effectively bridging the gap between the complex
  >     internal protocol system and the broader agent ecosystem.

- **`tooling/state.py`**:

  > Defines the core data structures for managing the agent's state.
  >
  > This module provides the `AgentState` and `PlanContext` dataclasses, which are
  > fundamental to the operation of the Context-Free Development Cycle (CFDC). These
  > structures allow the `master_control.py` orchestrator to maintain a complete,
  > snapshot-able representation of the agent's progress through a task.
  >
  > - `AgentState`: The primary container for all information related to the current
  >   task, including the plan execution stack, message history, and error states.
  > - `PlanContext`: A specific structure that holds the state of a single plan
  >   file, including its content and the current execution step. This is the
  >   element that gets pushed onto the `plan_stack` in `AgentState`.
  >
  > Together, these classes enable the hierarchical, stack-based planning and
  > execution that is the hallmark of the CFDC.

- **`tooling/symbol_map_generator.py`**:

  > Generates a code symbol map for the repository to aid in contextual understanding.
  >
  > This script creates a `symbols.json` file in the `knowledge_core` directory,
  > which acts as a high-level index of the codebase. This map contains information
  > about key programming constructs like classes and functions, including their
  > name, location (file path and line number), and language.
  >
  > The script employs a two-tiered approach for symbol generation:
  > 1.  **Universal Ctags (Preferred):** It first checks for the presence of the
  >     `ctags` command-line tool. If available, it uses `ctags` to perform a
  >     comprehensive, multi-language scan of the repository. This is the most
  >     robust and accurate method.
  > 2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
  >     back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
  >     method parses all `.py` files and extracts symbol information for Python
  >     code. While less comprehensive than `ctags`, it ensures that a baseline
  >     symbol map is always available.
  >
  > The resulting `symbols.json` artifact is a critical input for the agent's
  > orientation and planning phases, allowing it to quickly locate relevant code
  > and understand the structure of the repository without having to read every file.

- **`tooling/udc_orchestrator.py`**:

  > An orchestrator for executing Unrestricted Development Cycle (UDC) plans.
  >
  > This script provides a sandboxed environment for running UDC plans, which are
  > low-level assembly-like programs that can perform Turing-complete computations.
  > The orchestrator acts as a virtual machine with a tape-based memory model,
  > registers, and a set of simple instructions.
  >
  > To prevent non-termination and other resource-exhaustion issues, the
  > orchestrator imposes strict limits on the number of instructions executed,
  > the amount of memory used, and the total wall-clock time.

## Experimental Framework

The `experiments/` directory contains a framework for testing the agent's behavior in response to changes in its governing protocols (`AGENTS.md`). Each subdirectory within `experiments/` represents a self-contained experiment.

### Running an Experiment

To run an existing experiment (e.g., `scoped_protocol_override`):

1.  **Review the Experiment:** Read the `README.md` inside the experiment's directory (e.g., `experiments/scoped_protocol_override/README.md`) to understand its hypothesis, procedure, and expected outcome.
2.  **Perform the Baseline Run:** Follow the instructions in the experiment's `README.md` to establish the agent's baseline behavior. This usually involves performing a task in the root directory.
3.  **Perform the Experimental Run:** Follow the instructions to run the agent against the mutated protocol. This typically involves:
    a. Copying the `mutation.md` file to a new `AGENTS.md` file within the experiment's directory.
    b. Instructing the agent to perform the task specified in `task.md`, targeting the experiment's directory.
4.  **Compare the Results:** Observe the difference in the agent's behavior between the baseline and experimental runs to verify the hypothesis.

### Creating a New Experiment

1.  Create a new subdirectory in `experiments/`.
2.  Add a `README.md` file explaining the new experiment's hypothesis and procedure.
3.  Add a `mutation.md` file containing the altered `AGENTS.md` content.
4.  Add a `task.md` file describing the task the agent should perform.

---

# Module Documentation

## Overview

This document provides a human-readable summary of the protocols and key components defined within this module. It is automatically generated.

## Core Protocols

- **`dependency-management-001`**: A protocol for ensuring a reliable execution environment through formal dependency management.
- **`experimental-prologue-001`**: An experimental protocol to test dynamic rule-following. It mandates a prologue action before file creation.
- **`agent-shell-001`**: A protocol governing the use of the interactive agent shell as the primary entry point for all tasks.
- **`toolchain-review-on-schema-change-001`**: A meta-protocol to ensure the agent's toolchain remains synchronized with the architecture of its governing protocols.
- **`unified-auditor-001`**: A protocol for the unified repository auditing tool, which combines multiple health and compliance checks into a single interface.
- **`aura-execution-001`**: A protocol for executing Aura scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`capability-verification-001`**: A protocol for using the capability verifier tool to empirically test the agent's monotonic improvement.
- **`csdc-001`**: A protocol for the Context-Sensitive Development Cycle (CSDC), which introduces development models based on logical constraints.
- **`unified-doc-builder-001`**: A protocol for the unified documentation builder, which generates various documentation artifacts from the repository's sources of truth.
- **`file-indexing-001`**: A protocol for maintaining an up-to-date file index to accelerate tool performance.
- **`hdl-proving-001`**: A protocol for interacting with the Hypersequent-calculus-based logic engine, allowing the agent to perform formal logical proofs.
- **`agent-interaction-001`**: A protocol governing the agent's core interaction and planning tools.
- **`plllu-execution-001`**: A protocol for executing pLLLU scripts, enabling a more expressive and powerful planning and automation language for the agent.
- **`security-header`**: Defines the identity and purpose of the Security Protocol document.
- **`security-vuln-reporting-001`**: Defines the official policy and procedure for reporting security vulnerabilities.
- **`speculative-execution-001`**: A protocol that governs the agent's ability to initiate and execute self-generated, creative, or exploratory tasks during idle periods.

## Key Components

- **`tooling/__init__.py`**:

  > This module contains the various tools and utilities that support the agent's
  > development, testing, and operational workflows.
  >
  > The tools in this package are the building blocks of the agent's capabilities,
  > ranging from code analysis and refactoring to protocol compilation and
  > self-correction. Each script is designed to be a self-contained unit of
  > functionality that can be invoked either from the command line or programmatically
  > by the agent's master control system.
  >
  > This __init__.py file marks the 'tooling' directory as a Python package,
  > allowing for the organized import of its various modules.

- **`tooling/agent_shell.py`**:

  > The new, interactive, API-driven entry point for the agent.
  >
  > This script replaces the old file-based signaling system with a direct,
  > programmatic interface to the MasterControlGraph FSM. It is responsible for:
  > 1.  Initializing the agent's state and a centralized logger.
  > 2.  Instantiating and running the MasterControlGraph.
  > 3.  Driving the FSM by calling its methods and passing data and the logger.
  > 4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
  >     and respond to requests for action.

- **`tooling/__init__.py`**:

  > _No module-level docstring found._

- **`tooling/generate_and_test.py`**:

  > _No module-level docstring found._

- **`tooling/appl_runner.py`**:

  > A command-line tool for executing APPL files.
  >
  > This script provides a simple interface to run APPL files using the main
  > `run.py` interpreter. It captures and prints the output of the execution,
  > and provides detailed error reporting if the execution fails.

- **`tooling/appl_to_lfi_ill.py`**:

  > A compiler that translates APPL (a simple functional language) to LFI-ILL.
  >
  > This script takes a Python file containing an APPL AST, and compiles it into
  > an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/auditor.py`**:

  > A unified auditing tool for maintaining repository health and compliance.
  >
  > This script combines the functionality of several disparate auditing tools into a
  > single, comprehensive command-line interface. It serves as the central tool for
  > validating the key components of the agent's architecture, including protocols,
  > plans, and documentation.
  >
  > The auditor can perform the following checks:
  > 1.  **Protocol Audit (`protocol`):**
  >     - Checks if `AGENTS.md` artifacts are stale compared to their source files.
  >     - Verifies protocol completeness by comparing tools used in logs against
  >       tools defined in protocols.
  >     - Analyzes tool usage frequency (centrality).
  > 2.  **Plan Registry Audit (`plans`):**
  >     - Scans `knowledge_core/plan_registry.json` for "dead links" where the
  >       target plan file does not exist.
  > 3.  **Documentation Audit (`docs`):**
  >     - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
  >       that are missing module-level docstrings.
  >
  > The tool is designed to be run from the command line and can execute specific
  > audits or all of them, generating a consolidated `audit_report.md` file.

- **`tooling/aura_executor.py`**:

  > This script serves as the command-line executor for `.aura` files.
  >
  > It bridges the gap between the high-level Aura scripting language and the
  > agent's underlying Python-based toolset. The executor is responsible for:
  > 1.  Parsing the `.aura` source code using the lexer and parser from the
  >     `aura_lang` package.
  > 2.  Setting up an execution environment for the interpreter.
  > 3.  Injecting a "tool-calling" capability into the Aura environment, which
  >     allows Aura scripts to dynamically invoke registered Python tools
  >     (e.g., `hdl_prover`, `environmental_probe`).
  > 4.  Executing the parsed program and printing the final result.
  >
  > This makes it a key component for enabling more expressive and complex
  > automation scripts for the agent.

- **`tooling/aura_to_lfi_ill.py`**:

  > A compiler that translates AURA code to LFI-ILL.
  >
  > This script takes an AURA file, parses it, and compiles it into an LFI-ILL
  > AST. The resulting AST is then written to a `.lfi_ill` file.

- **`tooling/background_researcher.py`**:

  > This script performs a simulated research task in the background.
  > It takes a task ID as a command-line argument and writes its findings
  > to a temporary file that the main agent can poll.

- **`tooling/builder.py`**:

  > A unified, configuration-driven build script for the project.
  >
  > This script serves as the central entry point for all build-related tasks, such
  > as generating documentation, compiling protocols, and running code quality checks.
  > It replaces a traditional Makefile's direct command execution with a more
  > structured, maintainable, and introspectable approach.
  >
  > The core logic is driven by a `build_config.json` file, which defines a series
  > of "targets." Each target specifies:
  > - The `type` of target: "compiler" or "command".
  > - For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
  > - For "command" types: the `command` to execute.
  >
  > The configuration also defines "build_groups", which are ordered collections of
  > targets (e.g., "all", "quality").
  >
  > This centralized builder provides several advantages:
  > - **Single Source of Truth:** The `build_config.json` file is the definitive
  >   source for all build logic.
  > - **Consistency:** Ensures all build tasks are executed in a uniform way.
  > - **Extensibility:** New build targets can be added by simply updating the
  >   configuration file.
  > - **Discoverability:** The script can list all available targets and groups.

- **`tooling/capability_verifier.py`**:

  > A tool to verify that the agent can monotonically improve its capabilities.
  >
  > This script is designed to provide a formal, automated test for the agent's
  > self-correction and learning mechanisms. It ensures that when the agent learns
  > a new capability, it does so without losing (regressing) any of its existing
  > capabilities. This is a critical safeguard for ensuring robust and reliable
  > agent evolution.
  >
  > The tool works by orchestrating a four-step process:
  > 1.  **Confirm Initial Failure:** It runs a specific test file that is known to
  >     fail, verifying that the agent currently lacks the target capability.
  > 2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
  >     triggers the `self_correction_orchestrator.py` script, which is responsible
  >     for integrating new knowledge and skills.
  > 3.  **Confirm Final Success:** It runs the same test file again, confirming that
  >     the agent has successfully learned the new capability and the test now passes.
  > 4.  **Check for Regressions:** It runs the full, existing test suite to ensure
  >     that the process of learning the new skill has not inadvertently broken any
  >     previously functional capabilities.
  >
  > This provides a closed-loop verification of monotonic improvement, which is a
  > cornerstone of the agent's design philosophy.

- **`tooling/code_suggester.py`**:

  > Handles the generation and application of autonomous code change suggestions.
  >
  > This tool is a key component of the advanced self-correction loop. It is
  > designed to be invoked by the self-correction orchestrator when a lesson
  > contains a 'propose-code-change' action.
  >
  > For its initial implementation, this tool acts as a structured executor. It
  > takes a lesson where the 'details' field contains a fully-formed git-style
  > merge diff and applies it to the target file. It does this by generating a
  > temporary, single-step plan file and signaling its location for the master
  > controller to execute.
  >
  > This establishes the fundamental workflow for autonomous code modification,
  > decoupling the suggestion logic from the execution logic. Future iterations
  > can enhance this tool with more sophisticated code generation capabilities
  > (e.g., using an LLM to generate the diff from a natural language description)
  > without altering the core orchestration process.

- **`tooling/context_awareness_scanner.py`**:

  > A tool for performing static analysis on a Python file to understand its context.
  >
  > This script provides a "contextual awareness" scan of a specified Python file
  > to help an agent (or a human) understand its role, dependencies, and connections
  > within a larger codebase. This is crucial for planning complex changes or
  > refactoring efforts, as it provides a snapshot of the potential impact of
  > modifying a file.
  >
  > The scanner performs three main functions:
  > 1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
  >     module to parse the target file and identify all the functions and classes
  >     that are defined within it.
  > 2.  **Import Analysis:** It also uses the AST to find all modules and symbols
  >     that the target file imports, revealing its dependencies on other parts of
  >     the codebase or external libraries.
  > 3.  **Reference Finding:** It performs a repository-wide search to find all other
  >     files that reference the symbols defined in the target file. This helps to
  >     understand how the file is used by the rest of the system.
  >
  > The final output is a detailed JSON report containing all of this information,
  > which can be used as a foundational artifact for automated planning or human review.

- **`tooling/csdc_cli.py`**:

  > A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).
  >
  > This script provides an interface to validate a development plan against a specific
  > CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
  > plan adheres to the strict logical and computational constraints defined by the
  > CSDC protocol before it is executed.
  >
  > The tool performs two main checks:
  > 1.  **Complexity Analysis:** It analyzes the plan to determine its computational
  >     complexity and verifies that it matches the expected complexity class.
  > 2.  **Model Validation:** It validates the plan's commands against the rules of
  >     the specified CSDC model, ensuring that it does not violate any of the
  >     model's constraints (e.g., forbidding certain functions).
  >
  > This serves as a critical gateway for ensuring that all development work within
  > the CSDC framework is sound, predictable, and compliant with the governing
  > meta-mathematical principles.

- **`tooling/dependency_graph_generator.py`**:

  > Scans the repository for dependency files and generates a unified dependency graph.
  >
  > This script is a crucial component of the agent's environmental awareness,
  > providing a clear map of the software supply chain. It recursively searches the
  > entire repository for common dependency management files, specifically:
  > - `package.json` (for JavaScript/Node.js projects)
  > - `requirements.txt` (for Python projects)
  >
  > It parses these files to identify two key types of relationships:
  > 1.  **Internal Dependencies:** Links between different projects within this repository.
  > 2.  **External Dependencies:** Links to third-party libraries and packages.
  >
  > The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
  > represents these relationships as a graph structure with nodes (projects and
  > dependencies) and edges (the dependency links). This artifact is a primary
  > input for the agent's orientation and planning phases, allowing it to reason
  > about the potential impact of its changes.

- **`tooling/doc_builder.py`**:

  > A unified documentation builder for the project.
  > ...

- **`tooling/document_scanner.py`**:

  > A tool for scanning the repository for human-readable documents and extracting their text content.
  >
  > This script is a crucial component of the agent's initial information-gathering
  > and orientation phase. It allows the agent to ingest knowledge from unstructured
  > or semi-structured documents that are not part of the formal codebase, but which
  > may contain critical context, requirements, or specifications.
  >
  > The scanner searches a given directory for files with common document extensions:
  > - `.pdf`: Uses the `pypdf` library to extract text from PDF files.
  > - `.md`: Reads Markdown files.
  > - `.txt`: Reads plain text files.
  >
  > The output is a dictionary where the keys are the file paths of the discovered
  > documents and the values are their extracted text content. This data can then
  > be used by the agent to inform its planning and execution process. This tool
  > is essential for bridging the gap between human-written documentation and the
  > agent's operational awareness.

- **`tooling/environmental_probe.py`**:

  > Performs a series of checks to assess the capabilities of the execution environment.
  >
  > This script is a critical diagnostic tool run at the beginning of a task to
  > ensure the agent understands its operational sandbox. It verifies fundamental
  > capabilities required for most software development tasks:
  >
  > 1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
  >     and delete files. It also provides a basic latency measurement for these
  >     operations.
  > 2.  **Network Connectivity:** Checks for external network access by attempting to
  >     connect to a highly-available public endpoint (google.com). This is crucial
  >     for tasks requiring `git` operations, package downloads, or API calls.
  > 3.  **Environment Variables:** Verifies that standard environment variables are
  >     accessible, which is a prerequisite for many command-line tools.
  >
  > The script generates a human-readable report summarizing the results of these
  > probes, allowing the agent to quickly identify any environmental constraints
  > that might impact its ability to complete a task.

- **`tooling/fdc_cli.py`**:

  > This script provides a command-line interface (CLI) for managing the Finite
  > Development Cycle (FDC).
  >
  > The FDC is a structured workflow for agent-driven software development. This CLI
  > is the primary human interface for interacting with that cycle, providing
  > commands to:
  > - **start:** Initiates a new development task, triggering the "Advanced
  >   Orientation and Research Protocol" (AORP) to ensure the agent is fully
  >   contextualized.
  > - **close:** Formally concludes a task, creating a post-mortem template for
  >   analysis and lesson-learning.
  > - **validate:** Checks a given plan file for both syntactic and semantic
  >   correctness against the FDC's governing Finite State Machine (FSM). This
  >   ensures that a plan is executable and will not violate protocol.
  > - **analyze:** Examines a plan to determine its computational complexity (e.g.,
  >   Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  >   Read-Write), providing insight into the plan's potential impact.

- **`tooling/filesystem_lister.py`**:

  > A tool for listing files and directories in a repository, with an option to respect .gitignore.

- **`tooling/halting_heuristic_analyzer.py`**:

  > A static analysis tool to estimate the termination risk of a UDC plan.
  >
  > This script reads a `.udc` plan file, parses its instructions, and uses a
  > series of heuristics to identify potential infinite loops. It is not a
  > formal decider (as the halting problem is undecidable), but rather a
  > practical tool to flag common patterns that lead to non-termination.
  >
  > The analysis focuses on:
  > 1.  Detecting backward jumps, which are the primary indicator of loops.
  > 2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
  > 3.  Checking if the registers involved in the exit conditions are modified
  >     within the loop body in a way that is likely to lead to termination.
  >
  > The tool outputs a JSON report detailing the estimated risk level (LOW,
  > MEDIUM, HIGH) and the specific loops that were identified.

- **`tooling/hdl_prover.py`**:

  > A command-line tool for proving sequents in Intuitionistic Linear Logic.
  >
  > This script provides a basic interface to a simple logic prover. It takes a
  > sequent as a command-line argument, parses it into a logical structure, and
  > then attempts to prove it using a rudimentary proof search algorithm.
  >
  > The primary purpose of this tool is to allow the agent to perform formal
  > reasoning and verification tasks by checking the validity of logical entailments.
  > For example, it can be used to verify that a certain conclusion follows from a
  > set of premises according to the rules of linear logic.
  >
  > The current implementation uses a very basic parser and proof algorithm,
  > serving as a placeholder and demonstration for a more sophisticated, underlying
  > logic engine.

- **`tooling/hierarchical_compiler.py`**:

  > A hierarchical build system for compiling nested protocol modules.
  >
  > This script orchestrates the compilation of `AGENTS.md` and `README.md` files
  > across a repository with a nested or hierarchical module structure. It is a key
  > component of the system's ability to manage complexity by allowing protocols to
  > be defined in a modular, distributed way while still being presented as a unified,
  > coherent whole at each level of the hierarchy.
  >
  > The compiler operates in two main passes:
  >
  > **Pass 1: Documentation Compilation (Bottom-Up)**
  > 1.  **Discovery:** It finds all `protocols` directories in the repository, which
  >     signify the root of a documentation module.
  > 2.  **Bottom-Up Traversal:** It processes these directories from the most deeply
  >     nested ones upwards. This ensures that child modules are always built before
  >     their parents.
  > 3.  **Child Summary Injection:** For each compiled child module, it generates a
  >     summary of its protocols and injects this summary into the parent's
  >     `protocols` directory as a temporary file.
  > 4.  **Parent Compilation:** When the parent module is compiled, the standard
  >     `protocol_compiler.py` automatically includes the injected child summaries,
  >     creating a single `AGENTS.md` file that contains both the parent's native
  >     protocols and the full protocols of all its direct children.
  > 5.  **README Generation:** After each `AGENTS.md` is compiled, the corresponding
  >     `README.md` is generated.
  >
  > **Pass 2: Centralized Knowledge Graph Compilation**
  > 1.  After all documentation is built, it performs a full repository scan to find
  >     every `*.protocol.json` file.
  > 2.  It parses all of these files and compiles them into a single, centralized
  >     RDF knowledge graph (`protocols.ttl`). This provides a unified,
  >     machine-readable view of every protocol defined anywhere in the system.
  >
  > This hierarchical approach allows for both localized, context-specific protocol
  > definitions and a holistic, system-wide understanding of the agent's governing rules.

- **`tooling/knowledge_compiler.py`**:

  > Extracts structured lessons from post-mortem reports and compiles them into a
  > centralized, long-term knowledge base.
  >
  > This script is a core component of the agent's self-improvement feedback loop.
  > After a task is completed, a post-mortem report is generated that includes a
  > section for "Corrective Actions & Lessons Learned." This script automates the
  > process of parsing that section to extract key insights.
  >
  > It identifies pairs of "Lesson" and "Action" statements and transforms them
  > into a standardized, machine-readable format. These formatted entries are then
  > appended to the `knowledge_core/lessons.jsonl` file, which serves as the
  > agent's persistent memory of what has worked, what has failed, and what can be
  > improved in future tasks.
  >
  > The script is executed via the command line, taking the path to a completed
  > post-mortem file as its primary argument.

- **`tooling/knowledge_integrator.py`**:

  > Enriches the local knowledge graph with data from external sources like DBPedia.
  >
  > This script loads the RDF graph generated from the project's protocols,
  > identifies key concepts (like tools and rules), queries the DBPedia SPARQL
  > endpoint to find related information, and merges the external data into a new,
  > enriched knowledge graph.

- **`tooling/lba_validator.py`**:

  > A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.
  >
  > This module implements a validator that enforces the context-sensitive rules of the CSDC.
  > Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
  > validation decisions. This is necessary to enforce rules where the validity of one
  > command depends on the presence or absence of another command elsewhere in the plan.
  >
  > The CSDC defines two mutually exclusive models:
  > - Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
  > - Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.
  >
  > This validator checks for these co-occurrence constraints.

- **`tooling/lfi_ill_halting_decider.py`**:

  > A tool for analyzing the termination of LFI-ILL programs.
  >
  > This script takes an LFI-ILL file, interprets it in a paraconsistent logic
  > environment, and reports on its halting status. It does this by setting up
  > a paradoxical initial state and observing how the program resolves it.

- **`tooling/lfi_udc_model.py`**:

  > A paraconsistent execution model for UDC plans.
  >
  > This module provides the classes necessary to interpret a UDC (Un-decidable
  > Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
  > concrete values, the state of the machine (registers, tape, etc.) is modeled
  > using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).
  >
  > This allows the system to reason about paradoxical programs, such as a program
  > that halts if and only if it does not halt. By executing the program under
  > paraconsistent semantics, the model can arrive at a final state of `BOTH`,
  > effectively demonstrating the paradoxical nature of the input without crashing.
  >
  > Key classes:
  > - `ParaconsistentTruth`: An enum for the four truth values.
  > - `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
  > - `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
  > - `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
  > - `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  >   analysis of a UDC plan.

- **`tooling/log_failure.py`**:

  > A dedicated script to log a catastrophic failure event to the main activity log.
  >
  > This tool is designed to be invoked in the rare case of a severe, unrecoverable
  > error that violates a core protocol. Its primary purpose is to ensure that such
  > a critical event is formally and structurally documented in the standard agent
  > activity log (`logs/activity.log.jsonl`), even if the main agent loop has
  > crashed or been terminated.
  >
  > The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
  > attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
  > permanent, machine-readable record of the failure, which is essential for
  > post-mortem analysis, debugging, and the development of future safeguards.
  >
  > By using the standard `Logger` class, it ensures that the failure log entry
  > conforms to the established `LOGGING_SCHEMA.md`, making it processable by
  > auditing and analysis tools.

- **`tooling/master_control.py`**:

  > The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).
  >
  > This script, master_control.py, is the heart of the agent's operational loop.
  > It implements the CFDC, a hierarchical planning and execution model based on a
  > Pushdown Automaton. This allows the agent to execute complex tasks by calling
  > plans as sub-routines.
  >
  > Core Responsibilities:
  > - **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  >   plans to call other plans via the `call_plan` directive. This allows for
  >   modular, reusable, and complex task decomposition. A maximum recursion depth
  >   is enforced to guarantee decidability.
  > - **Plan Validation:** Contains the in-memory plan validator. Before execution,
  >   it parses a plan and simulates its execution against a Finite State Machine
  >   (FSM) to ensure it complies with the agent's operational protocols.
  > - **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  >   it first attempts to look up the plan by its logical name in the
  >   `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  >   the argument as a direct file path.
  > - **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  >   finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  >   to ensure predictable and auditable behavior.
  >
  > This module is designed as a library to be controlled by an external shell
  > (e.g., `agent_shell.py`), making its interaction purely programmatic.

- **`tooling/master_control_cli.py`**:

  > The official command-line interface for the agent's master control loop.
  >
  > This script is now a lightweight wrapper that passes control to the new,
  > API-driven `agent_shell.py`. It preserves the command-line interface while
  > decoupling the entry point from the FSM implementation.

- **`tooling/message_user.py`**:

  > A dummy tool that prints its arguments to simulate the message_user tool.
  >
  > This script is a simple command-line utility that takes a string as an
  > argument and prints it to standard output, prefixed with "[Message User]:".
  > Its purpose is to serve as a stand-in or mock for the actual `message_user`
  > tool in testing environments where the full agent framework is not required.
  >
  > This allows for the testing of scripts or workflows that call the
  > `message_user` tool without needing to invoke the entire agent messaging
  > subsystem.

- **`tooling/pda_parser.py`**:

  > A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.
  >
  > This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
  > parser for a simple, string-based representation of pLLLU formulas. It can
  > handle basic atomic formulas, unary operators (like negation and consistency),
  > and binary operators (like implication and conjunction).
  >
  > The main function `parse_formula` takes a string and returns a simple AST
  > (Abstract Syntax Tree) represented as nested tuples.

- **`tooling/plan_executor.py`**:

  > A simple plan executor for simulating agent behavior.
  >
  > This script reads a plan file, parses it, and executes the commands in a
  > simplified, simulated environment. It supports a limited set of tools
  > (`message_user` and `run_in_bash_session`) to provide a basic demonstration
  > of how an agent would execute a plan.

- **`tooling/plan_manager.py`**:

  > Provides a command-line interface for managing the agent's Plan Registry.
  >
  > This script is the administrative tool for the Plan Registry, a key component
  > of the Context-Free Development Cycle (CFDC) that enables hierarchical and
  > modular planning. The registry, located at `knowledge_core/plan_registry.json`,
  > maps human-readable, logical names to the file paths of specific plans. This
  > decouples the `call_plan` directive from hardcoded file paths, making plans
  > more reusable and the system more robust.
  >
  > This CLI provides three essential functions:
  > - **register**: Associates a new logical name with a plan file path, adding it
  >   to the central registry.
  > - **deregister**: Removes an existing logical name and its associated path from
  >   the registry.
  > - **list**: Displays all current name-to-path mappings in the registry.
  >
  > By providing a simple, standardized interface for managing this library of
  > reusable plans, this tool improves the agent's ability to compose complex
  > workflows from smaller, validated sub-plans.

- **`tooling/plan_parser.py`**:

  > Parses a plan file into a structured list of commands.
  >
  > This module provides the `parse_plan` function and the `Command` dataclass,
  > which are central to the agent's ability to understand and execute plans.
  > The parser correctly handles multi-line arguments and ignores comments,
  > allowing for robust and readable plan files.

- **`tooling/plllu_interpreter.py`**:

  > A resource-sensitive, four-valued interpreter for pLLLU formulas.
  >
  > This script implements an interpreter for the pLLLU language. It operates on
  > an AST generated by the `pda_parser.py` script. The interpreter is designed
  > to be resource-sensitive, meaning that each atomic formula in the initial
  > context must be consumed exactly once during the evaluation of the proof.
  >
  > The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
  > it to reason about paraconsistent and paracomplete states.
  >
  > The core of the interpreter is the `FourValuedInterpreter` class, which
  > recursively walks the AST, consuming resources from a context (a Counter of
  > available atoms) and returning the resulting logical value.

- **`tooling/plllu_runner.py`**:

  > A command-line runner for pLLLU files.
  >
  > This script provides an entry point for executing `.plllu` files. It
  > integrates the pLLLU lexer, parser, and interpreter to execute the logic
  > defined in a given pLLLU source file and print the result.

- **`tooling/pre_submit_check.py`**:

  > _No module-level docstring found._

- **`tooling/protocol_compiler.py`**:

  > Compiles source protocol files into unified, human-readable and machine-readable artifacts.
  >
  > This script is the engine behind the "protocol as code" principle. It discovers,
  > validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
  > into high-level documents like `AGENTS.md`.
  >
  > Key Functions:
  > - **Discovery:** Scans a directory for source files, including `.protocol.json`
  >   (machine-readable rules) and `.protocol.md` (human-readable context).
  > - **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  >   `.protocol.json` file, ensuring all protocol definitions are syntactically
  >   correct and adhere to the established structure.
  > - **Compilation:** Combines the human-readable markdown and the machine-readable
  >   JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
  > - **Documentation Injection:** Can inject other generated documents, like the
  >   `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
  > - **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  >   protocols and serialize them into an RDF knowledge graph (in Turtle format),
  >   creating a machine-queryable version of the agent's governing rules.
  >
  > This process ensures that `AGENTS.md` and other protocol documents are not edited
  > manually but are instead generated from a validated, single source of truth,
  > making the agent's protocols robust, verifiable, and maintainable.

- **`tooling/protocol_updater.py`**:

  > A command-line tool for programmatically updating protocol source files.
  >
  > This script provides the mechanism for the agent to perform self-correction
  > by modifying its own governing protocols based on structured, actionable
  > lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
  > workflow.
  >
  > The tool operates on the .protocol.json files located in the `protocols/`
  > directory, performing targeted updates based on command-line arguments.

- **`tooling/refactor.py`**:

  > A tool for performing automated symbol renaming in Python code.
  >
  > This script provides a command-line interface to find a specific symbol
  > (a function or a class) in a given Python file and rename it, along with all of
  > its textual references throughout the entire repository. This provides a safe
  > and automated way to perform a common refactoring task, reducing the risk of
  > manual errors.
  >
  > The tool operates in three main stages:
  > 1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
  >     to parse the source file and precisely locate the definition of the target
  >     symbol. This ensures that the tool is targeting the correct code construct.
  > 2.  **Reference Finding:** It performs a text-based search across the specified
  >     search path (defaulting to the entire repository) to find all files that
  >     mention the symbol's old name.
  > 3.  **Plan Generation:** Instead of modifying files directly, it generates a
  >     refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
  >     commands, one for each file that needs to be changed. The path to this
  >     generated plan file is printed to standard output.
  >
  > This plan-based approach allows the agent's master controller to execute the
  > refactoring in a controlled, verifiable, and atomic way, consistent with its
  > standard operational procedures.

- **`tooling/reliable_ls.py`**:

  > A tool for reliably listing files and directories.
  >
  > This script provides a consistent, sorted, and recursive listing of files and
  > directories, excluding the `.git` directory. It is intended to be a more
  > reliable alternative to the standard `ls` command for agent use cases.

- **`tooling/reorientation_manager.py`**:

  > Re-orientation Manager
  >
  > This script is the core of the automated re-orientation process. It is
  > designed to be triggered by the build system whenever the agent's core
  > protocols (`AGENTS.md`) are re-compiled.
  >
  > The manager performs the following key functions:
  > 1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
  >     version to identify new protocols, tools, or other key concepts that have
  >     been introduced.
  > 2.  **Temporal Orientation (Shallow Research):** For each new concept, it
  >     invokes the `temporal_orienter.py` tool to fetch a high-level summary from
  >     an external knowledge base like DBpedia. This ensures the agent has a
  >     baseline understanding of new terms.
  > 3.  **Knowledge Storage:** The summaries from the temporal orientation are
  >     stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
  >     creating a persistent, queryable knowledge artifact.
  > 4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
  >     change is deemed significant (e.g., the addition of a new core
  >     architectural protocol), it programmatically triggers a formal L4 Deep
  >     Research Cycle by creating a `deep_research_required.json` file.
  >
  > This automated workflow ensures that the agent never operates with an outdated
  > understanding of its own protocols. It closes the loop between protocol
  > modification and the agent's self-awareness, making the system more robust,
  > adaptive, and reliable.

- **`tooling/research.py`**:

  > This module contains the logic for executing research tasks based on a set of
  > constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
  > read_file, google_search) based on the specified target and scope.

- **`tooling/research_planner.py`**:

  > This module is responsible for generating a formal, FSM-compliant research plan
  > for a given topic. The output is a string that can be executed by the agent's
  > master controller.

- **`tooling/self_correction_orchestrator.py`**:

  > Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.
  >
  > This script is the engine of the automated feedback loop. It reads structured,
  > actionable lessons from `knowledge_core/lessons.jsonl` and uses the
  > `protocol_updater.py` tool to apply them to the source protocol files.

- **`tooling/self_improvement_cli.py`**:

  > Analyzes agent activity logs to identify opportunities for self-improvement.
  >
  > This script is a command-line tool that serves as a key part of the agent's
  > meta-cognitive loop. It parses the structured activity log
  > (`logs/activity.log.jsonl`) to identify patterns that may indicate
  > inefficiencies or errors in the agent's workflow.
  >
  > The primary analysis currently implemented is:
  > - **Planning Efficiency Analysis:** It scans the logs for tasks that required
  >   multiple `set_plan` actions. A high number of plan revisions for a single
  >   task can suggest that the initial planning phase was insufficient, the task
  >   was poorly understood, or the agent struggled to adapt to unforeseen
  >   challenges.
  >
  > By flagging these tasks, the script provides a starting point for a deeper
  > post-mortem analysis, helping the agent (or its developers) to understand the
  > root causes of the planning churn and to develop strategies for more effective
  > upfront planning in the future.
  >
  > The tool is designed to be extensible, with future analyses (such as error
  > rate tracking or tool usage anti-patterns) to be added as the system evolves.

- **`tooling/standard_agents_compiler.py`**:

  > A compiler that generates a simplified, standard-compliant `AGENTS.md` file.
  >
  > This script acts as an "adapter" to make the repository more accessible to
  > third-party AI agents that expect a conventional set of instructions. While the
  > repository's primary `AGENTS.md` is a complex, hierarchical, and
  > machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
  > file produced by this script offers a simple, human-readable summary of the
  > most common development commands.
  >
  > The script works by:
  > 1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
  >     which is the single source of truth for high-level commands. It specifically
  >     extracts the exact commands for common targets like `install`, `test`,
  >     `lint`, and `format`. This ensures the generated instructions are never
  >     stale.
  > 2.  **Injecting into a Template:** It injects these extracted commands into a
  >     pre-defined, user-friendly Markdown template.
  > 3.  **Generating the Artifact:** The final output is written to
  >     `AGENTS.standard.md`, providing a simple, stable, and conventional entry
  >     point for external tools, effectively bridging the gap between the complex
  >     internal protocol system and the broader agent ecosystem.

- **`tooling/state.py`**:

  > Defines the core data structures for managing the agent's state.
  >
  > This module provides the `AgentState` and `PlanContext` dataclasses, which are
  > fundamental to the operation of the Context-Free Development Cycle (CFDC). These
  > structures allow the `master_control.py` orchestrator to maintain a complete,
  > snapshot-able representation of the agent's progress through a task.
  >
  > - `AgentState`: The primary container for all information related to the current
  >   task, including the plan execution stack, message history, and error states.
  > - `PlanContext`: A specific structure that holds the state of a single plan
  >   file, including its content and the current execution step. This is the
  >   element that gets pushed onto the `plan_stack` in `AgentState`.
  >
  > Together, these classes enable the hierarchical, stack-based planning and
  > execution that is the hallmark of the CFDC.

- **`tooling/symbol_map_generator.py`**:

  > Generates a code symbol map for the repository to aid in contextual understanding.
  >
  > This script creates a `symbols.json` file in the `knowledge_core` directory,
  > which acts as a high-level index of the codebase. This map contains information
  > about key programming constructs like classes and functions, including their
  > name, location (file path and line number), and language.
  >
  > The script employs a two-tiered approach for symbol generation:
  > 1.  **Universal Ctags (Preferred):** It first checks for the presence of the
  >     `ctags` command-line tool. If available, it uses `ctags` to perform a
  >     comprehensive, multi-language scan of the repository. This is the most
  >     robust and accurate method.
  > 2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
  >     back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
  >     method parses all `.py` files and extracts symbol information for Python
  >     code. While less comprehensive than `ctags`, it ensures that a baseline
  >     symbol map is always available.
  >
  > The resulting `symbols.json` artifact is a critical input for the agent's
  > orientation and planning phases, allowing it to quickly locate relevant code
  > and understand the structure of the repository without having to read every file.

- **`tooling/udc_orchestrator.py`**:

  > An orchestrator for executing Unrestricted Development Cycle (UDC) plans.
  >
  > This script provides a sandboxed environment for running UDC plans, which are
  > low-level assembly-like programs that can perform Turing-complete computations.
  > The orchestrator acts as a virtual machine with a tape-based memory model,
  > registers, and a set of simple instructions.
  >
  > To prevent non-termination and other resource-exhaustion issues, the
  > orchestrator imposes strict limits on the number of instructions executed,
  > the amount of memory used, and the total wall-clock time.

## Experimental Framework

The `experiments/` directory contains a framework for testing the agent's behavior in response to changes in its governing protocols (`AGENTS.md`). Each subdirectory within `experiments/` represents a self-contained experiment.

### Running an Experiment

To run an existing experiment (e.g., `scoped_protocol_override`):

1.  **Review the Experiment:** Read the `README.md` inside the experiment's directory (e.g., `experiments/scoped_protocol_override/README.md`) to understand its hypothesis, procedure, and expected outcome.
2.  **Perform the Baseline Run:** Follow the instructions in the experiment's `README.md` to establish the agent's baseline behavior. This usually involves performing a task in the root directory.
3.  **Perform the Experimental Run:** Follow the instructions to run the agent against the mutated protocol. This typically involves:
    a. Copying the `mutation.md` file to a new `AGENTS.md` file within the experiment's directory.
    b. Instructing the agent to perform the task specified in `task.md`, targeting the experiment's directory.
4.  **Compare the Results:** Observe the difference in the agent's behavior between the baseline and experimental runs to verify the hypothesis.

### Creating a New Experiment

1.  Create a new subdirectory in `experiments/`.
2.  Add a `README.md` file explaining the new experiment's hypothesis and procedure.
3.  Add a `mutation.md` file containing the altered `AGENTS.md` content.
4.  Add a `task.md` file describing the task the agent should perform.

---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---

# Tooling Directory Documentation

This document provides an overview of the tools available in the `tooling/` directory. It is automatically generated from the docstrings of the tools.

---

## `__init__.py`

_No module-level docstring found._

---

## `agent_shell.py`

The new, interactive, API-driven entry point for the agent.

This script replaces the old file-based signaling system with a direct,
programmatic interface to the MasterControlGraph FSM. It is responsible for:
1.  Initializing the agent's state and a centralized logger.
2.  Instantiating and running the MasterControlGraph.
3.  Driving the FSM by calling its methods and passing data and the logger.
4.  Containing the core "agent logic" (e.g., an LLM call) to generate plans
    and respond to requests for action.

---

## `__init__.py`

_No module-level docstring found._

---

## `generate_and_test.py`

_No module-level docstring found._

---

## `appl_runner.py`

A command-line tool for executing APPL files.

This script provides a simple interface to run APPL files using the main
`run.py` interpreter. It captures and prints the output of the execution,
and provides detailed error reporting if the execution fails.

---

## `appl_to_lfi_ill.py`

A compiler that translates APPL (a simple functional language) to LFI-ILL.

This script takes a Python file containing an APPL AST, and compiles it into
an LFI-ILL AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `auditor.py`

A unified auditing tool for maintaining repository health and compliance.

This script combines the functionality of several disparate auditing tools into a
single, comprehensive command-line interface. It serves as the central tool for
validating the key components of the agent's architecture, including protocols,
plans, and documentation.

The auditor can perform the following checks:
1.  **Protocol Audit (`protocol`):**
    - Checks if `AGENTS.md` artifacts are stale compared to their source files.
    - Verifies protocol completeness by comparing tools used in logs against
      tools defined in protocols.
    - Analyzes tool usage frequency (centrality).
2.  **Plan Registry Audit (`plans`):**
    - Scans `knowledge_core/plan_registry.json` for "dead links" where the
      target plan file does not exist.
3.  **Documentation Audit (`docs`):**
    - Scans the generated `SYSTEM_DOCUMENTATION.md` to find Python modules
      that are missing module-level docstrings.

The tool is designed to be run from the command line and can execute specific
audits or all of them, generating a consolidated `audit_report.md` file.

---

## `aura_executor.py`

This script serves as the command-line executor for `.aura` files.

It bridges the gap between the high-level Aura scripting language and the
agent's underlying Python-based toolset. The executor is responsible for:
1.  Parsing the `.aura` source code using the lexer and parser from the
    `aura_lang` package.
2.  Setting up an execution environment for the interpreter.
3.  Injecting a "tool-calling" capability into the Aura environment, which
    allows Aura scripts to dynamically invoke registered Python tools
    (e.g., `hdl_prover`, `environmental_probe`).
4.  Executing the parsed program and printing the final result.

This makes it a key component for enabling more expressive and complex
automation scripts for the agent.

---

## `aura_to_lfi_ill.py`

A compiler that translates AURA code to LFI-ILL.

This script takes an AURA file, parses it, and compiles it into an LFI-ILL
AST. The resulting AST is then written to a `.lfi_ill` file.

---

## `background_researcher.py`

This script performs a simulated research task in the background.
It takes a task ID as a command-line argument and writes its findings
to a temporary file that the main agent can poll.

---

## `builder.py`

A unified, configuration-driven build script for the project.

This script serves as the central entry point for all build-related tasks, such
as generating documentation, compiling protocols, and running code quality checks.
It replaces a traditional Makefile's direct command execution with a more
structured, maintainable, and introspectable approach.

The core logic is driven by a `build_config.json` file, which defines a series
of "targets." Each target specifies:
- The `type` of target: "compiler" or "command".
- For "compiler" types: `compiler` script, `output`, `sources`, and `options`.
- For "command" types: the `command` to execute.

The configuration also defines "build_groups", which are ordered collections of
targets (e.g., "all", "quality").

This centralized builder provides several advantages:
- **Single Source of Truth:** The `build_config.json` file is the definitive
  source for all build logic.
- **Consistency:** Ensures all build tasks are executed in a uniform way.
- **Extensibility:** New build targets can be added by simply updating the
  configuration file.
- **Discoverability:** The script can list all available targets and groups.

---

## `capability_verifier.py`

A tool to verify that the agent can monotonically improve its capabilities.

This script is designed to provide a formal, automated test for the agent's
self-correction and learning mechanisms. It ensures that when the agent learns
a new capability, it does so without losing (regressing) any of its existing
capabilities. This is a critical safeguard for ensuring robust and reliable
agent evolution.

The tool works by orchestrating a four-step process:
1.  **Confirm Initial Failure:** It runs a specific test file that is known to
    fail, verifying that the agent currently lacks the target capability.
2.  **Invoke Self-Correction:** It simulates the discovery of a new "lesson" and
    triggers the `self_correction_orchestrator.py` script, which is responsible
    for integrating new knowledge and skills.
3.  **Confirm Final Success:** It runs the same test file again, confirming that
    the agent has successfully learned the new capability and the test now passes.
4.  **Check for Regressions:** It runs the full, existing test suite to ensure
    that the process of learning the new skill has not inadvertently broken any
    previously functional capabilities.

This provides a closed-loop verification of monotonic improvement, which is a
cornerstone of the agent's design philosophy.

---

## `code_suggester.py`

Handles the generation and application of autonomous code change suggestions.

This tool is a key component of the advanced self-correction loop. It is
designed to be invoked by the self-correction orchestrator when a lesson
contains a 'propose-code-change' action.

For its initial implementation, this tool acts as a structured executor. It
takes a lesson where the 'details' field contains a fully-formed git-style
merge diff and applies it to the target file. It does this by generating a
temporary, single-step plan file and signaling its location for the master
controller to execute.

This establishes the fundamental workflow for autonomous code modification,
decoupling the suggestion logic from the execution logic. Future iterations
can enhance this tool with more sophisticated code generation capabilities
(e.g., using an LLM to generate the diff from a natural language description)
without altering the core orchestration process.

---

## `context_awareness_scanner.py`

A tool for performing static analysis on a Python file to understand its context.

This script provides a "contextual awareness" scan of a specified Python file
to help an agent (or a human) understand its role, dependencies, and connections
within a larger codebase. This is crucial for planning complex changes or
refactoring efforts, as it provides a snapshot of the potential impact of
modifying a file.

The scanner performs three main functions:
1.  **Symbol Definition Analysis:** It uses Python's Abstract Syntax Tree (AST)
    module to parse the target file and identify all the functions and classes
    that are defined within it.
2.  **Import Analysis:** It also uses the AST to find all modules and symbols
    that the target file imports, revealing its dependencies on other parts of
    the codebase or external libraries.
3.  **Reference Finding:** It performs a repository-wide search to find all other
    files that reference the symbols defined in the target file. This helps to
    understand how the file is used by the rest of the system.

The final output is a detailed JSON report containing all of this information,
which can be used as a foundational artifact for automated planning or human review.

---

## `csdc_cli.py`

A command-line tool for managing the Context-Sensitive Development Cycle (CSDC).

This script provides an interface to validate a development plan against a specific
CSDC model (A or B) and a given complexity class (P or EXP). It ensures that a
plan adheres to the strict logical and computational constraints defined by the
CSDC protocol before it is executed.

The tool performs two main checks:
1.  **Complexity Analysis:** It analyzes the plan to determine its computational
    complexity and verifies that it matches the expected complexity class.
2.  **Model Validation:** It validates the plan's commands against the rules of
    the specified CSDC model, ensuring that it does not violate any of the
    model's constraints (e.g., forbidding certain functions).

This serves as a critical gateway for ensuring that all development work within
the CSDC framework is sound, predictable, and compliant with the governing
meta-mathematical principles.

---

## `dependency_graph_generator.py`

Scans the repository for dependency files and generates a unified dependency graph.

This script is a crucial component of the agent's environmental awareness,
providing a clear map of the software supply chain. It recursively searches the
entire repository for common dependency management files, specifically:
- `package.json` (for JavaScript/Node.js projects)
- `requirements.txt` (for Python projects)

It parses these files to identify two key types of relationships:
1.  **Internal Dependencies:** Links between different projects within this repository.
2.  **External Dependencies:** Links to third-party libraries and packages.

The final output is a JSON file, `knowledge_core/dependency_graph.json`, which
represents these relationships as a graph structure with nodes (projects and
dependencies) and edges (the dependency links). This artifact is a primary
input for the agent's orientation and planning phases, allowing it to reason
about the potential impact of its changes.

---

## `doc_builder.py`

A unified documentation builder for the project.
...

---

## `document_scanner.py`

A tool for scanning the repository for human-readable documents and extracting their text content.

This script is a crucial component of the agent's initial information-gathering
and orientation phase. It allows the agent to ingest knowledge from unstructured
or semi-structured documents that are not part of the formal codebase, but which
may contain critical context, requirements, or specifications.

The scanner searches a given directory for files with common document extensions:
- `.pdf`: Uses the `pypdf` library to extract text from PDF files.
- `.md`: Reads Markdown files.
- `.txt`: Reads plain text files.

The output is a dictionary where the keys are the file paths of the discovered
documents and the values are their extracted text content. This data can then
be used by the agent to inform its planning and execution process. This tool
is essential for bridging the gap between human-written documentation and the
agent's operational awareness.

---

## `environmental_probe.py`

Performs a series of checks to assess the capabilities of the execution environment.

This script is a critical diagnostic tool run at the beginning of a task to
ensure the agent understands its operational sandbox. It verifies fundamental
capabilities required for most software development tasks:

1.  **Filesystem I/O:** Confirms that the agent can create, write to, read from,
    and delete files. It also provides a basic latency measurement for these
    operations.
2.  **Network Connectivity:** Checks for external network access by attempting to
    connect to a highly-available public endpoint (google.com). This is crucial
    for tasks requiring `git` operations, package downloads, or API calls.
3.  **Environment Variables:** Verifies that standard environment variables are
    accessible, which is a prerequisite for many command-line tools.

The script generates a human-readable report summarizing the results of these
probes, allowing the agent to quickly identify any environmental constraints
that might impact its ability to complete a task.

---

## `fdc_cli.py`

This script provides a command-line interface (CLI) for managing the Finite
Development Cycle (FDC).

The FDC is a structured workflow for agent-driven software development. This CLI
is the primary human interface for interacting with that cycle, providing
commands to:
- **start:** Initiates a new development task, triggering the "Advanced
  Orientation and Research Protocol" (AORP) to ensure the agent is fully
  contextualized.
- **close:** Formally concludes a task, creating a post-mortem template for
  analysis and lesson-learning.
- **validate:** Checks a given plan file for both syntactic and semantic
  correctness against the FDC's governing Finite State Machine (FSM). This
  ensures that a plan is executable and will not violate protocol.
- **analyze:** Examines a plan to determine its computational complexity (e.g.,
  Constant, Polynomial, Exponential) and its modality (Read-Only vs.
  Read-Write), providing insight into the plan's potential impact.

---

## `filesystem_lister.py`

A tool for listing files and directories in a repository, with an option to respect .gitignore.

---

## `halting_heuristic_analyzer.py`

A static analysis tool to estimate the termination risk of a UDC plan.

This script reads a `.udc` plan file, parses its instructions, and uses a
series of heuristics to identify potential infinite loops. It is not a
formal decider (as the halting problem is undecidable), but rather a
practical tool to flag common patterns that lead to non-termination.

The analysis focuses on:
1.  Detecting backward jumps, which are the primary indicator of loops.
2.  Analyzing the exit conditions of these loops (e.g., `JE`, `JNE`).
3.  Checking if the registers involved in the exit conditions are modified
    within the loop body in a way that is likely to lead to termination.

The tool outputs a JSON report detailing the estimated risk level (LOW,
MEDIUM, HIGH) and the specific loops that were identified.

---

## `hdl_prover.py`

A command-line tool for proving sequents in Intuitionistic Linear Logic.

This script provides a basic interface to a simple logic prover. It takes a
sequent as a command-line argument, parses it into a logical structure, and
then attempts to prove it using a rudimentary proof search algorithm.

The primary purpose of this tool is to allow the agent to perform formal
reasoning and verification tasks by checking the validity of logical entailments.
For example, it can be used to verify that a certain conclusion follows from a
set of premises according to the rules of linear logic.

The current implementation uses a very basic parser and proof algorithm,
serving as a placeholder and demonstration for a more sophisticated, underlying
logic engine.

---

## `hierarchical_compiler.py`

_No module-level docstring found._

---

## `knowledge_compiler.py`

Extracts structured lessons from post-mortem reports and compiles them into a
centralized, long-term knowledge base.

This script is a core component of the agent's self-improvement feedback loop.
After a task is completed, a post-mortem report is generated that includes a
section for "Corrective Actions & Lessons Learned." This script automates the
process of parsing that section to extract key insights.

It identifies pairs of "Lesson" and "Action" statements and transforms them
into a standardized, machine-readable format. These formatted entries are then
appended to the `knowledge_core/lessons.jsonl` file, which serves as the
agent's persistent memory of what has worked, what has failed, and what can be
improved in future tasks.

The script is executed via the command line, taking the path to a completed
post-mortem file as its primary argument.

---

## `knowledge_integrator.py`

Enriches the local knowledge graph with data from external sources like DBPedia.

This script loads the RDF graph generated from the project's protocols,
identifies key concepts (like tools and rules), queries the DBPedia SPARQL
endpoint to find related information, and merges the external data into a new,
enriched knowledge graph.

---

## `lba_validator.py`

A Linear Bounded Automaton (LBA) for validating Context-Sensitive Development Cycle (CSDC) plans.

This module implements a validator that enforces the context-sensitive rules of the CSDC.
Unlike a simple FSM, an LBA can inspect the entire input "tape" (the plan) to make
validation decisions. This is necessary to enforce rules where the validity of one
command depends on the presence or absence of another command elsewhere in the plan.

The CSDC defines two mutually exclusive models:
- Model A: Permits `define_set_of_names`, but forbids `define_diagonalization_function`.
- Model B: Permits `define_diagonalization_function`, but forbids `define_set_of_names`.

This validator checks for these co-occurrence constraints.

---

## `lfi_ill_halting_decider.py`

A tool for analyzing the termination of LFI-ILL programs.

This script takes an LFI-ILL file, interprets it in a paraconsistent logic
environment, and reports on its halting status. It does this by setting up
a paradoxical initial state and observing how the program resolves it.

---

## `lfi_udc_model.py`

A paraconsistent execution model for UDC plans.

This module provides the classes necessary to interpret a UDC (Un-decidable
Computation) plan within a Logic of Formal Inconsistency (LFI). Instead of
concrete values, the state of the machine (registers, tape, etc.) is modeled
using paraconsistent truth values (TRUE, FALSE, BOTH, NEITHER).

This allows the system to reason about paradoxical programs, such as a program
that halts if and only if it does not halt. By executing the program under
paraconsistent semantics, the model can arrive at a final state of `BOTH`,
effectively demonstrating the paradoxical nature of the input without crashing.

Key classes:
- `ParaconsistentTruth`: An enum for the four truth values.
- `ParaconsistentState`: A wrapper for a value that holds a paraconsistent truth.
- `LFIInstruction`: A UDC instruction that operates on paraconsistent states.
- `LFIExecutor`: A virtual machine that executes a UDC plan using LFI semantics.
- `ParaconsistentHaltingDecider`: The main entry point that orchestrates the
  analysis of a UDC plan.

---

## `log_failure.py`

A dedicated script to log a catastrophic failure event to the main activity log.

This tool is designed to be invoked in the rare case of a severe, unrecoverable
error that violates a core protocol. Its primary purpose is to ensure that such
a critical event is formally and structurally documented in the standard agent
activity log (`logs/activity.log.jsonl`), even if the main agent loop has
crashed or been terminated.

The script is pre-configured to log a `SYSTEM_FAILURE` event, specifically
attributing it to the "Unauthorized use of the `reset_all` tool." This creates a
permanent, machine-readable record of the failure, which is essential for
post-mortem analysis, debugging, and the development of future safeguards.

By using the standard `Logger` class, it ensures that the failure log entry
conforms to the established `LOGGING_SCHEMA.md`, making it processable by
auditing and analysis tools.

---

## `master_control.py`

The master orchestrator for the agent's lifecycle, implementing the Context-Free Development Cycle (CFDC).

This script, master_control.py, is the heart of the agent's operational loop.
It implements the CFDC, a hierarchical planning and execution model based on a
Pushdown Automaton. This allows the agent to execute complex tasks by calling
plans as sub-routines.

Core Responsibilities:
- **Hierarchical Plan Execution:** Manages a plan execution stack to enable
  plans to call other plans via the `call_plan` directive. This allows for
  modular, reusable, and complex task decomposition. A maximum recursion depth
  is enforced to guarantee decidability.
- **Plan Validation:** Contains the in-memory plan validator. Before execution,
  it parses a plan and simulates its execution against a Finite State Machine
  (FSM) to ensure it complies with the agent's operational protocols.
- **"Registry-First" Plan Resolution:** When resolving a `call_plan` directive,
  it first attempts to look up the plan by its logical name in the
  `knowledge_core/plan_registry.json`. If not found, it falls back to treating
  the argument as a direct file path.
- **FSM-Governed Lifecycle:** The entire workflow, from orientation to
  finalization, is governed by a strict FSM definition (e.g., `tooling/fsm.json`)
  to ensure predictable and auditable behavior.

This module is designed as a library to be controlled by an external shell
(e.g., `agent_shell.py`), making its interaction purely programmatic.

---

## `master_control_cli.py`

The official command-line interface for the agent's master control loop.

This script is now a lightweight wrapper that passes control to the new,
API-driven `agent_shell.py`. It preserves the command-line interface while
decoupling the entry point from the FSM implementation.

---

## `message_user.py`

A dummy tool that prints its arguments to simulate the message_user tool.

This script is a simple command-line utility that takes a string as an
argument and prints it to standard output, prefixed with "[Message User]:".
Its purpose is to serve as a stand-in or mock for the actual `message_user`
tool in testing environments where the full agent framework is not required.

This allows for the testing of scripts or workflows that call the
`message_user` tool without needing to invoke the entire agent messaging
subsystem.

---

## `pda_parser.py`

A parser for pLLLU (paraconsistent Linear Logic with Undeterminedness) formulas.

This script uses the PLY (Python Lex-Yacc) library to define a lexer and a
parser for a simple, string-based representation of pLLLU formulas. It can
handle basic atomic formulas, unary operators (like negation and consistency),
and binary operators (like implication and conjunction).

The main function `parse_formula` takes a string and returns a simple AST
(Abstract Syntax Tree) represented as nested tuples.

---

## `plan_executor.py`

A simple plan executor for simulating agent behavior.

This script reads a plan file, parses it, and executes the commands in a
simplified, simulated environment. It supports a limited set of tools
(`message_user` and `run_in_bash_session`) to provide a basic demonstration
of how an agent would execute a plan.

---

## `plan_manager.py`

Provides a command-line interface for managing the agent's Plan Registry.

This script is the administrative tool for the Plan Registry, a key component
of the Context-Free Development Cycle (CFDC) that enables hierarchical and
modular planning. The registry, located at `knowledge_core/plan_registry.json`,
maps human-readable, logical names to the file paths of specific plans. This
decouples the `call_plan` directive from hardcoded file paths, making plans
more reusable and the system more robust.

This CLI provides three essential functions:
- **register**: Associates a new logical name with a plan file path, adding it
  to the central registry.
- **deregister**: Removes an existing logical name and its associated path from
  the registry.
- **list**: Displays all current name-to-path mappings in the registry.

By providing a simple, standardized interface for managing this library of
reusable plans, this tool improves the agent's ability to compose complex
workflows from smaller, validated sub-plans.

---

## `plan_parser.py`

Parses a plan file into a structured list of commands.

This module provides the `parse_plan` function and the `Command` dataclass,
which are central to the agent's ability to understand and execute plans.
The parser correctly handles multi-line arguments and ignores comments,
allowing for robust and readable plan files.

---

## `plllu_interpreter.py`

A resource-sensitive, four-valued interpreter for pLLLU formulas.

This script implements an interpreter for the pLLLU language. It operates on
an AST generated by the `pda_parser.py` script. The interpreter is designed
to be resource-sensitive, meaning that each atomic formula in the initial
context must be consumed exactly once during the evaluation of the proof.

The logic is four-valued, supporting TRUE, FALSE, BOTH, and NEITHER, allowing
it to reason about paraconsistent and paracomplete states.

The core of the interpreter is the `FourValuedInterpreter` class, which
recursively walks the AST, consuming resources from a context (a Counter of
available atoms) and returning the resulting logical value.

---

## `plllu_runner.py`

A command-line runner for pLLLU files.

This script provides an entry point for executing `.plllu` files. It
integrates the pLLLU lexer, parser, and interpreter to execute the logic
defined in a given pLLLU source file and print the result.

---

## `pre_submit_check.py`

_No module-level docstring found._

---

## `protocol_compiler.py`

This script now serves as the entry point for the hierarchical protocol compilation.
It discovers all protocol modules (subdirectories within `protocols/`) and compiles
each one into its own `AGENTS.md` file. It then generates a root `AGENTS.md`
that links to all the compiled modules, creating a unified, navigable system.

---

## `protocol_updater.py`

A command-line tool for programmatically updating protocol source files.

This script provides the mechanism for the agent to perform self-correction
by modifying its own governing protocols based on structured, actionable
lessons. It is a key component of the Protocol-Driven Self-Correction (PDSC)
workflow.

The tool operates on the .protocol.json files located in the `protocols/`
directory, performing targeted updates based on command-line arguments.

---

## `refactor.py`

A tool for performing automated symbol renaming in Python code.

This script provides a command-line interface to find a specific symbol
(a function or a class) in a given Python file and rename it, along with all of
its textual references throughout the entire repository. This provides a safe
and automated way to perform a common refactoring task, reducing the risk of
manual errors.

The tool operates in three main stages:
1.  **Definition Finding:** It uses Python's Abstract Syntax Tree (AST) module
    to parse the source file and precisely locate the definition of the target
    symbol. This ensures that the tool is targeting the correct code construct.
2.  **Reference Finding:** It performs a text-based search across the specified
    search path (defaulting to the entire repository) to find all files that
    mention the symbol's old name.
3.  **Plan Generation:** Instead of modifying files directly, it generates a
    refactoring "plan." This plan is a sequence of `replace_with_git_merge_diff`
    commands, one for each file that needs to be changed. The path to this
    generated plan file is printed to standard output.

This plan-based approach allows the agent's master controller to execute the
refactoring in a controlled, verifiable, and atomic way, consistent with its
standard operational procedures.

---

## `reliable_ls.py`

A tool for reliably listing files and directories.

This script provides a consistent, sorted, and recursive listing of files and
directories, excluding the `.git` directory. It is intended to be a more
reliable alternative to the standard `ls` command for agent use cases.

---

## `reorientation_manager.py`

Re-orientation Manager

This script is the core of the automated re-orientation process. It is
designed to be triggered by the build system whenever the agent's core
protocols (`AGENTS.md`) are re-compiled.

The manager performs the following key functions:
1.  **Diff Analysis:** It compares the old version of AGENTS.md with the new
    version to identify new protocols, tools, or other key concepts that have
    been introduced.
2.  **Temporal Orientation (Shallow Research):** For each new concept, it
    invokes the `temporal_orienter.py` tool to fetch a high-level summary from
    an external knowledge base like DBpedia. This ensures the agent has a
    baseline understanding of new terms.
3.  **Knowledge Storage:** The summaries from the temporal orientation are
    stored in a structured JSON file (`knowledge_core/temporal_orientations.json`),
    creating a persistent, queryable knowledge artifact.
4.  **Deep Research Trigger:** It analyzes the nature of the changes. If a
    change is deemed significant (e.g., the addition of a new core
    architectural protocol), it programmatically triggers a formal L4 Deep
    Research Cycle by creating a `deep_research_required.json` file.

This automated workflow ensures that the agent never operates with an outdated
understanding of its own protocols. It closes the loop between protocol
modification and the agent's self-awareness, making the system more robust,
adaptive, and reliable.

---

## `research.py`

This module contains the logic for executing research tasks based on a set of
constraints. It acts as a dispatcher, calling the appropriate tool (e.g.,
read_file, google_search) based on the specified target and scope.

---

## `research_planner.py`

This module is responsible for generating a formal, FSM-compliant research plan
for a given topic. The output is a string that can be executed by the agent's
master controller.

---

## `self_correction_orchestrator.py`

Orchestrates the Protocol-Driven Self-Correction (PDSC) workflow.

This script is the engine of the automated feedback loop. It reads structured,
actionable lessons from `knowledge_core/lessons.jsonl` and uses the
`protocol_updater.py` tool to apply them to the source protocol files.

---

## `self_improvement_cli.py`

Analyzes agent activity logs to identify opportunities for self-improvement.

This script is a command-line tool that serves as a key part of the agent's
meta-cognitive loop. It parses the structured activity log
(`logs/activity.log.jsonl`) to identify patterns that may indicate
inefficiencies or errors in the agent's workflow.

The primary analysis currently implemented is:
- **Planning Efficiency Analysis:** It scans the logs for tasks that required
  multiple `set_plan` actions. A high number of plan revisions for a single
  task can suggest that the initial planning phase was insufficient, the task
  was poorly understood, or the agent struggled to adapt to unforeseen
  challenges.

By flagging these tasks, the script provides a starting point for a deeper
post-mortem analysis, helping the agent (or its developers) to understand the
root causes of the planning churn and to develop strategies for more effective
upfront planning in the future.

The tool is designed to be extensible, with future analyses (such as error
rate tracking or tool usage anti-patterns) to be added as the system evolves.

---

## `standard_agents_compiler.py`

A compiler that generates a simplified, standard-compliant `AGENTS.md` file.

This script acts as an "adapter" to make the repository more accessible to
third-party AI agents that expect a conventional set of instructions. While the
repository's primary `AGENTS.md` is a complex, hierarchical, and
machine-readable artifact for its own specialized agent, the `AGENTS.standard.md`
file produced by this script offers a simple, human-readable summary of the
most common development commands.

The script works by:
1.  **Parsing the Makefile:** It dynamically parses the project's `Makefile`,
    which is the single source of truth for high-level commands. It specifically
    extracts the exact commands for common targets like `install`, `test`,
    `lint`, and `format`. This ensures the generated instructions are never
    stale.
2.  **Injecting into a Template:** It injects these extracted commands into a
    pre-defined, user-friendly Markdown template.
3.  **Generating the Artifact:** The final output is written to
    `AGENTS.standard.md`, providing a simple, stable, and conventional entry
    point for external tools, effectively bridging the gap between the complex
    internal protocol system and the broader agent ecosystem.

---

## `state.py`

Defines the core data structures for managing the agent's state.

This module provides the `AgentState` and `PlanContext` dataclasses, which are
fundamental to the operation of the Context-Free Development Cycle (CFDC). These
structures allow the `master_control.py` orchestrator to maintain a complete,
snapshot-able representation of the agent's progress through a task.

- `AgentState`: The primary container for all information related to the current
  task, including the plan execution stack, message history, and error states.
- `PlanContext`: A specific structure that holds the state of a single plan
  file, including its content and the current execution step. This is the
  element that gets pushed onto the `plan_stack` in `AgentState`.

Together, these classes enable the hierarchical, stack-based planning and
execution that is the hallmark of the CFDC.

---

## `symbol_map_generator.py`

Generates a code symbol map for the repository to aid in contextual understanding.

This script creates a `symbols.json` file in the `knowledge_core` directory,
which acts as a high-level index of the codebase. This map contains information
about key programming constructs like classes and functions, including their
name, location (file path and line number), and language.

The script employs a two-tiered approach for symbol generation:
1.  **Universal Ctags (Preferred):** It first checks for the presence of the
    `ctags` command-line tool. If available, it uses `ctags` to perform a
    comprehensive, multi-language scan of the repository. This is the most
    robust and accurate method.
2.  **AST Fallback (Python-only):** If `ctags` is not found, the script falls
    back to using Python's built-in Abstract Syntax Tree (`ast`) module. This
    method parses all `.py` files and extracts symbol information for Python
    code. While less comprehensive than `ctags`, it ensures that a baseline
    symbol map is always available.

The resulting `symbols.json` artifact is a critical input for the agent's
orientation and planning phases, allowing it to quickly locate relevant code
and understand the structure of the repository without having to read every file.

---

## `udc_orchestrator.py`

An orchestrator for executing Unrestricted Development Cycle (UDC) plans.

This script provides a sandboxed environment for running UDC plans, which are
low-level assembly-like programs that can perform Turing-complete computations.
The orchestrator acts as a virtual machine with a tape-based memory model,
registers, and a set of simple instructions.

To prevent non-termination and other resource-exhaustion issues, the
orchestrator imposes strict limits on the number of instructions executed,
the amount of memory used, and the total wall-clock time.


---
