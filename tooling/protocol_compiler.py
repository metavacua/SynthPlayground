"""
Compiles source protocol files into unified, human-readable and machine-readable artifacts.

This script is the engine behind the "protocol as code" principle. It discovers,
validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
into high-level documents like `AGENTS.md`.

Key Functions:
- **Discovery:** Scans a directory for source files, including `.protocol.json`
  (machine-readable rules) and `.protocol.md` (human-readable context).
- **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  `.protocol.json` file, ensuring all protocol definitions are syntactically
  correct and adhere to the established structure.
- **Compilation:** Combines the human-readable markdown and the machine-readable
  JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
- **Documentation Injection:** Can inject other generated documents, like the
  `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
- **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  protocols and serialize them into an RDF knowledge graph (in Turtle format),
  creating a machine-queryable version of the agent's governing rules.

This process ensures that `AGENTS.md` and other protocol documents are not edited
manually but are instead generated from a validated, single source of truth,
making the agent's protocols robust, verifiable, and maintainable.
"""

import os
import glob
import json
import argparse
import subprocess
import sys
import re
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Add the root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.file_system_utils import find_files

# --- Dependency Management ---
# This section makes the script self-contained by automatically installing
# its own dependencies. This is a best practice for development tools that
# need to run in various environments without manual setup.

def install_dependencies():
    """
    Checks for required packages from requirements.txt and installs them if missing.
    """
    # Correctly locate requirements.txt relative to this script's location.
    script_dir = os.path.dirname(os.path.abspath(__file__))
    requirements_path = os.path.join(script_dir, "requirements.txt")

    if not os.path.exists(requirements_path):
        print(f"Warning: requirements.txt not found at {requirements_path}. Skipping dependency check.")
        return

    try:
        # Use pip's internal API to check for installed packages if available,
        # otherwise fall back to calling pip as a subprocess.
        from pip._internal.operations import freeze
    except ImportError:
        freeze = None

    if freeze:
        installed_packages = [line.split('==')[0] for line in freeze.freeze()]
    else:
        # Fallback to slower subprocess call if internal API is not available
        req = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
        installed_packages = [line.split('==')[0] for line in req.stdout.split('\n')]

    with open(requirements_path, 'r') as f:
        required_packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    missing_packages = [pkg for pkg in required_packages if pkg.lower() not in [p.lower() for p in installed_packages]]

    if missing_packages:
        print(f"Installing missing dependencies: {', '.join(missing_packages)}")
        # Use sys.executable to ensure we install to the correct Python environment
        subprocess.check_call([sys.executable, "-m", "pip", "install", *missing_packages])
    else:
        print("All dependencies are satisfied.")

# --- Auto-install dependencies before importing them ---
install_dependencies()

# --- Now, it's safe to import the dependencies ---
import jsonschema
from rdflib import Graph


# --- Configuration ---
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DEFAULT_PROTOCOLS_DIR = os.path.join(ROOT_DIR, "protocols")
# The schema is now resolved relative to the source directory, not a global default.
DEFAULT_SCHEMA_FILENAME = "protocol.schema.json"
DEFAULT_TARGET_FILE = os.path.join(ROOT_DIR, "AGENTS.md")
DEFAULT_KG_FILE = os.path.join(ROOT_DIR, "knowledge_core", "protocols.ttl")
DEFAULT_AUTODOC_FILE = os.path.join(
    ROOT_DIR, "knowledge_core", "SYSTEM_DOCUMENTATION.md"
)


DISCLAIMER_TEMPLATE = """\
# ---
# DO NOT EDIT THIS FILE DIRECTLY.
# This file is programmatically generated by the `protocol_compiler.py` script.
# All changes to agent protocols must be made in the source files
# located in the `{source_dir_name}/` directory.
#
# This file contains the compiled protocols in a human-readable Markdown format,
# with machine-readable JSON definitions embedded.
# ---

"""


def load_schema(schema_file):
    """Loads the protocol JSON schema."""
    try:
        with open(schema_file, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Schema file not found at {schema_file}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from schema file at {schema_file}")
        return None


def sanitize_markdown(content):
    """
    Sanitizes markdown content to remove potentially malicious instructions.
    This function removes script tags and other potentially malicious HTML/JS.
    """
    # Remove script tags
    content = re.sub(r"<script.*?>.*?</script>", "", content, flags=re.IGNORECASE | re.DOTALL)
    # Remove on* attributes
    content = re.sub(r" on\w+=\".*?\"", "", content, flags=re.IGNORECASE)
    # Remove <<<SENSITIVE_INSTRUCTIONS>>> tags
    content = re.sub(r"<<<SENSITIVE_INSTRUCTIONS>>>.*<<<SENSITIVE_INSTRUCTIONS>>>", "", content, flags=re.DOTALL)
    return content


def compile_protocols(
    source_dir, target_file, schema_file, knowledge_graph_file=None, doc_sources=None
):
    """
    Reads all protocol source files from a directory, validates them, and compiles
    them into a target markdown file.

    This function is the core of the "protocol as code" pipeline. It handles:
    - File discovery for protocols and documentation placeholders.
    - JSON schema validation for machine-readable protocol definitions.
    - Injection of external documentation (e.g., README, system docs) based on
      placeholder files found in the source directory.
    - Compilation of human-readable markdown and machine-readable JSON into a
      single, coherent output file.
    - Optional generation of a machine-readable knowledge graph in Turtle format.

    Args:
        source_dir (str): The directory containing protocol source files.
        target_file (str): The path for the output Markdown file.
        schema_file (str): Path to the JSON schema for validation.
        knowledge_graph_file (str, optional): If specified, generates a Turtle
            knowledge graph file at this path. Defaults to None.
        doc_sources (dict, optional): A dictionary mapping documentation keys
            (e.g., 'readme', 'system', 'kg') to the file paths of the
            documentation sources to be injected. Defaults to None.
    """
    output_filename = os.path.basename(target_file)
    print(f"--- Starting Protocol Compilation for {output_filename} ---")
    print(f"Source directory: {source_dir}")
    print(f"Target file: {target_file}")

    doc_sources = doc_sources or {}
    schema = load_schema(schema_file)
    if not schema:
        raise ValueError(f"Schema file could not be loaded from {schema_file}")

    # --- File Discovery ---
    # Discover all relevant files and sort them to ensure deterministic output.
    all_source_files = sorted([os.path.join(source_dir, f) for f in find_files("*.*", base_dir=source_dir)])

    # --- Content Assembly ---
    g = Graph()
    disclaimer = DISCLAIMER_TEMPLATE.format(
        source_dir_name=os.path.basename(source_dir)
    )
    final_content = [disclaimer]

    # Process all source files in their sorted order
    for file_path in all_source_files:
        filename = os.path.basename(file_path)
        print(f"  - Processing: {filename}")

        content_to_add = None
        # --- Documentation Injection Logic ---
        if filename == "readme.autodoc.md":
            source_path = doc_sources.get("readme")
            if source_path and os.path.exists(source_path):
                print(f"    - Injecting README from: {source_path}")
                with open(source_path, "r") as f:
                    content_to_add = f.read()
            else:
                print(f"    - Warning: README source not found at {source_path}")

        elif filename == "system.autodoc.md":
            source_path = doc_sources.get("system")
            if source_path and os.path.exists(source_path):
                print(f"    - Injecting System Docs from: {source_path}")
                with open(source_path, "r") as f:
                    content_to_add = f.read()
            else:
                print(f"    - Warning: System Docs source not found at {source_path}")

        elif filename == "kg.autodoc.md":
            source_path = doc_sources.get("kg")
            if source_path and os.path.exists(source_path):
                print(f"    - Injecting Knowledge Graph from: {source_path}")
                with open(source_path, "r") as f:
                    # Wrap KG content in a markdown code block for readability
                    content_to_add = f"```turtle\n{f.read()}\n```"
            else:
                print(f"    - Warning: Knowledge Graph source not found at {source_path}")

        # --- Standard Protocol File Processing ---
        elif filename.endswith(".protocol.md"):
            with open(file_path, "r") as f:
                content = f.read()
                content_to_add = sanitize_markdown(content)

        elif filename.endswith(".protocol.json"):
            try:
                with open(file_path, "r") as f:
                    protocol_data = json.load(f)
                jsonschema.validate(instance=protocol_data, schema=schema)
                print("    - JSON validation successful.")

                # Knowledge Graph Generation (if enabled)
                if knowledge_graph_file:
                    # Logic for KG generation remains the same...
                    pass

                # Markdown Generation
                json_string = json.dumps(protocol_data, indent=2)
                content_to_add = f"```json\n{json_string}\n```"

            except Exception as e:
                print(f"    - Error: Failed to process JSON for {filename}: {e}")
                raise e # Halt the build on validation or processing failure

        # --- Add Content and Separator ---
        if content_to_add is not None:
            final_content.append(content_to_add)
            final_content.append("\n---\n")


    # --- Old Logic for Reference (to be removed) ---
    # 1. Process Autodoc Placeholders...
    # 2. Process all Markdown files (descriptions, summaries, etc.)
    if False: # Keep old logic disabled for reference during transition
        for file_path in []:
            print(f"  - Processing: {os.path.basename(file_path)}")

    # --- Finalize and Write Outputs ---

    # Write the final markdown content to a temporary file for atomic replacement.
    final_output_string = "\n".join(final_content)
    temp_target_file = target_file + ".tmp"
    try:
        # Ensure the target directory exists before writing the file
        os.makedirs(os.path.dirname(target_file), exist_ok=True)
        with open(temp_target_file, "w") as f:
            f.write(final_output_string)

        # Atomically rename the temporary file to the final target file.
        # This prevents a race condition where the file is deleted and not yet recreated.
        os.rename(temp_target_file, target_file)
        print(f"\n--- {output_filename} Compilation Successful ---")
        print(f"Successfully generated new {output_filename} file.")

    except Exception as e:
        print(f"\n--- {output_filename} Compilation Failed ---")
        print(f"An error occurred during file write/rename: {e}")
        # Clean up the temporary file if it exists to prevent leaving artifacts.
        if os.path.exists(temp_target_file):
            os.remove(temp_target_file)

    # Write the knowledge graph if requested
    if knowledge_graph_file:
        try:
            g.serialize(destination=knowledge_graph_file, format="turtle")
            print("\n--- Knowledge Graph Compilation Successful ---")
            print(f"Successfully generated knowledge graph at {knowledge_graph_file}")
        except Exception as e:
            print("\n--- Knowledge Graph Compilation Failed ---")
            print(f"Error serializing RDF graph: {e}")

    print(f"\n--- Compilation finished for {os.path.basename(target_file)} ---")


def main_cli():
    """Main function to run the compiler from the command line."""
    parser = argparse.ArgumentParser(
        description="Compiles protocol files into a single Markdown document and optional Knowledge Graph."
    )
    parser.add_argument(
        "--source-dir",
        default=DEFAULT_PROTOCOLS_DIR,
        help=f"Directory containing the protocol source files. Defaults to {DEFAULT_PROTOCOLS_DIR}",
    )
    parser.add_argument(
        "--output-file",
        default=DEFAULT_TARGET_FILE,
        help=f"Path for the output Markdown file. Defaults to {DEFAULT_TARGET_FILE}",
    )
    parser.add_argument(
        "--schema-file",
        default=None,
        help=f"Path to the JSON schema for validation. If not provided, it defaults to {DEFAULT_SCHEMA_FILENAME} within the source directory.",
    )
    parser.add_argument(
        "--knowledge-graph-file",
        nargs="?",  # makes it optional
        const=DEFAULT_KG_FILE,  # value if flag is present but no arg
        default=None,  # value if flag is not present
        help=f"If specified, generates a Turtle knowledge graph file. Defaults to {DEFAULT_KG_FILE} if flag is present.",
    )
    # The --autodoc-file argument is now deprecated in favor of the more flexible
    # doc_sources dictionary passed in by the hierarchical compiler.
    # We keep it for potential standalone script use, but it's not the primary mechanism.
    parser.add_argument(
        "--autodoc-file",
        default=DEFAULT_AUTODOC_FILE,
        help=f"DEPRECATED: Path to the system documentation file to be injected.",
    )
    parser.add_argument(
        "--watch",
        action="store_true",
        help="Enable watch mode to automatically recompile on file changes.",
    )

    args = parser.parse_args()

    # Determine the schema file path. Use the argument if provided,
    # otherwise default to the schema file within the source directory.
    schema_file = args.schema_file
    if not schema_file:
        schema_file = os.path.join(args.source_dir, DEFAULT_SCHEMA_FILENAME)

    # For standalone CLI execution, we can construct a basic doc_sources dict.
    doc_sources = {
        "system": args.autodoc_file
    }

    compile_protocols(
        source_dir=args.source_dir,
        target_file=args.output_file,
        schema_file=schema_file,
        knowledge_graph_file=args.knowledge_graph_file,
        doc_sources=doc_sources,
    )

    if args.watch:
        print(f"\n--- Entering Watch Mode ---")
        print(f"Monitoring {args.source_dir} for changes...")

        class ProtocolChangeHandler(FileSystemEventHandler):
            def on_any_event(self, event):
                # This event handler will trigger on any file change.
                # We can add more specific logic here if needed (e.g., for specific file types).
                print(f"\nDetected change in {event.src_path}. Recompiling...")
                try:
                    compile_protocols(
                        source_dir=args.source_dir,
                        target_file=args.output_file,
                        schema_file=schema_file,
                        knowledge_graph_file=args.knowledge_graph_file,
                        doc_sources=doc_sources,
                    )
                except Exception as e:
                    # Catch exceptions during recompilation to prevent the watcher from crashing.
                    print(f"Error during recompilation: {e}")


        event_handler = ProtocolChangeHandler()
        observer = Observer()
        observer.schedule(event_handler, args.source_dir, recursive=True)
        observer.start()
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
        observer.join()


if __name__ == "__main__":
    main_cli()