"""
Compiles source protocol files into unified, human-readable and machine-readable artifacts.

This script is the engine behind the "protocol as code" principle. It discovers,
validates, and assembles protocol definitions from a source directory (e.g., `protocols/`)
into high-level documents like `AGENTS.md`.

Key Functions:
- **Discovery:** Scans a directory for source files, including `.protocol.json`
  (machine-readable rules) and `.protocol.md` (human-readable context).
- **Validation:** Uses a JSON schema (`protocol.schema.json`) to validate every
  `.protocol.json` file, ensuring all protocol definitions are syntactically
  correct and adhere to the established structure.
- **Compilation:** Combines the human-readable markdown and the machine-readable
  JSON into a single, cohesive Markdown file, embedding the JSON in code blocks.
- **Documentation Injection:** Can inject other generated documents, like the
  `SYSTEM_DOCUMENTATION.md`, into the final output at specified locations.
- **Knowledge Graph Generation:** Optionally, it can process the validated JSON
  protocols and serialize them into an RDF knowledge graph (in Turtle format),
  creating a machine-queryable version of the agent's governing rules.

This process ensures that `AGENTS.md` and other protocol documents are not edited
manually but are instead generated from a validated, single source of truth,
making the agent's protocols robust, verifiable, and maintainable.
"""

import os
import glob
import json
import argparse
import subprocess
import sys
import re
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Add the root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from utils.file_system_utils import find_files

# --- Dependency Management ---
# This section makes the script self-contained by automatically installing
# its own dependencies. This is a best practice for development tools that
# need to run in various environments without manual setup.


def install_dependencies():
    """
    Checks for required packages from requirements.txt and installs them if missing.
    """
    # Correctly locate requirements.txt relative to this script's location.
    script_dir = os.path.dirname(os.path.abspath(__file__))
    requirements_path = os.path.join(script_dir, "requirements.txt")

    if not os.path.exists(requirements_path):
        print(
            f"Warning: requirements.txt not found at {requirements_path}. Skipping dependency check."
        )
        return

    try:
        # Use pip's internal API to check for installed packages if available,
        # otherwise fall back to calling pip as a subprocess.
        from pip._internal.operations import freeze
    except ImportError:
        freeze = None

    if freeze:
        installed_packages = [line.split("==")[0] for line in freeze.freeze()]
    else:
        # Fallback to slower subprocess call if internal API is not available
        req = subprocess.run(
            [sys.executable, "-m", "pip", "freeze"], capture_output=True, text=True
        )
        installed_packages = [line.split("==")[0] for line in req.stdout.split("\n")]

    with open(requirements_path, "r") as f:
        required_packages = [
            line.strip() for line in f if line.strip() and not line.startswith("#")
        ]

    missing_packages = [
        pkg
        for pkg in required_packages
        if pkg.lower() not in [p.lower() for p in installed_packages]
    ]

    if missing_packages:
        print(f"Installing missing dependencies: {', '.join(missing_packages)}")
        # Use sys.executable to ensure we install to the correct Python environment
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", *missing_packages]
        )
    else:
        print("All dependencies are satisfied.")


# --- Auto-install dependencies before importing them ---
install_dependencies()

# --- Now, it's safe to import the dependencies ---
import jsonschema
from rdflib import Graph


# --- Configuration ---
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DEFAULT_PROTOCOLS_DIR = os.path.join(ROOT_DIR, "protocols")
# The schema is now resolved relative to the source directory, not a global default.
DEFAULT_SCHEMA_FILENAME = "protocol.schema.json"
DEFAULT_TARGET_FILE = os.path.join(ROOT_DIR, "AGENTS.md")
DEFAULT_KG_FILE = os.path.join(ROOT_DIR, "knowledge_core", "protocols.ttl")
DEFAULT_AUTODOC_FILE = os.path.join(
    ROOT_DIR, "knowledge_core", "SYSTEM_DOCUMENTATION.md"
)


DISCLAIMER_TEMPLATE = """\
# ---
# DO NOT EDIT THIS FILE DIRECTLY.
# This file is programmatically generated by the `protocol_compiler.py` script.
# All changes to agent protocols must be made in the source files
# located in the `{source_dir_name}/` directory.
#
# This file contains the compiled protocols in a human-readable Markdown format,
# with machine-readable JSON definitions embedded.
# ---

"""


def load_schema(schema_file):
    """Loads the protocol JSON schema."""
    try:
        with open(schema_file, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Schema file not found at {schema_file}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from schema file at {schema_file}")
        return None


def sanitize_markdown(content):
    """
    Sanitizes markdown content to remove potentially malicious instructions.
    This function removes script tags and other potentially malicious HTML/JS.
    """
    # Remove script tags
    content = re.sub(
        r"<script.*?>.*?</script>", "", content, flags=re.IGNORECASE | re.DOTALL
    )
    # Remove on* attributes
    content = re.sub(r" on\w+=\".*?\"", "", content, flags=re.IGNORECASE)
    # Remove <<<SENSITIVE_INSTRUCTIONS>>> tags
    content = re.sub(
        r"<<<SENSITIVE_INSTRUCTIONS>>>.*<<<SENSITIVE_INSTRUCTIONS>>>",
        "",
        content,
        flags=re.DOTALL,
    )
    return content


def compile_protocols(
    source_dir, target_file, schema_file, knowledge_graph_file=None, autodoc_file=None
):
    """
    Reads all .protocol.json and corresponding .protocol.md files from the
    source directory, validates them, and compiles them into a target markdown file.
    Optionally, it can also generate a machine-readable knowledge graph.
    """
    output_filename = os.path.basename(target_file)
    print(f"--- Starting Protocol Compilation for {output_filename} ---")
    print(f"Source directory: {source_dir}")
    print(f"Target file: {target_file}")

    schema = load_schema(schema_file)
    if not schema:
        return

    # --- File Discovery ---
    # Discover all relevant files and sort them to ensure deterministic output.
    # The sort order is: autodoc placeholders, then all markdown, then all json.
    # This ensures descriptions and summaries appear before the JSON blocks.
    autodoc_placeholders = sorted(
        [
            os.path.join(source_dir, f)
            for f in find_files("*.autodoc.md", base_dir=source_dir)
        ]
    )
    all_md_files = sorted(
        [
            os.path.join(source_dir, f)
            for f in find_files("*.protocol.md", base_dir=source_dir)
        ]
    )
    all_json_files = sorted(
        [
            os.path.join(source_dir, f)
            for f in find_files("*.protocol.json", base_dir=source_dir)
        ]
    )

    if not all_md_files and not all_json_files and not autodoc_placeholders:
        print(f"Warning: No protocol or documentation files found in {source_dir}.")
        with open(target_file, "w") as f:
            f.write(
                DISCLAIMER_TEMPLATE.format(source_dir_name=os.path.basename(source_dir))
            )
        return

    print(
        f"Found {len(all_json_files)} protocol, {len(all_md_files)} markdown, and {len(autodoc_placeholders)} autodoc files."
    )

    # --- Content Assembly ---
    g = Graph()
    disclaimer = DISCLAIMER_TEMPLATE.format(
        source_dir_name=os.path.basename(source_dir)
    )
    final_content = [disclaimer]

    # 1. Process Autodoc Placeholders
    for file_path in autodoc_placeholders:
        print(f"  - Processing: {os.path.basename(file_path)}")
        if autodoc_file and os.path.exists(autodoc_file):
            try:
                with open(autodoc_file, "r") as f:
                    final_content.append(f.read())
                print(f"    - Injected system documentation from {autodoc_file}")
            except Exception as e:
                print(f"    - Error reading autodoc file {autodoc_file}: {e}")
        else:
            print(
                f"    - Warning: System documentation file not found at {autodoc_file}"
            )
        final_content.append("\n---\n")

    # 2. Process all Markdown files (descriptions, summaries, etc.)
    for file_path in all_md_files:
        print(f"  - Processing: {os.path.basename(file_path)}")
        with open(file_path, "r") as f:
            content = f.read()
            sanitized_content = sanitize_markdown(content)
            final_content.append(sanitized_content)
        final_content.append("\n---\n")

    # 3. Process all JSON protocol files
    for file_path in all_json_files:
        base_name = os.path.basename(file_path)
        print(f"  - Processing: {base_name}")
        try:
            with open(file_path, "r") as f:
                protocol_data = json.load(f)
            jsonschema.validate(instance=protocol_data, schema=schema)
            print("    - JSON validation successful.")

            # Knowledge Graph Generation
            if knowledge_graph_file:
                protocol_data_for_ld = protocol_data.copy()
                # The context file should be relative to the source dir being processed
                context_path = os.path.join(source_dir, "protocol.context.jsonld")
                if os.path.exists(context_path):
                    relative_context_path = os.path.relpath(
                        context_path, os.path.dirname(file_path)
                    )
                    protocol_data_for_ld["@context"] = relative_context_path
                    base_uri = (
                        "file://" + os.path.abspath(os.path.dirname(file_path)) + "/"
                    )
                    g.parse(
                        data=json.dumps(protocol_data_for_ld),
                        format="json-ld",
                        publicID=base_uri,
                    )
                    print(f"    - Parsed {base_name} into knowledge graph.")
                else:
                    print(
                        f"    - Warning: JSON-LD context file not found at {context_path}"
                    )

            # Markdown Generation
            json_string = json.dumps(protocol_data, indent=2)
            md_json_block = f"```json\n{json_string}\n```\n"
            final_content.append(md_json_block)

        except Exception as e:
            print(f"    - Error: Failed to process JSON for {base_name}: {e}")
            # Re-raise the exception to cause the script to exit with a non-zero status code.
            # This ensures that a validation failure stops the entire build process.
            raise e
        final_content.append("\n---\n")

    # --- Finalize and Write Outputs ---

    # Write the final markdown content to a temporary file for atomic replacement.
    final_output_string = "\n".join(final_content)
    temp_target_file = target_file + ".tmp"
    try:
        # Ensure the target directory exists before writing the file
        os.makedirs(os.path.dirname(target_file), exist_ok=True)
        with open(temp_target_file, "w") as f:
            f.write(final_output_string)

        # Atomically rename the temporary file to the final target file.
        # This prevents a race condition where the file is deleted and not yet recreated.
        os.rename(temp_target_file, target_file)
        print(f"\n--- {output_filename} Compilation Successful ---")
        print(f"Successfully generated new {output_filename} file.")

    except Exception as e:
        print(f"\n--- {output_filename} Compilation Failed ---")
        print(f"An error occurred during file write/rename: {e}")
        # Clean up the temporary file if it exists to prevent leaving artifacts.
        if os.path.exists(temp_target_file):
            os.remove(temp_target_file)

    # Write the knowledge graph if requested
    if knowledge_graph_file:
        try:
            g.serialize(destination=knowledge_graph_file, format="turtle")
            print("\n--- Knowledge Graph Compilation Successful ---")
            print(f"Successfully generated knowledge graph at {knowledge_graph_file}")
        except Exception as e:
            print("\n--- Knowledge Graph Compilation Failed ---")
            print(f"Error serializing RDF graph: {e}")

    print(f"\n--- Compilation finished for {os.path.basename(target_file)} ---")


def main_cli():
    """Main function to run the compiler from the command line."""
    parser = argparse.ArgumentParser(
        description="Compiles protocol files into a single Markdown document and optional Knowledge Graph."
    )
    parser.add_argument(
        "--source-dir",
        default=DEFAULT_PROTOCOLS_DIR,
        help=f"Directory containing the protocol source files. Defaults to {DEFAULT_PROTOCOLS_DIR}",
    )
    parser.add_argument(
        "--output-file",
        default=DEFAULT_TARGET_FILE,
        help=f"Path for the output Markdown file. Defaults to {DEFAULT_TARGET_FILE}",
    )
    parser.add_argument(
        "--schema-file",
        default=None,
        help=f"Path to the JSON schema for validation. If not provided, it defaults to {DEFAULT_SCHEMA_FILENAME} within the source directory.",
    )
    parser.add_argument(
        "--knowledge-graph-file",
        nargs="?",  # makes it optional
        const=DEFAULT_KG_FILE,  # value if flag is present but no arg
        default=None,  # value if flag is not present
        help=f"If specified, generates a Turtle knowledge graph file. Defaults to {DEFAULT_KG_FILE} if flag is present.",
    )
    parser.add_argument(
        "--autodoc-file",
        default=DEFAULT_AUTODOC_FILE,
        help=f"Path to the system documentation file to be injected. Defaults to {DEFAULT_AUTODOC_FILE}",
    )
    parser.add_argument(
        "--watch",
        action="store_true",
        help="Enable watch mode to automatically recompile on file changes.",
    )

    args = parser.parse_args()

    # Determine the schema file path. Use the argument if provided,
    # otherwise default to the schema file within the source directory.
    schema_file = args.schema_file
    if not schema_file:
        schema_file = os.path.join(args.source_dir, DEFAULT_SCHEMA_FILENAME)

    compile_protocols(
        source_dir=args.source_dir,
        target_file=args.output_file,
        schema_file=schema_file,
        knowledge_graph_file=args.knowledge_graph_file,
        autodoc_file=args.autodoc_file,
    )

    if args.watch:
        print(f"\n--- Entering Watch Mode ---")
        print(f"Monitoring {args.source_dir} for changes...")

        class ProtocolChangeHandler(FileSystemEventHandler):
            def on_any_event(self, event):
                # This event handler will trigger on any file change.
                # We can add more specific logic here if needed (e.g., for specific file types).
                print(f"\nDetected change in {event.src_path}. Recompiling...")
                try:
                    compile_protocols(
                        source_dir=args.source_dir,
                        target_file=args.output_file,
                        schema_file=schema_file,
                        knowledge_graph_file=args.knowledge_graph_file,
                        autodoc_file=args.autodoc_file,
                    )
                except Exception as e:
                    # Catch exceptions during recompilation to prevent the watcher from crashing.
                    print(f"Error during recompilation: {e}")

        event_handler = ProtocolChangeHandler()
        observer = Observer()
        observer.schedule(event_handler, args.source_dir, recursive=True)
        observer.start()
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
        observer.join()


if __name__ == "__main__":
    main_cli()
